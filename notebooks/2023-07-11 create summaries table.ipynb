{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from silvhua import *\n",
    "from article_processing import create_text_dict_from_folder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the option to wrap text within cells\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From [previous note book](2023-07-11%20create%20references%20table.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    created = mapped_column(TIMESTAMP(timezone=True))\n",
    "    full_template = mapped_column(Text)\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    original_notification = mapped_column(String(255))\n",
    "    simple_summary = mapped_column(Text)\n",
    "    simple_notification = mapped_column(String(255))\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications'):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "@testing_session\n",
    "def get_session(session):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    return session\n",
    "\n",
    "\n",
    "def bulk_append(references_df, engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(references_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                reference = Sources(\n",
    "                    title=row['title'],\n",
    "                    text=row['text'],\n",
    "                    abstract=row['abstract'],\n",
    "                    publication=row['publication'],\n",
    "                    authors=row['authors'],\n",
    "                    year=row['year'],\n",
    "                    month=row['month'],\n",
    "                    pub_volume=row['pub_volume'],\n",
    "                    pub_issue=row['pub_issue'],\n",
    "                    start_page=row['start_page'],\n",
    "                    end_page=row['end_page'],\n",
    "                    doi=row['doi']\n",
    "                )\n",
    "                session.add(reference)\n",
    "                print(f'\\t{row[\"title\"]}')\n",
    "\n",
    "            references_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.text_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['date'] = datetime.now().strftime(\"%Y-%m-%d %H%M\")\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.qna['text_id'] = self.text_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model\n",
    "        self.qna['prep step'] = prep_step.strip()\n",
    "        self.qna['summarization task'] = task.strip()\n",
    "        self.qna['edit task'] = edit_task.strip()\n",
    "        self.qna['full summarization task'] = full_task.strip()\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "system_role = \"You are a helpful assistant.\"\n",
    "prep_step = [\n",
    "    # \"Tell your friend about the research in a text message.\",\n",
    "    \"In the summary, cover the following information: \\\n",
    "    \\n- Identify the key points and statistics from this text that would make interesting or helpful health content. \\\n",
    "    \\n- If available, include the effect sizes found in the research. \\\n",
    "    Otherwise, skip this step. \\\n",
    "    \\n- If applicable, get a brief description of the research participants, \\\n",
    "    such as age, sex, and health conditions. Otherwise, you can skip this step.\\\n",
    "    \\n- Think about why the general population should care about the research.\",\n",
    "]\n",
    "\n",
    "summarize_task = [\n",
    "    f\"1. Summarize the text for a LinkedIn post.\",\n",
    "]\n",
    "edit_task = [\n",
    "    \"\"\"\n",
    "    Once you have written your text message: \\\n",
    "    \\nEvaluate your text message to see if it may be confusing or redundant. \\\n",
    "    \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same. \\\n",
    "    \\n2. Create an intriguing subject line for the text.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "simplify_task = [\n",
    "    \"\"\"3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same.\\\n",
    "    Follow these steps to accomplish this: \\\n",
    "    \\na. Check if the content and language are appropriate for the audience. \\\n",
    "    \\nb. If it is suitable for the audience, keep it the same. If not, rewrite using terms appropriate for the audience. \\ \n",
    "    \\nc. Return the final version of the summary to be shown to the audience. \\\n",
    "    \\n\\nYour audience is\"\"\",\n",
    "]\n",
    "\n",
    "simplify_audience = [\n",
    "    \"people without a science background\",\n",
    "]\n",
    "\n",
    "format_task = [\n",
    "    \"\"\"4. Return your final response in a JSON format with the following format: \\\n",
    "    \\n{\"headline\": <subject line from step 2>, \\\n",
    "    \\n\"body\": <text from step 1>,\n",
    "    \\n\"audience\": <rewritten text from step 3>}\"\"\"\n",
    "]\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1.4\n",
    "n_choices = 1\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "save_outputs=False\n",
    "\n",
    "## Add rows from references dataframe\n",
    "# bulk_append(references_df)\n",
    "\n",
    "# session = get_session()\n",
    "# table_names = ['prompts', 'sources', 'summaries']\n",
    "# tables_dict = dict()\n",
    "\n",
    "for table in table_names:\n",
    "    tables_dict[table] = get_table(table=table)\n",
    "tables_dict['sources']\n",
    "\n",
    "# sources_df = tables_dict['sources'].head(2)\n",
    "# chaining_dict = batch_summarize(\n",
    "#     sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "#     simplify_task, simplify_audience, format_task,\n",
    "#     chatbot_dict,\n",
    "#     system_role=system_role, model=model, max_tokens=1000,\n",
    "#     n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "#     iteration_id=iteration_id, save_outputs=save_outputs\n",
    "#     )\n",
    "# chaining_dict[iteration_id]\n",
    "qna_dict = create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "    )\n",
    "qna_dict[iteration_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_dict = dict()\n",
    "chatbot_dict = dict()\n",
    "simple_summaries_dict = dict()\n",
    "relevance_dict = dict()\n",
    "chain_results_dict = dict()\n",
    "save = True\n",
    "# save_outputs = False\n",
    "save_outputs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 1 Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-07-11 21:45:47.442727'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from prompts\n",
      "Query: SELECT * from sources\n",
      "Query: SELECT * from summaries\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publication</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>pub_volume</th>\n",
       "      <th>pub_issue</th>\n",
       "      <th>start_page</th>\n",
       "      <th>end_page</th>\n",
       "      <th>doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Comparisons in the Recovery Response From Resi...</td>\n",
       "      <td>Decreases in muscle mass, function, and neurom...</td>\n",
       "      <td>Gordon, JA III, Hoffman, JR, Arroyo, E, Varano...</td>\n",
       "      <td>Journal of strength and conditioning research</td>\n",
       "      <td>Joseph A Gordon, Jay R Hoffman, Eliott Arroyo,...</td>\n",
       "      <td>2017</td>\n",
       "      <td></td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>3454</td>\n",
       "      <td>3462</td>\n",
       "      <td>10.1519/JSC.0000000000002219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Effect of dietary sources of calcium and prote...</td>\n",
       "      <td>Longevity increases the proportion of older ad...</td>\n",
       "      <td>To assess the antifracture efficacy and safety...</td>\n",
       "      <td>BMJ (Clinical research ed.)</td>\n",
       "      <td>S Iuliano, S Poon, J Robbins, M Bui, X Wang, L...</td>\n",
       "      <td>2021</td>\n",
       "      <td></td>\n",
       "      <td>375</td>\n",
       "      <td></td>\n",
       "      <td>n2364</td>\n",
       "      <td></td>\n",
       "      <td>10.1136/bmj.n2364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Exercise Snacks A Novel Strategy to Improve Ca...</td>\n",
       "      <td>We define exercise snacks as isolated ?1-min b...</td>\n",
       "      <td>We define exercise snacks as isolated &amp;#x2264;...</td>\n",
       "      <td>Exercise and sport sciences reviews</td>\n",
       "      <td>Hashim Islam, Martin J Gibala, Jonathan P Little</td>\n",
       "      <td>2022</td>\n",
       "      <td></td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>10.1249/JES.0000000000000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Food craving, cortisol and ghrelin responses i...</td>\n",
       "      <td>The United States is at the forefront of the g...</td>\n",
       "      <td>Overeating of highly palatable (HP) foods in t...</td>\n",
       "      <td>Physiology &amp;amp; behavior</td>\n",
       "      <td>Rajita Sinha, Peihua Gu, Rachel Hart, J B Guar...</td>\n",
       "      <td>2019</td>\n",
       "      <td></td>\n",
       "      <td>208</td>\n",
       "      <td></td>\n",
       "      <td>112563</td>\n",
       "      <td></td>\n",
       "      <td>10.1016/j.physbeh.2019.112563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Hypohydration but not Menstrual Phase Influenc...</td>\n",
       "      <td>Pain is recognized as a public health problem ...</td>\n",
       "      <td>Chronic pain is a pervasive health problem and...</td>\n",
       "      <td>Journal of applied physiology (Bethesda, Md. :...</td>\n",
       "      <td>Beverly Tan, Michael C Philipp, Ahmad Munir Ch...</td>\n",
       "      <td>2022</td>\n",
       "      <td></td>\n",
       "      <td>132</td>\n",
       "      <td>3</td>\n",
       "      <td>611</td>\n",
       "      <td>621</td>\n",
       "      <td>10.1152/japplphysiol.00402.2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Weight stigma and health behaviors: evidence f...</td>\n",
       "      <td>Weight stigma is pervasive. Higher weight indi...</td>\n",
       "      <td>Weight stigma is pervasive across the U.S. and...</td>\n",
       "      <td>International journal of obesity (2005)</td>\n",
       "      <td>Kristen M Lee, Jeffrey M Hunger, A Janet Tomiyama</td>\n",
       "      <td>2021</td>\n",
       "      <td></td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>1499</td>\n",
       "      <td>1509</td>\n",
       "      <td>10.1038/s41366-021-00814-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   1  Comparisons in the Recovery Response From Resi...   \n",
       "1   2  Effect of dietary sources of calcium and prote...   \n",
       "2   3  Exercise Snacks A Novel Strategy to Improve Ca...   \n",
       "3   4  Food craving, cortisol and ghrelin responses i...   \n",
       "4   5  Hypohydration but not Menstrual Phase Influenc...   \n",
       "5   6  Weight stigma and health behaviors: evidence f...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Decreases in muscle mass, function, and neurom...   \n",
       "1  Longevity increases the proportion of older ad...   \n",
       "2  We define exercise snacks as isolated ?1-min b...   \n",
       "3  The United States is at the forefront of the g...   \n",
       "4  Pain is recognized as a public health problem ...   \n",
       "5  Weight stigma is pervasive. Higher weight indi...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Gordon, JA III, Hoffman, JR, Arroyo, E, Varano...   \n",
       "1  To assess the antifracture efficacy and safety...   \n",
       "2  We define exercise snacks as isolated &#x2264;...   \n",
       "3  Overeating of highly palatable (HP) foods in t...   \n",
       "4  Chronic pain is a pervasive health problem and...   \n",
       "5  Weight stigma is pervasive across the U.S. and...   \n",
       "\n",
       "                                         publication  \\\n",
       "0      Journal of strength and conditioning research   \n",
       "1                        BMJ (Clinical research ed.)   \n",
       "2                Exercise and sport sciences reviews   \n",
       "3                          Physiology &amp; behavior   \n",
       "4  Journal of applied physiology (Bethesda, Md. :...   \n",
       "5            International journal of obesity (2005)   \n",
       "\n",
       "                                             authors  year month pub_volume  \\\n",
       "0  Joseph A Gordon, Jay R Hoffman, Eliott Arroyo,...  2017               31   \n",
       "1  S Iuliano, S Poon, J Robbins, M Bui, X Wang, L...  2021              375   \n",
       "2   Hashim Islam, Martin J Gibala, Jonathan P Little  2022               50   \n",
       "3  Rajita Sinha, Peihua Gu, Rachel Hart, J B Guar...  2019              208   \n",
       "4  Beverly Tan, Michael C Philipp, Ahmad Munir Ch...  2022              132   \n",
       "5  Kristen M Lee, Jeffrey M Hunger, A Janet Tomiyama  2021               45   \n",
       "\n",
       "  pub_issue start_page end_page                              doi  \n",
       "0        12       3454     3462     10.1519/JSC.0000000000002219  \n",
       "1                n2364                         10.1136/bmj.n2364  \n",
       "2         1         31       37     10.1249/JES.0000000000000275  \n",
       "3               112563             10.1016/j.physbeh.2019.112563  \n",
       "4         3        611      621  10.1152/japplphysiol.00402.2021  \n",
       "5         7       1499     1509       10.1038/s41366-021-00814-5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    created = mapped_column(TIMESTAMP(timezone=True))\n",
    "    full_template = mapped_column(Text)\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    original_headline = mapped_column(String(255))\n",
    "    simple_summary = mapped_column(Text)\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications'):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "@testing_session\n",
    "def get_session(session):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    return session\n",
    "\n",
    "\n",
    "def bulk_append(references_df, engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(references_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                reference = Sources(\n",
    "                    title=row['title'],\n",
    "                    text=row['text'],\n",
    "                    abstract=row['abstract'],\n",
    "                    publication=row['publication'],\n",
    "                    authors=row['authors'],\n",
    "                    year=row['year'],\n",
    "                    month=row['month'],\n",
    "                    pub_volume=row['pub_volume'],\n",
    "                    pub_issue=row['pub_issue'],\n",
    "                    start_page=row['start_page'],\n",
    "                    end_page=row['end_page'],\n",
    "                    doi=row['doi']\n",
    "                )\n",
    "                session.add(reference)\n",
    "                print(f'\\t{row[\"title\"]}')\n",
    "\n",
    "            references_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.text_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['date'] = datetime.now().strftime(\"%Y-%m-%d %H%M\")\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.qna['text_id'] = self.text_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model\n",
    "        self.qna['prep step'] = prep_step.strip()\n",
    "        self.qna['summarization task'] = task.strip()\n",
    "        self.qna['edit task'] = edit_task.strip()\n",
    "        self.qna['full summarization task'] = full_task.strip()\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "system_role = \"You are a helpful assistant.\"\n",
    "prep_step = [\n",
    "    # \"Tell your friend about the research in a text message.\",\n",
    "    \"In the summary, cover the following information: \\\n",
    "    \\n- Identify the key points and statistics from this text that would make interesting or helpful health content. \\\n",
    "    \\n- If available, include the effect sizes found in the research. \\\n",
    "    Otherwise, skip this step. \\\n",
    "    \\n- If applicable, get a brief description of the research participants, \\\n",
    "    such as age, sex, and health conditions. Otherwise, you can skip this step.\\\n",
    "    \\n- Think about why the general population should care about the research.\",\n",
    "]\n",
    "\n",
    "summarize_task = [\n",
    "    f\"1. Summarize the text for a LinkedIn post.\",\n",
    "]\n",
    "edit_task = [\n",
    "    \"\"\"\n",
    "    Once you have written your text message: \\\n",
    "    \\nEvaluate your text message to see if it may be confusing or redundant. \\\n",
    "    \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same. \\\n",
    "    \\n2. Create an intriguing subject line for the text.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "simplify_task = [\n",
    "    \"\"\"3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same.\\\n",
    "    Follow these steps to accomplish this: \\\n",
    "    \\na. Check if the content and language are appropriate for the audience. \\\n",
    "    \\nb. If it is suitable for the audience, keep it the same. \\\n",
    "    If not, rewrite using terms appropriate for the audience while keeping the news-worthy details. \\ \n",
    "    \\nc. Return the final version of the summary to be shown to the audience. \\\n",
    "    \\n\\nYour audience is\"\"\",\n",
    "]\n",
    "\n",
    "simplify_audience = [\n",
    "    \"people without a science background\",\n",
    "]\n",
    "\n",
    "format_task = [\n",
    "    \"\"\"4. Return your final response in a JSON format with the following format: \\\n",
    "    \\n{\"headline\": <subject line from step 2>, \\\n",
    "    \\n\"body\": <text from step 1>,\n",
    "    \\n\"audience\": <rewritten text from step 3>}\"\"\"\n",
    "]\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1\n",
    "n_choices = 2\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "save_outputs=False\n",
    "\n",
    "## Add rows from references dataframe\n",
    "# bulk_append(references_df)\n",
    "\n",
    "# session = get_session()\n",
    "table_names = ['prompts', 'sources', 'summaries']\n",
    "tables_dict = dict()\n",
    "\n",
    "for table in table_names:\n",
    "    tables_dict[table] = get_table(table=table)\n",
    "tables_dict['sources']\n",
    "\n",
    "# sources_df = tables_dict['sources'].head(2)\n",
    "# chaining_dict = batch_summarize(\n",
    "#     sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "#     simplify_task, simplify_audience, format_task,\n",
    "#     chatbot_dict,\n",
    "#     system_role=system_role, model=model, max_tokens=1000,\n",
    "#     n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "#     iteration_id=iteration_id, save_outputs=save_outputs\n",
    "#     )\n",
    "# chaining_dict[iteration_id]\n",
    "# qna_dict = create_summaries_df(\n",
    "#     qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "#     )\n",
    "# qna_dict[iteration_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from sources LIMIT 2\n",
      "**Text #1 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "**Text #2 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "Processing 1_prompt00...\n",
      "Processing 2_prompt00...\n",
      "Original summaries DataFrame shape: (4, 15)\n",
      "\tOriginal summaries Dataframe columns: Index(['choice', 'date', 'folder', 'text_id', 'article_title', 'text',\n",
      "       'system_role', 'model', 'prep step', 'summarization task', 'edit task',\n",
      "       'full summarization task', 'summary', 'headline', 'simple_summary'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>folder</th>\n",
       "      <th>text_id</th>\n",
       "      <th>choice</th>\n",
       "      <th>article_title</th>\n",
       "      <th>text</th>\n",
       "      <th>system_role</th>\n",
       "      <th>model</th>\n",
       "      <th>prep step</th>\n",
       "      <th>summarization task</th>\n",
       "      <th>edit task</th>\n",
       "      <th>full summarization task</th>\n",
       "      <th>summary</th>\n",
       "      <th>headline</th>\n",
       "      <th>simple_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-11 2131</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparisons in the Recovery Response From Resi...</td>\n",
       "      <td>Decreases in muscle mass, function, and neurom...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>A recent study found that middle-aged adults w...</td>\n",
       "      <td>New Study Shows Age Does Not Affect Recovery f...</td>\n",
       "      <td>A recent study found that middle-aged adults w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-11 2131</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Comparisons in the Recovery Response From Resi...</td>\n",
       "      <td>Decreases in muscle mass, function, and neurom...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "      <td>New Study Shows How Resistance Training Can He...</td>\n",
       "      <td>A recent study found that participating in rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-11 2131</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Effect of dietary sources of calcium and prote...</td>\n",
       "      <td>Longevity increases the proportion of older ad...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>A recent study found that a high calcium and h...</td>\n",
       "      <td>New Study Shows Nutritional Intervention Reduc...</td>\n",
       "      <td>A recent study has found that a simple nutriti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-11 2131</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Effect of dietary sources of calcium and prote...</td>\n",
       "      <td>Longevity increases the proportion of older ad...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>A recent study conducted on institutionalized ...</td>\n",
       "      <td>New Study Shows Nutritional Intervention Reduc...</td>\n",
       "      <td>A recent study has found that a simple nutriti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date                  folder  text_id  choice  \\\n",
       "0  2023-07-11 2131  text/2023-07-11 for db        1       1   \n",
       "1  2023-07-11 2131  text/2023-07-11 for db        1       2   \n",
       "2  2023-07-11 2131  text/2023-07-11 for db        2       1   \n",
       "3  2023-07-11 2131  text/2023-07-11 for db        2       2   \n",
       "\n",
       "                                       article_title  \\\n",
       "0  Comparisons in the Recovery Response From Resi...   \n",
       "1  Comparisons in the Recovery Response From Resi...   \n",
       "2  Effect of dietary sources of calcium and prote...   \n",
       "3  Effect of dietary sources of calcium and prote...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Decreases in muscle mass, function, and neurom...   \n",
       "1  Decreases in muscle mass, function, and neurom...   \n",
       "2  Longevity increases the proportion of older ad...   \n",
       "3  Longevity increases the proportion of older ad...   \n",
       "\n",
       "                    system_role                   model  \\\n",
       "0  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "1  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "2  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "3  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "\n",
       "                                           prep step  \\\n",
       "0  In the summary, cover the following informatio...   \n",
       "1  In the summary, cover the following informatio...   \n",
       "2  In the summary, cover the following informatio...   \n",
       "3  In the summary, cover the following informatio...   \n",
       "\n",
       "                           summarization task  \\\n",
       "0  1. Summarize the text for a LinkedIn post.   \n",
       "1  1. Summarize the text for a LinkedIn post.   \n",
       "2  1. Summarize the text for a LinkedIn post.   \n",
       "3  1. Summarize the text for a LinkedIn post.   \n",
       "\n",
       "                                           edit task  \\\n",
       "0  Once you have written your text message:     \\...   \n",
       "1  Once you have written your text message:     \\...   \n",
       "2  Once you have written your text message:     \\...   \n",
       "3  Once you have written your text message:     \\...   \n",
       "\n",
       "                             full summarization task  \\\n",
       "0  1. Summarize the text for a LinkedIn post.\\n\\n...   \n",
       "1  1. Summarize the text for a LinkedIn post.\\n\\n...   \n",
       "2  1. Summarize the text for a LinkedIn post.\\n\\n...   \n",
       "3  1. Summarize the text for a LinkedIn post.\\n\\n...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  A recent study found that middle-aged adults w...   \n",
       "1  A recent study compared the recovery response ...   \n",
       "2  A recent study found that a high calcium and h...   \n",
       "3  A recent study conducted on institutionalized ...   \n",
       "\n",
       "                                            headline  \\\n",
       "0  New Study Shows Age Does Not Affect Recovery f...   \n",
       "1  New Study Shows How Resistance Training Can He...   \n",
       "2  New Study Shows Nutritional Intervention Reduc...   \n",
       "3  New Study Shows Nutritional Intervention Reduc...   \n",
       "\n",
       "                                      simple_summary  \n",
       "0  A recent study found that middle-aged adults w...  \n",
       "1  A recent study found that participating in rec...  \n",
       "2  A recent study has found that a simple nutriti...  \n",
       "3  A recent study has found that a simple nutriti...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    created = mapped_column(TIMESTAMP(timezone=True))\n",
    "    full_template = mapped_column(Text)\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    original_headline = mapped_column(String(255))\n",
    "    simple_summary = mapped_column(Text)\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=5):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "@testing_session\n",
    "def get_session(session):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    return session\n",
    "\n",
    "\n",
    "def bulk_append(references_df, engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(references_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                reference = Sources(\n",
    "                    title=row['title'],\n",
    "                    text=row['text'],\n",
    "                    abstract=row['abstract'],\n",
    "                    publication=row['publication'],\n",
    "                    authors=row['authors'],\n",
    "                    year=row['year'],\n",
    "                    month=row['month'],\n",
    "                    pub_volume=row['pub_volume'],\n",
    "                    pub_issue=row['pub_issue'],\n",
    "                    start_page=row['start_page'],\n",
    "                    end_page=row['end_page'],\n",
    "                    doi=row['doi']\n",
    "                )\n",
    "                session.add(reference)\n",
    "                print(f'\\t{row[\"title\"]}')\n",
    "\n",
    "            references_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.text_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['date'] = datetime.now().strftime(\"%Y-%m-%d %H%M\")\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.qna['text_id'] = self.text_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model\n",
    "        self.qna['prep step'] = prep_step.strip()\n",
    "        self.qna['summarization task'] = task.strip()\n",
    "        self.qna['edit task'] = edit_task.strip()\n",
    "        self.qna['full summarization task'] = full_task.strip()\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "system_role = \"You are a helpful assistant.\"\n",
    "prep_step = [\n",
    "    # \"Tell your friend about the research in a text message.\",\n",
    "    \"In the summary, cover the following information: \\\n",
    "    \\n- Identify the key points and statistics from this text that would make interesting or helpful health content. \\\n",
    "    \\n- If available, include the effect sizes found in the research. \\\n",
    "    Otherwise, skip this step. \\\n",
    "    \\n- If applicable, get a brief description of the research participants, \\\n",
    "    such as age, sex, and health conditions. Otherwise, you can skip this step.\\\n",
    "    \\n- Think about why the general population should care about the research.\",\n",
    "]\n",
    "\n",
    "summarize_task = [\n",
    "    f\"1. Summarize the text for a LinkedIn post.\",\n",
    "]\n",
    "edit_task = [\n",
    "    \"\"\"\n",
    "    Once you have written your text message: \\\n",
    "    \\nEvaluate your text message to see if it may be confusing or redundant. \\\n",
    "    \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same. \\\n",
    "    \\n2. Create an intriguing subject line for the text.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "simplify_task = [\n",
    "    \"\"\"3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same.\\\n",
    "    Follow these steps to accomplish this: \\\n",
    "    \\na. Check if the content and language are appropriate for the audience. \\\n",
    "    \\nb. If it is suitable for the audience, keep it the same. \\\n",
    "    If not, rewrite using terms appropriate for the audience while keeping the news-worthy details. \\ \n",
    "    \\nc. Return the final version of the summary to be shown to the audience. \\\n",
    "    \\n\\nYour audience is\"\"\",\n",
    "]\n",
    "\n",
    "simplify_audience = [\n",
    "    \"people without a science background\",\n",
    "]\n",
    "\n",
    "format_task = [\n",
    "    \"\"\"4. Return your final response in a JSON format with the following format: \\\n",
    "    \\n{\"headline\": <subject line from step 2>, \\\n",
    "    \\n\"body\": <text from step 1>,\n",
    "    \\n\"audience\": <rewritten text from step 3>}\"\"\"\n",
    "]\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1\n",
    "n_choices = 2\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "save_outputs=False\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "## Add rows from references dataframe\n",
    "# bulk_append(references_df)\n",
    "\n",
    "\n",
    "# table_names = ['prompts', 'sources', 'summaries']\n",
    "# tables_dict = dict()\n",
    "\n",
    "# for table in table_names:\n",
    "#     tables_dict[table] = get_table(table=table)\n",
    "# tables_dict['sources']\n",
    "\n",
    "sources_df = get_table(table='sources', limit=2)\n",
    "\n",
    "chaining_dict = batch_summarize(\n",
    "    sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chatbot_dict,\n",
    "    system_role=system_role, model=model, max_tokens=1000,\n",
    "    n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "    iteration_id=iteration_id, save_outputs=save_outputs\n",
    "    )\n",
    "# chaining_dict[iteration_id]\n",
    "qna_dict = create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "    )\n",
    "qna_dict[iteration_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_prompt00\n",
      "\ttext_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "2_prompt00\n",
      "\ttext_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "Object saved as JSON: ../text/2023-07-11 for db//batch_Chaining_attributes_initial_2023-07-11_2135.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1_prompt00': {'text_id': 1,\n",
       "  'title': 'Comparisons in the Recovery Response From Resistance Exercise Between Young and Middle-Aged Men',\n",
       "  'text': \"Decreases in muscle mass, function, and neuromuscular activation are significant factors contributing to the decline in the quality of life during the\\xa0aging\\xa0process (21,26). However, the majority of research investigating the effect of\\xa0aging\\xa0has primarily focused on comparing adults older than 60 years with younger adult populations in their second, third, or fourth decade of life (7,16\\x9618,25). It seems that only a single study is known that has compared young and middle-aged (40\\x9659 years) adults on strength performance and recovery (28). Understanding changes in muscle function as adults move from young to middle-age may provide for a better understanding of how to minimize the significant declines in muscle function as one reaches retirement age.\\nChanges in recovery from a bout of exercise may be associated with the\\xa0aging\\xa0process (12). Recovery is often determined by the return of performance measures to baseline (BL) levels or by an attenuation of the inflammatory or\\xa0muscle damage\\xa0response to exercise (15,35). Previous research has reported a decrease in the magnitude of the immune response in older adults (e.g., >65 years) compared with younger adults after maximal graded exercise tests on a treadmill (8) and on an electrically braked cycle ergometer (27). Such a decline may be indicative of a delay in recovery because the inflammatory response is thought to play a major role in muscle remodeling and recovery (37). Decreases in muscle mass and strength associated with\\xa0aging\\xa0have been suggested to impair the recovery process after exercise, by increasing the time to adaptation or muscle repair (7,33). Previously, recovery from resistance exercise has been reported to be impaired in recreationally trained (3\\x966 hours of exercise per week) older adults (?69 years) compared with younger adults (38). Similarly, McLester et al. (28) reported a delay in recovery (e.g., in repetitions performed up to 96 hours after exercise) from a total body resistance exercise program in middle-aged men (56  5 years) compared with younger men (23  5 years). By contrast, other investigations have reported no difference in recovery between younger and older populations when assessing recovery through electrically stimulated muscle contraction (1,23). The differences between these studies are likely related to differences in the mode of exercise, training intensity, and pretraining status (1,23,28,38). Furthermore, only one of these investigations included men with\\xa0resistance training\\xa0experience (28). Considering the differences noted between younger and older adults, examining changes in recovery as one moves from young to older age may provide a better understanding regarding reducing age-associated performance decreases. Therefore, the primary purpose of this investigation was to compare the recovery response between young (18\\x9630 years) and middle-aged men (40\\x9659 years) from an acute high-volume isokinetic resistance exercise workout. A secondary purpose was to compare the effects of this workout on markers of\\xa0inflammation\\xa0and\\xa0muscle damage\\xa0during the 48-hour recovery period.\\nMethods\\nExperimental Approach to the Problem\\nA parallel study design was used to determine the effects of a single bout of high-volume isokinetic resistance exercise on changes in muscle strength, as well as markers of\\xa0muscle damage, and the inflammatory response between young (18\\x9630 years) and middle-aged adults (40\\x9659 years). Before data collection, all participants reported to the Human Performance Laboratory (HPL) for anthropometric assessment and familiarization with training and assessment protocols. Participants then returned to the HPL to perform a high-volume resistance exercise protocol. Testing was performed before and immediately after the\\xa0resistance training\\xa0protocol. Recovery assessments (both performance,\\xa0muscle damage\\xa0and inflammatory) were also assessed at 24 and 48 hours after exercise.\\nSubjects\\nNineteen recreationally trained men volunteered to participate in this investigation. Study participants were recruited and placed into 2 groups based on age. The young adult (YA) group consisted of men between the ages of 18\\x9630 years, whereas the middle-aged (MA) group consisted of men between the ages of 40\\x9659 years. None of the participants were competitive athletes, and all were recreationally resistance-trained at study enrollment. Inclusion criteria required participants to meet the age requirements of one of the groups and be recreationally active, including\\xa0resistance training\\xa0for the previous 6 months before enrollment as defined by the American College of Sports Medicine (150 minutes of exercise per week) (14). All participants were free of any physical limitations that may have affected performance and were not using any medications, dietary supplements, or any other performance-enhancing drugs before or during the study as determined by a health and activity questionnaire. After an explanation of all procedures, risks, and benefits, each participant gave his written informed consent before participation in this study. The Institutional Review Board of the University of Central Florida approved the research protocol. Experimental group characteristics are presented in\\xa0Table 1.\\nProcedures\\nBoth groups reported to the HPL on 4 separate occasions. On the first visit (D1), participants reported to the HPL after a 2-hour fast. Anthropometric assessments were performed and included height, body mass, and body composition. After anthropometric assessments, participants performed a standardized warm-up consisting of 5 minutes of pedaling on a cycle ergometer at 50 watts. After the warm-up, participants completed a familiarization protocol on the isokinetic device (S4; Biodex Medical System, Inc., New York, NY, USA). On the second visit (D2), participants arrived after a 10-hour fast and recorded their subjective levels of pain and soreness on a visual analog scale (VAS). There were at least 48 hours between the first and second visits, with no more than 7 days between these 2 visits. After the VAS, participants then provided a BL blood sample followed by an ultrasound (US) assessment of the vastus lateralis (VL) of their right leg. Participants then performed the first lower-body performance assessment protocol (BL). After a 5-minute rest period, participants completed the high-volume isokinetic resistance exercise protocol (HVP), followed immediately post (IP) by another lower-body performance assessment protocol. Blood samples were obtained at IP, 30 (30P), 60 (60P), and 120 minutes (120P) after the exercise protocol. Ultrasound assessments were obtained at IP and 120P, whereas an additional VAS assessment was completed at 30P. Participants reported to the HPL 24 (24 H) and 48 hours (48 H) after D2 for lower-body assessments. In addition, blood draws, US, and VAS measures were also obtained. The order of lower-body assessment, VAS, blood draw, and US measures were the same for all testing sessions.\\xa0Figure 1\\xa0displays the study procedures and timeline.\\nDietary Recall\\nAll participants provided a 3-day dietary recall beginning the day before D2 testing until the morning of 48 H testing. Participants were asked to maintain their regular diet for the duration of the investigation. FoodWorks nutrient analysis software (McGraw-Hill, New York, NY, USA) was used to analyze the self-reported dietary recalls for total kilocalorie intake and macronutrient distributions (carbohydrate, protein, and fat).\\nAnthropometric Assessment\\nBody mass (0.1 kg) and height (0.1 cm) were measured using a Health-o-meter Professional scale (Patient Weighing Scale, Model 500 KL; Pelstar, Alsip, IL, USA). Body composition was assessed using a multifrequency bioelectrical impedance analyzer (InBody 770; Cerritos, CA, USA) according to the manufacturer's guidelines.\\nVisual Analog Scale\\nParticipants were instructed to assess their subjective feelings of pain and soreness using a 100-mm VAS (23). Participants were asked to rate pain and soreness intensity by placing a mark on a horizontal 100-mm VAS (5,29). Verbal anchors were used at each end of the scale representing no pain or soreness and the worst possible soreness or pain. The VAS was performed at BL, IP, 30P, 24 H, and 48 H.\\nUltrasound Assessments\\nAll US assessments were performed on the VL of the right leg as previously described (39). Participants were asked to lay supine on an examination table with both legs fully extended for a minimum of 5 minutes to allow fluid shifts to occur (2). Before image collection, all anatomical locations of interest were identified using standardized landmarks for the VL muscle. The length of the VL encompassed the distance from the lateral condyle of the tibia to the most prominent point of the greater trochanter of the femur. The VL measurement required the participant to lay on their side, whereas cross-sectional area (CSA) and muscle thickness (MT) measurements were obtained from the US. The sampling location was determined by the point of intersection between the VL and 50% of the straight-line distance between the greater trochanter and the lateral epicondyle of the femur (39). After determining the desired anatomical position, a linear probe coated with transmission gel was positioned on the surface of the skin to collect the US image. For all US measurements, a 12 MHz probe (LOGIQ P5; General Electric, Wauwatosa, WI, USA) coated with water-soluble transmission gel (Aquasonic 100; Parker Laboratories, Inc., Fairfield, NJ, USA) was passed over the surface of the thigh at the predetermined anatomical locations outlined above. The depth was set at 6.0 cm, gain at 50 dB, and dynamic range at 72. Further analysis of US images was performed offline using image analysis software (ImageJ, version 1.45 s) available from the National Institutes of Health (NIH, Bethesda, MD, USA). All US images were obtained and analyzed by the same technician. Test-related reliability for all US measurements was determined by using 10 different subjects (not participants from this study) examined 24\\x9648 hours apart. Intraclass correlation coefficients and minimal differences (MD) were determined for CSA (R\\xa0= 0.99; MD = 0.85 cm2) and MT (R\\xa0= 0.97; MD = 0.09 cm).\\nIsokinetic Assessment Protocol\\nThe isokinetic assessment protocol was performed at BL, IP, 120P, 24 H, and 48 H to quantify performance decrements and recovery as a result of the HVP. Participants were seated in the isokinetic dynamometer (S4; Biodex Medical System, Inc.), positioned with a hip angle of 110 and strapped into the chair at the waist, shoulders, and across the thigh. Chair and dynamometer settings were adjusted for each participant to correctly align the axis of rotation with the lateral condyle of the femur. All participants were tested on their right leg, which was secured to the dynamometer arm just above the medial and lateral malleoli. The lever arm of the dynamometer was programmed to extend the participant's leg to 155 of knee flexion (where 180 is a full extension) and flex the participant's leg to 85 degrees of flexion. Isokinetic dynamometer settings for each individual were recorded and remained consistent throughout the study. The isokinetic assessment protocol at each time point consisted of 2 maximal voluntary isometric contractions performed at a 70 angle, 1 set of 3 repetitions of concentric knee extension at 240s?1\\xa0with passive knee flexion to starting position, and 1 set of 3 repetitions of concentric knee extension at 60s?1\\xa0with a passive knee flexion to starting position. Of the 3 maximal repetitions, the highest peak torque (PKT) and average torque (AVGT) were recorded for each speed.\\nHigh-Volume Isokinetic Resistance Exercise Protocol\\nAfter the isokinetic assessment protocol at BL on D2, the participant remained seated, and the HVP was performed. The HVP consisted of 8 sets of 10 repetitions of concentric knee extension and eccentric knee flexion at 60s?1. A 1-minute rest interval was provided between each set. Participants were encouraged to provide maximal effort throughout the HVP. Work performed in each set of the HVP was calculated as the product of the mean power of each kick over the time to complete the kick. Total work done was calculated as the sum of the work performed in each of the 8 sets of 10 repetitions during the HVP.\\nBlood Collection\\nBlood samples were obtained at 7 time points throughout the study (BL, IP, 30P, 60P, 120P, 24 H, and 48 H). The BL, IP, 30P, 60P, and 120P blood samples were obtained using a Teflon cannula placed in a superficial forearm vein using a 3-way stopcock with a male luer lock adapter and a plastic syringe after arrival to the HPL on D2. The cannula was maintained patent using a nonheparinized isotonic saline solution (Becton Dickinson, Franklin Lakes, NJ, USA). All blood samples were obtained after a 15-minute equilibration period with the exception of the IP blood draw. During the remaining time points (24 H and 48 H), blood was obtained by a single-use disposable needle with the participant lying in a supine position for at least 15 minutes before sampling. Blood draws during 24 H and 48 H were obtained after a 10-hour fast. A total of 20 ml of whole blood was collected at each time point in serum and K2EDTA-treated Vacutainer tubes, (Becton Dickinson). Blood in the K2EDTA tubes was immediately centrifuged at 4,000g\\xa0for 15 minutes, then placed into 1.8-ml microcentrifuge tubes, and frozen at ?80 C for later analysis. Blood in the serum tubes was allowed to clot at room temperature for 30 minutes and then subsequently centrifuged, aliquoted, and stored through the same procedure.\\nBiochemical Analyses\\nSerum concentrations of creatine kinase (CK) were analyzed with the use of a commercially available kinetic assay kit (Sekisui Diagnostics, Charlottetown, PE, Canada) as per manufacturer's instructions. C-reactive protein (CRP) and myoglobin (Mb) concentrations were obtained through commercially available enzyme-linked immunosorbent assays (CRP; R&D Systems; Minneapolis, MN, USA) (Mb: Calbiotech, Spring Valley, CA, USA), whereas interleukin-6 (IL-6) was obtained through high-sensitivity multiplex assay (R&D Systems, Minneapolis, MN, USA). To eliminate interassay variability, all samples for a particular assay were thawed once and analyzed by the same technician using a BioTek Eon spectrophotometer (BioTek, Winooski, VT). All samples were analyzed in duplicate with a mean coefficient of variation of 4.50% for CK, 7.38% for CRP, 3.55% for IL-6, and 5.03% for Mb. All biochemical assays were run as per the manufacturer's instructions.\\nStatistical Analyses\\nPerformance measures were analyzed through repeated measures analysis of covariance to control for BL differences between the groups. Changes in subjective levels of pain and soreness, as well as markers of\\xa0inflammation\\xa0and\\xa0muscle damage, were analyzed through repeated measures analysis of variance. In the event of a significant\\xa0F\\xa0value, least significant difference post hoc tests were used for pairwise comparison. Baseline performance comparisons of both groups were determined by independent t-tests. Outliers were identified when values exceeded 1.5 times the interquartile range (3). For all analyses, a criterion alpha level of ? ? 0.05 was used to determine statistical significance. Data were analyzed using SPSS v23 software (SPSS, Inc., Chicago, IL, USA). All data are reported as a mean \\xa0SD.\\nResults\\nPerformance Measures\\nIsometric Assessments\\nUnadjusted isometric performance measures are depicted in\\xa0Table 2. One participant from YA was identified as an outlier and removed from all study analyses. Baseline comparisons between YA and MA revealed that YA had significantly greater AVGT (p\\xa0= 0.043), rate of torque development at 200 ms (RTD200) (p\\xa0= 0.033), and a trend toward a greater isometric PKT (p\\xa0= 0.057) during the isometric assessment than MA. After controlling for BL, examination of the recovery response from the HVP revealed no significant group  time interactions for PKT (F\\xa0= 1.009;\\xa0p\\xa0= 0.377), AVGT (F\\xa0= 0.997;\\xa0p\\xa0= 0.402), or for RTD200 (F\\xa0= 0.225;\\xa0p\\xa0= 0.879). There were also no significant main effects of time for PKT (F\\xa0= 0.281;\\xa0p\\xa0= 0.759), AVGT (F\\xa0= 0.289;\\xa0p\\xa0= 0.833), or RTD200 (F\\xa0= 0.073;\\xa0p\\xa0= 0.974).\\nIsokinetic Assessment\\nUnadjusted isokinetic performance measures are depicted in\\xa0Table 2. No significant differences at BL were noted between the groups; however, a trend toward a greater PKT at both 240s?1\\xa0(F\\xa0= 0.081;\\xa0p\\xa0= 0.061) and 60s?1\\xa0(F\\xa0= 1.562;\\xa0p\\xa0= 0.083) was observed for YA compared with MA. After controlling for BL, no significant group  time interactions were observed for PKT at either 240s?1\\xa0(F\\xa0= 0.681;\\xa0p\\xa0= 0.503) or 60s?1\\xa0(F\\xa0= 0.450;\\xa0p\\xa0= 0.656). In addition, there were no significant main effects for time observed for PKT at either 240s?1\\xa0(F\\xa0= 0.272;\\xa0p\\xa0= 0.746) or 60s?1\\xa0(F\\xa0= 0.109;\\xa0p\\xa0= 0.910) for both groups combined.\\nNo significant difference at BL was found for AVGT at 60s?1\\xa0(F\\xa0= 0.940;\\xa0p\\xa0= 0.148), but a trend toward a greater AVGT was noted in YA at 240s?1\\xa0(F\\xa0= 0.008;\\xa0p\\xa0= 0.094). When controlling for BL, no significant group  time interactions were observed for AVGT at 240s?1\\xa0(F\\xa0= 0.897;\\xa0p\\xa0= 0.404) or 60s?1\\xa0(F\\xa0= 0.769;\\xa0p\\xa0= 0.484). In addition, no significant main effects for time were noted for AVGT at either 240s?1\\xa0(F\\xa0= 0.228;\\xa0p\\xa0= 0.762) or 60s?1\\xa0(F\\xa0= 0.155;\\xa0p\\xa0= 0.878).\\nBlood Analyses\\nChanges in CK and Mb can be found in\\xa0Figures 2\\xa0and\\xa03, respectively, whereas changes in CRP and IL-6 can be observed in\\xa0Figures 4\\xa0and\\xa05, respectively. No significant group  time interactions were observed for Mb (F\\xa0= 0.307;\\xa0p\\xa0= 0.640), CK (F\\xa0= 0.607;\\xa0p\\xa0= 0.551), CRP (F\\xa0= 0.320;\\xa0p\\xa0= 0.602), or IL-6 (F\\xa0= 0.466;\\xa0p\\xa0= 0.589). However, significant main effects for time were observed for both Mb (F\\xa0= 8.708;\\xa0p\\xa0= 0.005) and CK (F\\xa0= 8.127;\\xa0p\\xa0= 0.001). Myoglobin was significantly higher at 30P (p\\xa0= 0.002), 60P (p\\xa0= 0.001), and 120P (p\\xa0= 0.007) compared with BL, whereas CK concentrations at 24 H (p\\xa0= 0.002) and 48 H (p\\xa0= 0.006) were significantly higher than BL. Although no significant main effect for time was observed for CRP (F\\xa0= 3.042;\\xa0p\\xa0= 0.097), a significant main effect was noted for IL-6 (F\\xa0= 3.689;\\xa0p\\xa0= 0.05). Changes in IL-6 were significantly elevated from BL at 30P (p\\xa0= 0.032), 120P (p\\xa0= 0.002), 24 H (p\\xa0= 0.007), and 48 H (p\\xa0= 0.034) for both groups combined.\\nVisual Analog Scales\\nChanges in subjective feelings of pain and soreness can be observed in\\xa0Table 3. No significant group  time interactions were observed for subjective measures of pain (F\\xa0= 0.102;\\xa0p\\xa0= 0.959) or soreness (F\\xa0= 0.886;\\xa0p\\xa0= 0.455). No significant main effect for time was observed for subjective levels of pain (F\\xa0= 1.085;\\xa0p\\xa0= 0.351); however, a significant main effect for time was observed for subjective levels of muscle soreness (F\\xa0= 7.319;\\xa0p\\xa0= < 0.001) in both groups combined. Muscle soreness was significantly higher at 30P (p\\xa0= 0.001), 24 H (p\\xa0= 0.001), and 48 H (p\\xa0= 0.002) compared with BL.\\nUltrasound Assessment\\nChanges in muscle CSA and MT can be observed in\\xa0Table 3. No significant group  time interactions were observed for muscle CSA (F\\xa0= 0.246;\\xa0p\\xa00.747) or MT (F\\xa0= 0.687;\\xa0p\\xa0= 0.530). However, a significant main effect for time was observed for both CSA (F\\xa0= 13.460;\\xa0p\\xa0< 0.001) and MT (F\\xa0= 3.685;\\xa0p\\xa0= 0.028). Muscle CSA and MT were significantly greater at IP (p\\xa0< 0.001) compared with BL.\\nDiscussion\\nResults of this study indicated no differences in the recovery response between YA and MA for any of the performance measures, nor in subjective levels of muscle pain or soreness. Furthermore, no between-group differences were observed in the inflammatory or\\xa0muscle damage\\xa0response to the exercise protocol. To the best of our knowledge, this is the first study to examine differences in the recovery response from high-volume resistance exercise between recreationally trained young and middle-aged adults. Our results comparing BL performance measures seem to be in agreement with other investigations, demonstrating differences in strength measures between young (23\\x9629 years) and older adults (60\\x9665 years) (7,15,24), and between middle-aged (41\\x9642 years) and older adults (70\\x9672 years) (17,18). It does seem that the decline in strength may develop during middle age. However, recovery from exercise seems to be comparable in both younger and middle-aged adults as reflected by similar changes in performance and markers of\\xa0muscle damage\\xa0and\\xa0inflammation. Although changes in strength may occur in middle age, recovery from exercise does seem to be maintained in recreationally trained individuals.\\nThe differences observed at BL in isometric force and the rate of force development is consistent with other investigations comparing younger and older adults (16,19,36). Although differences in age between young and middle-aged adults do seem to affect BL levels of strength, it does not seem to affect recovery from exercise. This is supported by similar patterns of\\xa0muscle damage\\xa0and\\xa0inflammation\\xa0observed during the recovery period after the HVP. Elevations in Mb and CK concentrations are consistent with the expected physiological response from a bout of resistance exercise (9,34). These results also suggest that middle-aged recreationally active men who regularly engage in\\xa0resistance training\\xa0do not seem to be at increased risk for greater soreness or\\xa0muscle damage\\xa0in comparison with younger recreationally trained men.\\nSimilar response patterns were also noted in the inflammatory response for both groups. No significant elevations from BL were observed in CRP. However, significant increases in IL-6 concentrations were observed for both groups. Although CRP is an acute phase protein whose function is to activate the innate immune response and enhance phagocytosis (10), it does not seem to change after an acute bout of resistance exercise (6). Previous research has reported no significant changes in CRP concentrations after\\xa0resistance training\\xa0protocols recruiting primarily both upper-body (31) and lower-body exercises (4) in younger men. Furthermore, previous investigations have also indicated that training experience can reduce the CRP response to an acute bout of exercise (22,35). Considering that participants in both YA and MA were recreationally trained, it is likely that this training experience may have attenuated the CRP response. Physiologically, it is consistent with the lower stress and damage that occurs with repetitive training (9,35). Interleukin-6 is an inflammatory cytokine that facilitates communication for the mobilization, proliferation, and differentiation of immune cells to the site of tissue damage (6) and has been shown to increase after exercise (11). Elevations in IL-6 are related to the intensity, duration, and mode of training (11,13,32), but the literature is inconclusive regarding the IL-6 response to\\xa0resistance training. A recent study reported that a bout of high-volume resistance exercise (8 sets of 10 repetitions with 70% of maximal strength in the squat exercise) resulted in a significant elevation in IL-6 concentrations 30 min after exercise, but no changes were observed after a bout of high-intensity resistance exercise (8 sets of 3 repetitions with 90% of maximal strength in the squat exercise) in experienced, trained lifters (4). The IL-6 response observed during this investigation as a result of a bout of high-volume resistance exercise seems to be consistent with that recently reported by Bartolomei et al. (4). However, age did not seem to exacerbate nor attenuate the IL-6 response in recreationally trained participants.\\nNo differences were noted in the inflammatory or\\xa0muscle damage\\xa0response between YA and MA. As discussed earlier, this is likely related to the recreational training background of the participants. However, there were several limitations to the study that need to be acknowledged. The exercise protocol was performed on an isokinetic dynamometer, which may not have as much practical application as most\\xa0resistance training\\xa0programs use dynamic constant resistance exercises. In addition, unilateral, single-joint exercise recruited a relatively smaller muscle mass than is commonly used in most training programs. The use of this modality of exercise, which has been demonstrated to be very effective in eliciting\\xa0muscle damage\\xa0(20,30), may not have been sufficient to elicit significant performance and inflammatory changes in these recreationally trained participants. Future research should examine modes of training specific to the typical training program of the participants.\\nPractical Applications\\nThe results of this study indicate that changes in muscle performance seem to begin during middle age, even in recreationally trained individuals. However, participating in recreational\\xa0resistance training\\xa0may mitigate any alteration in the recovery response from exercise. These results should be examined in the context that recovery was investigated after a unilateral, single-joint isokinetic exercise protocol. Although this method has been previously used as an effective mode of exercise to elicit\\xa0muscle damage, it is not specific to the type of exercises typically used by recreational lifters. Future studies may wish to compare these population groups using multijoint, dynamic constant resistance exercises common to the training programs of most recreational lifters, regardless of age.\",\n",
       "  'folder': 'text/2023-07-11 for db',\n",
       "  'system_role': 'You are a helpful assistant.',\n",
       "  'temperature': 0.7,\n",
       "  'max_tokens': 1000,\n",
       "  'model': 'gpt-3.5-turbo-16k-0613',\n",
       "  'qna': {'date': '2023-07-11 2131',\n",
       "   'folder': 'text/2023-07-11 for db',\n",
       "   'text_id': 1,\n",
       "   'article_title': 'Comparisons in the Recovery Response From Resistance Exercise Between Young and Middle-Aged Men',\n",
       "   'text': \"Decreases in muscle mass, function, and neuromuscular activation are significant factors contributing to the decline in the quality of life during the\\xa0aging\\xa0process (21,26). However, the majority of research investigating the effect of\\xa0aging\\xa0has primarily focused on comparing adults older than 60 years with younger adult populations in their second, third, or fourth decade of life (7,16\\x9618,25). It seems that only a single study is known that has compared young and middle-aged (40\\x9659 years) adults on strength performance and recovery (28). Understanding changes in muscle function as adults move from young to middle-age may provide for a better understanding of how to minimize the significant declines in muscle function as one reaches retirement age.\\nChanges in recovery from a bout of exercise may be associated with the\\xa0aging\\xa0process (12). Recovery is often determined by the return of performance measures to baseline (BL) levels or by an attenuation of the inflammatory or\\xa0muscle damage\\xa0response to exercise (15,35). Previous research has reported a decrease in the magnitude of the immune response in older adults (e.g., >65 years) compared with younger adults after maximal graded exercise tests on a treadmill (8) and on an electrically braked cycle ergometer (27). Such a decline may be indicative of a delay in recovery because the inflammatory response is thought to play a major role in muscle remodeling and recovery (37). Decreases in muscle mass and strength associated with\\xa0aging\\xa0have been suggested to impair the recovery process after exercise, by increasing the time to adaptation or muscle repair (7,33). Previously, recovery from resistance exercise has been reported to be impaired in recreationally trained (3\\x966 hours of exercise per week) older adults (?69 years) compared with younger adults (38). Similarly, McLester et al. (28) reported a delay in recovery (e.g., in repetitions performed up to 96 hours after exercise) from a total body resistance exercise program in middle-aged men (56  5 years) compared with younger men (23  5 years). By contrast, other investigations have reported no difference in recovery between younger and older populations when assessing recovery through electrically stimulated muscle contraction (1,23). The differences between these studies are likely related to differences in the mode of exercise, training intensity, and pretraining status (1,23,28,38). Furthermore, only one of these investigations included men with\\xa0resistance training\\xa0experience (28). Considering the differences noted between younger and older adults, examining changes in recovery as one moves from young to older age may provide a better understanding regarding reducing age-associated performance decreases. Therefore, the primary purpose of this investigation was to compare the recovery response between young (18\\x9630 years) and middle-aged men (40\\x9659 years) from an acute high-volume isokinetic resistance exercise workout. A secondary purpose was to compare the effects of this workout on markers of\\xa0inflammation\\xa0and\\xa0muscle damage\\xa0during the 48-hour recovery period.\\nMethods\\nExperimental Approach to the Problem\\nA parallel study design was used to determine the effects of a single bout of high-volume isokinetic resistance exercise on changes in muscle strength, as well as markers of\\xa0muscle damage, and the inflammatory response between young (18\\x9630 years) and middle-aged adults (40\\x9659 years). Before data collection, all participants reported to the Human Performance Laboratory (HPL) for anthropometric assessment and familiarization with training and assessment protocols. Participants then returned to the HPL to perform a high-volume resistance exercise protocol. Testing was performed before and immediately after the\\xa0resistance training\\xa0protocol. Recovery assessments (both performance,\\xa0muscle damage\\xa0and inflammatory) were also assessed at 24 and 48 hours after exercise.\\nSubjects\\nNineteen recreationally trained men volunteered to participate in this investigation. Study participants were recruited and placed into 2 groups based on age. The young adult (YA) group consisted of men between the ages of 18\\x9630 years, whereas the middle-aged (MA) group consisted of men between the ages of 40\\x9659 years. None of the participants were competitive athletes, and all were recreationally resistance-trained at study enrollment. Inclusion criteria required participants to meet the age requirements of one of the groups and be recreationally active, including\\xa0resistance training\\xa0for the previous 6 months before enrollment as defined by the American College of Sports Medicine (150 minutes of exercise per week) (14). All participants were free of any physical limitations that may have affected performance and were not using any medications, dietary supplements, or any other performance-enhancing drugs before or during the study as determined by a health and activity questionnaire. After an explanation of all procedures, risks, and benefits, each participant gave his written informed consent before participation in this study. The Institutional Review Board of the University of Central Florida approved the research protocol. Experimental group characteristics are presented in\\xa0Table 1.\\nProcedures\\nBoth groups reported to the HPL on 4 separate occasions. On the first visit (D1), participants reported to the HPL after a 2-hour fast. Anthropometric assessments were performed and included height, body mass, and body composition. After anthropometric assessments, participants performed a standardized warm-up consisting of 5 minutes of pedaling on a cycle ergometer at 50 watts. After the warm-up, participants completed a familiarization protocol on the isokinetic device (S4; Biodex Medical System, Inc., New York, NY, USA). On the second visit (D2), participants arrived after a 10-hour fast and recorded their subjective levels of pain and soreness on a visual analog scale (VAS). There were at least 48 hours between the first and second visits, with no more than 7 days between these 2 visits. After the VAS, participants then provided a BL blood sample followed by an ultrasound (US) assessment of the vastus lateralis (VL) of their right leg. Participants then performed the first lower-body performance assessment protocol (BL). After a 5-minute rest period, participants completed the high-volume isokinetic resistance exercise protocol (HVP), followed immediately post (IP) by another lower-body performance assessment protocol. Blood samples were obtained at IP, 30 (30P), 60 (60P), and 120 minutes (120P) after the exercise protocol. Ultrasound assessments were obtained at IP and 120P, whereas an additional VAS assessment was completed at 30P. Participants reported to the HPL 24 (24 H) and 48 hours (48 H) after D2 for lower-body assessments. In addition, blood draws, US, and VAS measures were also obtained. The order of lower-body assessment, VAS, blood draw, and US measures were the same for all testing sessions.\\xa0Figure 1\\xa0displays the study procedures and timeline.\\nDietary Recall\\nAll participants provided a 3-day dietary recall beginning the day before D2 testing until the morning of 48 H testing. Participants were asked to maintain their regular diet for the duration of the investigation. FoodWorks nutrient analysis software (McGraw-Hill, New York, NY, USA) was used to analyze the self-reported dietary recalls for total kilocalorie intake and macronutrient distributions (carbohydrate, protein, and fat).\\nAnthropometric Assessment\\nBody mass (0.1 kg) and height (0.1 cm) were measured using a Health-o-meter Professional scale (Patient Weighing Scale, Model 500 KL; Pelstar, Alsip, IL, USA). Body composition was assessed using a multifrequency bioelectrical impedance analyzer (InBody 770; Cerritos, CA, USA) according to the manufacturer's guidelines.\\nVisual Analog Scale\\nParticipants were instructed to assess their subjective feelings of pain and soreness using a 100-mm VAS (23). Participants were asked to rate pain and soreness intensity by placing a mark on a horizontal 100-mm VAS (5,29). Verbal anchors were used at each end of the scale representing no pain or soreness and the worst possible soreness or pain. The VAS was performed at BL, IP, 30P, 24 H, and 48 H.\\nUltrasound Assessments\\nAll US assessments were performed on the VL of the right leg as previously described (39). Participants were asked to lay supine on an examination table with both legs fully extended for a minimum of 5 minutes to allow fluid shifts to occur (2). Before image collection, all anatomical locations of interest were identified using standardized landmarks for the VL muscle. The length of the VL encompassed the distance from the lateral condyle of the tibia to the most prominent point of the greater trochanter of the femur. The VL measurement required the participant to lay on their side, whereas cross-sectional area (CSA) and muscle thickness (MT) measurements were obtained from the US. The sampling location was determined by the point of intersection between the VL and 50% of the straight-line distance between the greater trochanter and the lateral epicondyle of the femur (39). After determining the desired anatomical position, a linear probe coated with transmission gel was positioned on the surface of the skin to collect the US image. For all US measurements, a 12 MHz probe (LOGIQ P5; General Electric, Wauwatosa, WI, USA) coated with water-soluble transmission gel (Aquasonic 100; Parker Laboratories, Inc., Fairfield, NJ, USA) was passed over the surface of the thigh at the predetermined anatomical locations outlined above. The depth was set at 6.0 cm, gain at 50 dB, and dynamic range at 72. Further analysis of US images was performed offline using image analysis software (ImageJ, version 1.45 s) available from the National Institutes of Health (NIH, Bethesda, MD, USA). All US images were obtained and analyzed by the same technician. Test-related reliability for all US measurements was determined by using 10 different subjects (not participants from this study) examined 24\\x9648 hours apart. Intraclass correlation coefficients and minimal differences (MD) were determined for CSA (R\\xa0= 0.99; MD = 0.85 cm2) and MT (R\\xa0= 0.97; MD = 0.09 cm).\\nIsokinetic Assessment Protocol\\nThe isokinetic assessment protocol was performed at BL, IP, 120P, 24 H, and 48 H to quantify performance decrements and recovery as a result of the HVP. Participants were seated in the isokinetic dynamometer (S4; Biodex Medical System, Inc.), positioned with a hip angle of 110 and strapped into the chair at the waist, shoulders, and across the thigh. Chair and dynamometer settings were adjusted for each participant to correctly align the axis of rotation with the lateral condyle of the femur. All participants were tested on their right leg, which was secured to the dynamometer arm just above the medial and lateral malleoli. The lever arm of the dynamometer was programmed to extend the participant's leg to 155 of knee flexion (where 180 is a full extension) and flex the participant's leg to 85 degrees of flexion. Isokinetic dynamometer settings for each individual were recorded and remained consistent throughout the study. The isokinetic assessment protocol at each time point consisted of 2 maximal voluntary isometric contractions performed at a 70 angle, 1 set of 3 repetitions of concentric knee extension at 240s?1\\xa0with passive knee flexion to starting position, and 1 set of 3 repetitions of concentric knee extension at 60s?1\\xa0with a passive knee flexion to starting position. Of the 3 maximal repetitions, the highest peak torque (PKT) and average torque (AVGT) were recorded for each speed.\\nHigh-Volume Isokinetic Resistance Exercise Protocol\\nAfter the isokinetic assessment protocol at BL on D2, the participant remained seated, and the HVP was performed. The HVP consisted of 8 sets of 10 repetitions of concentric knee extension and eccentric knee flexion at 60s?1. A 1-minute rest interval was provided between each set. Participants were encouraged to provide maximal effort throughout the HVP. Work performed in each set of the HVP was calculated as the product of the mean power of each kick over the time to complete the kick. Total work done was calculated as the sum of the work performed in each of the 8 sets of 10 repetitions during the HVP.\\nBlood Collection\\nBlood samples were obtained at 7 time points throughout the study (BL, IP, 30P, 60P, 120P, 24 H, and 48 H). The BL, IP, 30P, 60P, and 120P blood samples were obtained using a Teflon cannula placed in a superficial forearm vein using a 3-way stopcock with a male luer lock adapter and a plastic syringe after arrival to the HPL on D2. The cannula was maintained patent using a nonheparinized isotonic saline solution (Becton Dickinson, Franklin Lakes, NJ, USA). All blood samples were obtained after a 15-minute equilibration period with the exception of the IP blood draw. During the remaining time points (24 H and 48 H), blood was obtained by a single-use disposable needle with the participant lying in a supine position for at least 15 minutes before sampling. Blood draws during 24 H and 48 H were obtained after a 10-hour fast. A total of 20 ml of whole blood was collected at each time point in serum and K2EDTA-treated Vacutainer tubes, (Becton Dickinson). Blood in the K2EDTA tubes was immediately centrifuged at 4,000g\\xa0for 15 minutes, then placed into 1.8-ml microcentrifuge tubes, and frozen at ?80 C for later analysis. Blood in the serum tubes was allowed to clot at room temperature for 30 minutes and then subsequently centrifuged, aliquoted, and stored through the same procedure.\\nBiochemical Analyses\\nSerum concentrations of creatine kinase (CK) were analyzed with the use of a commercially available kinetic assay kit (Sekisui Diagnostics, Charlottetown, PE, Canada) as per manufacturer's instructions. C-reactive protein (CRP) and myoglobin (Mb) concentrations were obtained through commercially available enzyme-linked immunosorbent assays (CRP; R&D Systems; Minneapolis, MN, USA) (Mb: Calbiotech, Spring Valley, CA, USA), whereas interleukin-6 (IL-6) was obtained through high-sensitivity multiplex assay (R&D Systems, Minneapolis, MN, USA). To eliminate interassay variability, all samples for a particular assay were thawed once and analyzed by the same technician using a BioTek Eon spectrophotometer (BioTek, Winooski, VT). All samples were analyzed in duplicate with a mean coefficient of variation of 4.50% for CK, 7.38% for CRP, 3.55% for IL-6, and 5.03% for Mb. All biochemical assays were run as per the manufacturer's instructions.\\nStatistical Analyses\\nPerformance measures were analyzed through repeated measures analysis of covariance to control for BL differences between the groups. Changes in subjective levels of pain and soreness, as well as markers of\\xa0inflammation\\xa0and\\xa0muscle damage, were analyzed through repeated measures analysis of variance. In the event of a significant\\xa0F\\xa0value, least significant difference post hoc tests were used for pairwise comparison. Baseline performance comparisons of both groups were determined by independent t-tests. Outliers were identified when values exceeded 1.5 times the interquartile range (3). For all analyses, a criterion alpha level of ? ? 0.05 was used to determine statistical significance. Data were analyzed using SPSS v23 software (SPSS, Inc., Chicago, IL, USA). All data are reported as a mean \\xa0SD.\\nResults\\nPerformance Measures\\nIsometric Assessments\\nUnadjusted isometric performance measures are depicted in\\xa0Table 2. One participant from YA was identified as an outlier and removed from all study analyses. Baseline comparisons between YA and MA revealed that YA had significantly greater AVGT (p\\xa0= 0.043), rate of torque development at 200 ms (RTD200) (p\\xa0= 0.033), and a trend toward a greater isometric PKT (p\\xa0= 0.057) during the isometric assessment than MA. After controlling for BL, examination of the recovery response from the HVP revealed no significant group  time interactions for PKT (F\\xa0= 1.009;\\xa0p\\xa0= 0.377), AVGT (F\\xa0= 0.997;\\xa0p\\xa0= 0.402), or for RTD200 (F\\xa0= 0.225;\\xa0p\\xa0= 0.879). There were also no significant main effects of time for PKT (F\\xa0= 0.281;\\xa0p\\xa0= 0.759), AVGT (F\\xa0= 0.289;\\xa0p\\xa0= 0.833), or RTD200 (F\\xa0= 0.073;\\xa0p\\xa0= 0.974).\\nIsokinetic Assessment\\nUnadjusted isokinetic performance measures are depicted in\\xa0Table 2. No significant differences at BL were noted between the groups; however, a trend toward a greater PKT at both 240s?1\\xa0(F\\xa0= 0.081;\\xa0p\\xa0= 0.061) and 60s?1\\xa0(F\\xa0= 1.562;\\xa0p\\xa0= 0.083) was observed for YA compared with MA. After controlling for BL, no significant group  time interactions were observed for PKT at either 240s?1\\xa0(F\\xa0= 0.681;\\xa0p\\xa0= 0.503) or 60s?1\\xa0(F\\xa0= 0.450;\\xa0p\\xa0= 0.656). In addition, there were no significant main effects for time observed for PKT at either 240s?1\\xa0(F\\xa0= 0.272;\\xa0p\\xa0= 0.746) or 60s?1\\xa0(F\\xa0= 0.109;\\xa0p\\xa0= 0.910) for both groups combined.\\nNo significant difference at BL was found for AVGT at 60s?1\\xa0(F\\xa0= 0.940;\\xa0p\\xa0= 0.148), but a trend toward a greater AVGT was noted in YA at 240s?1\\xa0(F\\xa0= 0.008;\\xa0p\\xa0= 0.094). When controlling for BL, no significant group  time interactions were observed for AVGT at 240s?1\\xa0(F\\xa0= 0.897;\\xa0p\\xa0= 0.404) or 60s?1\\xa0(F\\xa0= 0.769;\\xa0p\\xa0= 0.484). In addition, no significant main effects for time were noted for AVGT at either 240s?1\\xa0(F\\xa0= 0.228;\\xa0p\\xa0= 0.762) or 60s?1\\xa0(F\\xa0= 0.155;\\xa0p\\xa0= 0.878).\\nBlood Analyses\\nChanges in CK and Mb can be found in\\xa0Figures 2\\xa0and\\xa03, respectively, whereas changes in CRP and IL-6 can be observed in\\xa0Figures 4\\xa0and\\xa05, respectively. No significant group  time interactions were observed for Mb (F\\xa0= 0.307;\\xa0p\\xa0= 0.640), CK (F\\xa0= 0.607;\\xa0p\\xa0= 0.551), CRP (F\\xa0= 0.320;\\xa0p\\xa0= 0.602), or IL-6 (F\\xa0= 0.466;\\xa0p\\xa0= 0.589). However, significant main effects for time were observed for both Mb (F\\xa0= 8.708;\\xa0p\\xa0= 0.005) and CK (F\\xa0= 8.127;\\xa0p\\xa0= 0.001). Myoglobin was significantly higher at 30P (p\\xa0= 0.002), 60P (p\\xa0= 0.001), and 120P (p\\xa0= 0.007) compared with BL, whereas CK concentrations at 24 H (p\\xa0= 0.002) and 48 H (p\\xa0= 0.006) were significantly higher than BL. Although no significant main effect for time was observed for CRP (F\\xa0= 3.042;\\xa0p\\xa0= 0.097), a significant main effect was noted for IL-6 (F\\xa0= 3.689;\\xa0p\\xa0= 0.05). Changes in IL-6 were significantly elevated from BL at 30P (p\\xa0= 0.032), 120P (p\\xa0= 0.002), 24 H (p\\xa0= 0.007), and 48 H (p\\xa0= 0.034) for both groups combined.\\nVisual Analog Scales\\nChanges in subjective feelings of pain and soreness can be observed in\\xa0Table 3. No significant group  time interactions were observed for subjective measures of pain (F\\xa0= 0.102;\\xa0p\\xa0= 0.959) or soreness (F\\xa0= 0.886;\\xa0p\\xa0= 0.455). No significant main effect for time was observed for subjective levels of pain (F\\xa0= 1.085;\\xa0p\\xa0= 0.351); however, a significant main effect for time was observed for subjective levels of muscle soreness (F\\xa0= 7.319;\\xa0p\\xa0= < 0.001) in both groups combined. Muscle soreness was significantly higher at 30P (p\\xa0= 0.001), 24 H (p\\xa0= 0.001), and 48 H (p\\xa0= 0.002) compared with BL.\\nUltrasound Assessment\\nChanges in muscle CSA and MT can be observed in\\xa0Table 3. No significant group  time interactions were observed for muscle CSA (F\\xa0= 0.246;\\xa0p\\xa00.747) or MT (F\\xa0= 0.687;\\xa0p\\xa0= 0.530). However, a significant main effect for time was observed for both CSA (F\\xa0= 13.460;\\xa0p\\xa0< 0.001) and MT (F\\xa0= 3.685;\\xa0p\\xa0= 0.028). Muscle CSA and MT were significantly greater at IP (p\\xa0< 0.001) compared with BL.\\nDiscussion\\nResults of this study indicated no differences in the recovery response between YA and MA for any of the performance measures, nor in subjective levels of muscle pain or soreness. Furthermore, no between-group differences were observed in the inflammatory or\\xa0muscle damage\\xa0response to the exercise protocol. To the best of our knowledge, this is the first study to examine differences in the recovery response from high-volume resistance exercise between recreationally trained young and middle-aged adults. Our results comparing BL performance measures seem to be in agreement with other investigations, demonstrating differences in strength measures between young (23\\x9629 years) and older adults (60\\x9665 years) (7,15,24), and between middle-aged (41\\x9642 years) and older adults (70\\x9672 years) (17,18). It does seem that the decline in strength may develop during middle age. However, recovery from exercise seems to be comparable in both younger and middle-aged adults as reflected by similar changes in performance and markers of\\xa0muscle damage\\xa0and\\xa0inflammation. Although changes in strength may occur in middle age, recovery from exercise does seem to be maintained in recreationally trained individuals.\\nThe differences observed at BL in isometric force and the rate of force development is consistent with other investigations comparing younger and older adults (16,19,36). Although differences in age between young and middle-aged adults do seem to affect BL levels of strength, it does not seem to affect recovery from exercise. This is supported by similar patterns of\\xa0muscle damage\\xa0and\\xa0inflammation\\xa0observed during the recovery period after the HVP. Elevations in Mb and CK concentrations are consistent with the expected physiological response from a bout of resistance exercise (9,34). These results also suggest that middle-aged recreationally active men who regularly engage in\\xa0resistance training\\xa0do not seem to be at increased risk for greater soreness or\\xa0muscle damage\\xa0in comparison with younger recreationally trained men.\\nSimilar response patterns were also noted in the inflammatory response for both groups. No significant elevations from BL were observed in CRP. However, significant increases in IL-6 concentrations were observed for both groups. Although CRP is an acute phase protein whose function is to activate the innate immune response and enhance phagocytosis (10), it does not seem to change after an acute bout of resistance exercise (6). Previous research has reported no significant changes in CRP concentrations after\\xa0resistance training\\xa0protocols recruiting primarily both upper-body (31) and lower-body exercises (4) in younger men. Furthermore, previous investigations have also indicated that training experience can reduce the CRP response to an acute bout of exercise (22,35). Considering that participants in both YA and MA were recreationally trained, it is likely that this training experience may have attenuated the CRP response. Physiologically, it is consistent with the lower stress and damage that occurs with repetitive training (9,35). Interleukin-6 is an inflammatory cytokine that facilitates communication for the mobilization, proliferation, and differentiation of immune cells to the site of tissue damage (6) and has been shown to increase after exercise (11). Elevations in IL-6 are related to the intensity, duration, and mode of training (11,13,32), but the literature is inconclusive regarding the IL-6 response to\\xa0resistance training. A recent study reported that a bout of high-volume resistance exercise (8 sets of 10 repetitions with 70% of maximal strength in the squat exercise) resulted in a significant elevation in IL-6 concentrations 30 min after exercise, but no changes were observed after a bout of high-intensity resistance exercise (8 sets of 3 repetitions with 90% of maximal strength in the squat exercise) in experienced, trained lifters (4). The IL-6 response observed during this investigation as a result of a bout of high-volume resistance exercise seems to be consistent with that recently reported by Bartolomei et al. (4). However, age did not seem to exacerbate nor attenuate the IL-6 response in recreationally trained participants.\\nNo differences were noted in the inflammatory or\\xa0muscle damage\\xa0response between YA and MA. As discussed earlier, this is likely related to the recreational training background of the participants. However, there were several limitations to the study that need to be acknowledged. The exercise protocol was performed on an isokinetic dynamometer, which may not have as much practical application as most\\xa0resistance training\\xa0programs use dynamic constant resistance exercises. In addition, unilateral, single-joint exercise recruited a relatively smaller muscle mass than is commonly used in most training programs. The use of this modality of exercise, which has been demonstrated to be very effective in eliciting\\xa0muscle damage\\xa0(20,30), may not have been sufficient to elicit significant performance and inflammatory changes in these recreationally trained participants. Future research should examine modes of training specific to the typical training program of the participants.\\nPractical Applications\\nThe results of this study indicate that changes in muscle performance seem to begin during middle age, even in recreationally trained individuals. However, participating in recreational\\xa0resistance training\\xa0may mitigate any alteration in the recovery response from exercise. These results should be examined in the context that recovery was investigated after a unilateral, single-joint isokinetic exercise protocol. Although this method has been previously used as an effective mode of exercise to elicit\\xa0muscle damage, it is not specific to the type of exercises typically used by recreational lifters. Future studies may wish to compare these population groups using multijoint, dynamic constant resistance exercises common to the training programs of most recreational lifters, regardless of age.\",\n",
       "   'system_role': 'You are a helpful assistant.',\n",
       "   'model': 'gpt-3.5-turbo-16k-0613',\n",
       "   'prep step': 'In the summary, cover the following information:     \\n- Identify the key points and statistics from this text that would make interesting or helpful health content.     \\n- If available, include the effect sizes found in the research.     Otherwise, skip this step.     \\n- If applicable, get a brief description of the research participants,     such as age, sex, and health conditions. Otherwise, you can skip this step.    \\n- Think about why the general population should care about the research.',\n",
       "   'summarization task': '1. Summarize the text for a LinkedIn post.',\n",
       "   'edit task': 'Once you have written your text message:     \\nEvaluate your text message to see if it may be confusing or redundant.     \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same.     \\n2. Create an intriguing subject line for the text.',\n",
       "   'full summarization task': '1. Summarize the text for a LinkedIn post.\\n\\nIn the summary, cover the following information:     \\n- Identify the key points and statistics from this text that would make interesting or helpful health content.     \\n- If available, include the effect sizes found in the research.     Otherwise, skip this step.     \\n- If applicable, get a brief description of the research participants,     such as age, sex, and health conditions. Otherwise, you can skip this step.    \\n- Think about why the general population should care about the research.\\n\\n\\n    Once you have written your text message:     \\nEvaluate your text message to see if it may be confusing or redundant.     \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same.     \\n2. Create an intriguing subject line for the text.\\n    \\n\\n3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same.    Follow these steps to accomplish this:     \\na. Check if the content and language are appropriate for the audience.     \\nb. If it is suitable for the audience, keep it the same.     If not, rewrite using terms appropriate for the audience while keeping the news-worthy details. \\\\ \\n    \\nc. Return the final version of the summary to be shown to the audience.     \\n\\nYour audience is people without a science background\\n\\n4. Return your final response in a JSON format with the following format:     \\n{\"headline\": <subject line from step 2>,     \\n\"body\": <text from step 1>,\\n    \\n\"audience\": <rewritten text from step 3>}',\n",
       "   'summary': ['{\\n  \"headline\": \"New Study Shows Age Does Not Affect Recovery from Exercise in Recreational Trainers\",\\n  \"body\": \"A recent study found that middle-aged adults who regularly engage in resistance training have similar recovery responses to exercise as younger adults. The study compared the recovery of young and middle-aged men after a high-volume resistance exercise workout and found no significant differences in muscle performance, inflammation, or muscle damage between the two groups. This suggests that middle-aged individuals who participate in recreational resistance training can maintain their recovery abilities. These findings are important for anyone interested in staying active and minimizing age-related declines in muscle function.\",\\n  \"audience\": \"A recent study found that middle-aged adults who regularly engage in resistance training have similar recovery responses to exercise as younger adults. The study compared the recovery of young and middle-aged men after a high-volume resistance exercise workout and found no significant differences in muscle performance, inflammation, or muscle damage between the two groups. This suggests that middle-aged individuals who participate in recreational resistance training can maintain their recovery abilities. These findings are important for anyone interested in staying active and minimizing age-related declines in muscle function.\"\\n}',\n",
       "    '{\"headline\": \"New Study Shows How Resistance Training Can Help Maintain Muscle Function as You Age\",\\n\"body\": \"A recent study compared the recovery response from high-volume resistance exercise between young and middle-aged adults. The results showed no differences in the recovery response between the two groups, indicating that participating in recreational resistance training may help mitigate the decline in muscle function that occurs with age. This is important for the general population to know because it highlights the benefits of resistance training in maintaining muscle health as you get older.\",\\n\"audience\": \"A recent study found that participating in recreational resistance training can help maintain muscle function as you age. This is important for everyone to know because it shows that exercise can have a positive impact on muscle health as you get older.\"}']},\n",
       "  'summaries_dict': {'response_01': '{\\n  \"headline\": \"New Study Shows Age Does Not Affect Recovery from Exercise in Recreational Trainers\",\\n  \"body\": \"A recent study found that middle-aged adults who regularly engage in resistance training have similar recovery responses to exercise as younger adults. The study compared the recovery of young and middle-aged men after a high-volume resistance exercise workout and found no significant differences in muscle performance, inflammation, or muscle damage between the two groups. This suggests that middle-aged individuals who participate in recreational resistance training can maintain their recovery abilities. These findings are important for anyone interested in staying active and minimizing age-related declines in muscle function.\",\\n  \"audience\": \"A recent study found that middle-aged adults who regularly engage in resistance training have similar recovery responses to exercise as younger adults. The study compared the recovery of young and middle-aged men after a high-volume resistance exercise workout and found no significant differences in muscle performance, inflammation, or muscle damage between the two groups. This suggests that middle-aged individuals who participate in recreational resistance training can maintain their recovery abilities. These findings are important for anyone interested in staying active and minimizing age-related declines in muscle function.\"\\n}',\n",
       "   'response_02': '{\"headline\": \"New Study Shows How Resistance Training Can Help Maintain Muscle Function as You Age\",\\n\"body\": \"A recent study compared the recovery response from high-volume resistance exercise between young and middle-aged adults. The results showed no differences in the recovery response between the two groups, indicating that participating in recreational resistance training may help mitigate the decline in muscle function that occurs with age. This is important for the general population to know because it highlights the benefits of resistance training in maintaining muscle health as you get older.\",\\n\"audience\": \"A recent study found that participating in recreational resistance training can help maintain muscle function as you age. This is important for everyone to know because it shows that exercise can have a positive impact on muscle health as you get older.\"}'},\n",
       "  'article_title': 'Decreases in muscle mass, function, and neuromuscular activation are significant factors contributing to the decline in the quality of life during the\\xa0aging\\xa0process (21,26). However, the majority of research investigating the effect of\\xa0aging\\xa0has primarily focused on comparing adults older than 60 years with younger adult populations in their second, third, or fourth decade of life (7,16\\x9618,25). It seems that only a single study is known that has compared young and middle-aged (40\\x9659 years) adults on strength performance and recovery (28). Understanding changes in muscle function as adults move from young to middle-age may provide for a better understanding of how to minimize the significant declines in muscle function as one reaches retirement age.',\n",
       "  'response_regex': 'response_(.*)',\n",
       "  'simple_summary_dict': {},\n",
       "  'relevance_dict': {},\n",
       "  'n_previous_prompts': {},\n",
       "  'date_created': '2023-07-11_2135'},\n",
       " '2_prompt00': {'text_id': 2,\n",
       "  'title': 'Effect of dietary sources of calcium and protein on hip fractures and falls in older adults in residential care cluster randomised controlled trial',\n",
       "  'text': 'Longevity increases the proportion of older adults in the population. The accompanying increased prevalences of chronic illnesses, loss of musculoskeletal mass, frailty, and bone fragility increase the risk of falls and fractures.1 Loss of independence increases the number of people needing full time institutionalised care, the source of around 30% of all hip fractures in the community.2 Thus, targeting an intervention to all aged care residents is a rational approach to reducing the fracture burden in the whole community.\\nThe widespread use of antiresorptive therapy is unlikely to reduce this fracture burden because of a paucity of evidence of antifracture efficacy in people over 80 years of age, the common occurrence of adverse events, and high cost given the large numbers of people that must be treated.3 However, these people often have calcium intakes below 700 mg daily, an amount unlikely to offset obligatory loss of calcium.4 They also often have protein intakes below 1 g/kg body weight/day, predisposing to loss of lean muscle mass.5 Thus, an alternative approach is to target all institutionalised older adults with a non-pharmaceutical nutritional intervention.\\nFew studies have investigated the efficacy and safety of a nutritional approach to reduction of fracture risk in aged care residents. Chapuy and colleagues showed antifracture efficacy with pharmacological doses of calcium and vitamin D in female nursing home residents with low calcium intakes and vitamin D deficiency.6 No studies have examined the effects of protein supplementation on reduction of fracture risk, despite evidence of improved muscle function and reduced falls.7\\nConsumption of milk, yoghurt, and cheese, foods rich in calcium and protein, slows bone loss and improves insulin-like growth factor 1.89 These foods are widely available, palatable, and low cost and so are likely to be adhered to. Accordingly, we conducted a prospective, two year, cluster randomised controlled trial to test the hypothesis that achieving recommended intakes of 1300 mg/day calcium and 1 g protein/kg body weight will reduce the risk of fragility fractures and falls when targeted to institutionalised older adults replete in vitamin D but with intakes below these levels.\\nMethods\\nStudy design\\nThis two year, cluster randomised controlled trial involved recruitment of 60 residential aged care facilities housing 7195 older adults in metropolitan Melbourne and regional Victoria, Australia, between December 2013 and August 2016. To ensure similar standards of care, we recruited only facilities accredited with the Australian Aged-Care Accreditation Agency that housed predominantly ambulant residents. These facilities are similar to residential care in the UK and assisted care facilities in the US. Facilities recruited were representative of charitable, private, and religious organisations, with an even distribution of small (?50 beds), medium (51-100 beds), and large (>100 beds) facilities (supplementary figure C). The ratio of women to men and the age of residents were representative of the national average.10\\nInclusion criteria\\nRandomisation was by facility, not by individuals. For inclusion, facilities were required to provide no more than two servings of dairy foods daily, which was assessed from menu audits, as this level of provision is associated with dietary intakes of <1 g/kg body weight and 600 mg calcium daily.11 Vitamin D adequacy is maintained in residents through routine supplementation as foods are not fortified with vitamin D. We included only permanent residents in data analyses\\x97that is, we excluded data from respite residents.\\nRandomisation procedure\\nThe unit of randomisation was facility, as the intervention was delivered to all residents by the food service at each facility. Eligible facilities were randomly assigned in a 1:1 ratio to either intervention (n=30) or control (n=30), with the control facilities maintaining their existing menus. The randomisation was done with the use of a computer, with block sizes being varied according to organisation (to ensure similar procedures and policies), and was stratified by geographical location (to ensure similar socioeconomic status). A statistician independent of the study did the randomisation and provided the concealed group allocation to the principal investigator (SI) who, in turn, conveyed this allocation to the facility. SI was not involved in any data collection. An organisation may have between two and 10 facilities, and randomisation was done within an organisation.\\nConsent\\nFacility managers consented to provide de-identified details of age and sex of residents, as well as access to all incident reports including those for falls and fractures. Reporting of all incidents of any nature is a mandatory requirement of all accredited aged care providers. The accreditation agency regularly audits incident reports. Facilities are sanctioned if breaches are observed, with potential for accreditation to be revoked and government funding terminated. Falls (time, location, circumstances, and outcome/injury), fractures, and other adverse events were verified from these incident reports. These reports were maintained at all facilities. An independent medically trained person blinded to study allocation verified fractures by using hospital radiographs and radiographic reports. Residents and families were informed of the study during regular meetings. A subset of 371 residents from all facilities voluntarily consented to have dietary intake recorded, medical records reviewed, blood sampling, and measurement of body composition, bone mineral density, and bone microarchitecture performed. A next of kin consented for an additional 345 residents to allow dietary intake to be recorded and medical records reviewed.\\nIntervention\\nWe classified dairy foods by using the Australian Dietary Guidelines, with a \\x93serving\\x94 defined as 250 mL of milk, 200 g of yoghurt, and 40 g of cheese.12 Lactose-free options were provided to accommodate the few participants (<0.001%) with reported lactose intolerance. Butter, cream, and ice cream were not provided, as they contain little calcium or protein. All facilities prepared and cooked foods on site. We assigned intervention facilities a food service dietician to assist food service staff to increase the provision of dairy foods at all meals and snacks. Methods used to increase dairy foods included use of milk powder to fortify milk used in recipes and beverages. Dairy based desserts and snacks were offered in place of less nutritious foods such as cakes and biscuits. Foods provided were based on the preferences expressed by the residents at intervention sites.\\nDairy foods were provided in-kind by Fonterra International (New Zealand) and distributed by a commercial food distribution company not associated with the project (Bidfoods, Australia). Use of a single distributor ensured accurate recording of costs for all dairy foods provided, with invoices used to verify compliance data. During dietary assessments, foods and beverages were weighed on a food scale (1 g) (Sohnele Page Profi, Germany) at all facilities. During two days every three months, dieticians assessed compliance by using the validated visual estimation of plate waste, with data collected from 55 000 foods and beverages during the study.13 We used nutritional analysis software (FoodWorks, Australia) or the Australian food composition database NUTTAB 2010 to calculate nutrient intakes.\\nData monitoring\\nData safety monitoring was carried out by the Study Trial Review Board, which was provided with quarterly reports.\\nOutcomes\\nAs per the approved study protocol, all pre-specified primary and secondary outcomes have been reported. The primary outcome was time to fragility fracture. Secondary outcomes were time to fall and changes in bone morphology and biochemistry. The tertiary outcomes of all cause mortality and changes in body composition are also reported. Exploratory outcomes including quality of life and muscle function were not examined (see original and final study protocols). Fasting morning serum samples were obtained from 189 residents at baseline and 12 months for measurement of 25-hydroxy-vitamin D (baseline only), C terminal telopeptide of type 1 collagen (a measure of bone resorption), procollagen type 1 N propeptide (a measure of bone formation), parathyroid hormone (Roche Cobas E170), and insulin-like growth factor 1 (LIASON) (supplementary figure A).\\nBody composition and bone morphology were assessed at baseline and 12 months in 72 residents (supplementary figure B). Total and appendicular (arms and legs) lean mass and fat mass were determined from total body scans, and bone mineral density was measured at the lumbar spine and femoral neck using dual x-ray absorptiometry (Prodigy, GE Lunar, Madison, WI, CV=1%). Volumetric bone mineral density (the amount of bone contained within the external volume of bone, in g/cm3) was measured at the distal tibia and distal radius by using high resolution peripheral quantitative computed tomography (Scanco Medical AG, Switzerland, CV 0.5\\x964.0%).14 Cortical porosity was determined using automated image processing (StrAx1.0, Straxcorp, Melbourne, Australia).\\nBlinding and sample size\\nOnce a facility was randomised, only the principal investigator, food service research dieticians, facility managers, and food service staff were aware of the allocation. Data acquisition and analyses were carried out by staff blinded to group allocation (SP, XW, MB, AGZ, and TN). Residents were blinded to the study; permission to conduct the study was obtained from the aged-care provider and facility managers. Some of the intervention strategies were not visible\\x97for example, fortification of milk with milk powder or modification of recipes. Some residents may have been aware of some changes, such as provision of cheese and biscuits for snacks, but not the reason for the changes.\\nThe sample size was determined on the basis of a hypothesised effect size and intra-cluster correlation coefficient (r). Under the hypothesis that the intervention reduces the risk of fracture by 30%, based on previous antifracture calcium/vitamin D intervention in this setting, and that r ranges from 0.10 to 0.50, the sample size needed was 25 to 50 residents per facility and 25 facilities per arm to achieve the power of 80%.6 From falls data, we used an r of 0.20 to calculate the sample size.15 To account for approximately 20% annual attrition, we recruited 60 facilities with a minimum of 50 residents per facility.\\nAt the start of the study, 3980 permanent residents were living in the participating facilities. We refer to these residents as the inception cohort. Recruitment continued throughout the 24 months to ensure that the required sample size was maintained, so we included data from residents admitted to facilities that replaced initial residents lost to follow-up due to death or discharge in analyses. We refer to these residents as the replacement cohort. In total, an additional 3215 residents were admitted to facilities after the study had started. We obtained details of new residents and those lost to follow-up from admission and discharge records from each facility.\\nAnalyses\\nWe expressed baseline data as mean and standard deviation, with the unit of analysis being clusters. We expressed fracture incidence, falls, and deaths per 100 person years of follow-up. We used the product limit (Kaplan-Meier) method to determine the cumulative risk of an event. No data were missing for these primary and secondary outcomes. The duration of follow-up was based on date of study entry to date of an event. When no event occurred, duration of follow-up was date of study entry to date of study termination.\\nAs individuals were \\x93nested\\x94 within clusters (facilities), the primary analysis was based on the mixed effects Cox\\x92s proportional hazards model; effects of intervention, age, and sex were fixed effects, and the facility was considered the random effect (see supplementary methods for additional statistical analysis). We expressed the results as a hazard ratio with 95% confidence limits. We used the \\x93coxme\\x94 package to estimate model parameters. We also used the Fine-Gray sub-distribution method with the \\x93cmprsk\\x94 package to do mortality competing risk analysis.\\nWe tested between group differences in serum biomarkers and measurements of body composition and bone morphology at baseline with the weighted t test, with cluster being the unit of analysis. Biomarkers were log transformed if they were not normally distributed. We analysed effects of the intervention by using the mixed effects model in which the within person change in outcome was modelled as a function of treatment or control group, time of follow-up, age, weight, and sex. All analyses used the R Statistical Environment.\\nAmendments to protocol and statistical analysis plan\\nInitially, facilities were matched only by location to account for socioeconomic status. We also accounted for organisations, as they contributed varying numbers of facilities and had different policies and procedures. We included facilities providing less than two servings of dairy food daily, as dietary assessments for all residents was not feasible. We quantified two day instead of three day diets, as this was adequate to capture regular intakes.11 We did not assess osteocalcin, as sufficient information is obtained from C terminal telopeptide of type 1 collagen and procollagen type 1 N propeptide. We used only all cause mortality as a tertiary outcome, as cardiovascular events were not obtainable and causes of death were poorly documented. We did not examine exploratory outcomes related to quality of life and muscle function, as unanticipated attrition reduced the sample size resulting in insufficient power to detect an effect of treatment. We did not include bayesian analyses and imputations, as no values were missing for falls and fracture outcomes (original and final study protocols).\\nPatient and public involvement\\nWe consulted aged care residents, providers, and food service staff after the initial feasibility study that guided the design of this intervention.16 The manuscript was read by non-academics.\\nResults\\nOf the 60 facilities, 54 completed the 24 month intervention (fig 1). One control facility and three intervention facilities withdrew after randomisation. Two intervention facilities closed at months 15 and 20, but we included the data up to the date of closure. At baseline, the two groups had comparable demographics and were vitamin D replete. Daily baseline calcium and protein intakes were 689 (SD 266) mg and 57 (16) g respectively (table 1; supplementary figure D).\\nNutritional changes\\nDairy food intake increased from 2.0 to 3.5 servings daily in the intervention facilities (fig 2). The additional dairy foods, equivalent to 250 mL of milk plus 20 g cheese or 100 g yoghurt, provided 562 (166) mg calcium, achieving 1142 (353) mg calcium daily, and 12 (6) g protein, achieving an intake of 69 (15) g (1.1 g/kg body weight) daily. In control facilities, residents\\x92 dairy intakes remained at less than two servings daily providing 700 (247) mg calcium and 58 (14) g protein (0.9 g/kg body weight) daily. No adverse gastrointestinal events related to the intervention were reported. No detectable within or between group differences in energy intake were observed during follow-up (fig 2). However, we observed group differences for the change in body weight (table 2). In absolute terms, no weight change occurred in the intervention group (0.3 (95% confidence interval ?0.8 to 1.4) kg; P=0.56). In controls, a weight loss of 1.4 (0.6 to 2.1) kg, (P<0.001) was due to a 0.3 (?0.6 to 0.0) kg decline in appendicular lean mass (P=0.03) and 0.8 kg (?1.6 to ?0.2) decline in total body fat mass (P=0.02).\\nFractures, falls, and mortality\\nDuring 90 557 person months of follow-up (mean 12.6 (8.9) months), 324 fractures occurred: 121 (3.7%) in the intervention group and 203 (5.2%) in controls\\x97a 33% risk reduction (hazard ratio 0.67, 95% confidence interval 0.48 to 0.93; P=0.02). Post hoc analysis indicated that the incidence of hip fracture was 1.3% (n=42) in the intervention group and 2.4% (n=93) in controls\\x97a 46% risk reduction (hazard ratio 0.54, 0.35 to 0.83; P=0.005). The separation in cumulative incidence of fractures between the groups achieved significance at five months for all fractures (P=0.02) and hip fractures (P=0.02) (fig 3). Competing risk analysis adjusted for mortality showed that the intervention was associated with average reductions in fracture risk of 27% (hazard ratio 0.73, 0.58 to 0.92) for all fractures and 44% (0.56, 0.39 to 0.82) for hip fractures.\\nThe cumulative incidence of falls was 57% (n=1879) in the intervention group and 62% (n=2423) in controls\\x97an 11% relative risk reduction (hazard ratio 0.89, 0.78 to 0.98; P=0.04). The separation in the incidence of falls between groups achieved significance at three months (P=0.04) (fig 3). All but one fracture was the result of a fall. Mortality did not differ between the intervention and control groups (27% (n=900) v 28% (n=1074), respectively; hazard ratio 1.01, 0.43 to 3.08; P=0.91) (fig 3). The numbers needing treatment to prevent any fracture, hip fracture, or a fall were 52, 82, and 17, respectively.\\nTo explore the veracity of the observations made in the entire cohort (n=7195), we examined the effects of the intervention on fracture risk and falls relative to controls in a post hoc analyses of residents present at the start of the study (n=3980, the inception cohort) and residents added after its start (n=3215, the replacement cohort), separately. The inception cohort was older than the replacement cohort (mean 86.5 (8.1) v 85.2 (8.4) years; P<0.001). However, age did not differ between the intervention and control groups in either the inception cohort (mean 86.7 (8.2) v 86.4 (8.0) years, respectively; P=0.25) or the replacement cohort (85.1 (8.5) v 85.3 (8.2) years, respectively; P=0.601). We observed significant reductions in all fractures, hip fractures, and falls in the intervention group relative to the controls in both the inception and replacement cohorts (see supplementary figure E).\\nBiochemistry and bone morphology\\nThe subgroup providing data for biochemistry, body composition, and bone morphology did not differ from the entire cohort in age (mean 85.9 (8.2) v 85.6 (8.2) years), proportion of women to men (70% (n=74) v 69% (n=57)), and proportion with previous fractures (38% (n=40) v 39 (n=32)). As shown in table 2, at 12 months, we observed a 20.4% between group difference in serum C terminal telopeptide of type 1 collagen (P=0.002), the result of no change in the intervention group and a 13.1% increase in controls (P<0.05). We observed no between group difference in procollagen type 1 N propeptide or parathyroid hormone, but we observed a 7.9% between group difference in serum insulin-like growth factor 1 (P=0.04), the result of a 5.9% increase in the intervention group (P<0.05) and no change in controls.\\nWe observed a 1.8% between group difference in spine bone mineral density (P=0.04), the result of a 2.1% increase in the intervention group (P<0.001) and no change in controls. The 1.7% between group difference in femoral neck bone mineral density was not significant (P=0.09). The 3.3% between group difference in distal radius total volumetric bone mineral density (P=0.02) and 2.0% between group difference in distal tibial total volumetric bone mineral density (P=0.07) were the result of decreases at each site in controls (both P<0.05). We observed a 4.6% between group difference in distal radial trabecular volumetric bone mineral density (P=0.03) due to a non-significant decrease in controls and a 0.7% increase in distal tibia cortical porosity in controls (P<0.05).\\nDiscussion\\nThis nutritional approach using high calcium and high protein dairy foods to increase calcium and protein intakes in institutionalised older adults replete in vitamin D was associated with a 33% reduction in risk of fractures of any type, a 46% reduction in risk of hip fractures, and an 11% reduction in risk of falls relative to controls. We found no group difference in all cause mortality.\\nMost interventions aimed at reducing fracture risk target a drug therapy to people with osteoporosis because they are at high risk of fracture. This approach confers a large benefit to the individual and does so cost effectively, because few people need to be treated to avert one event. However, averting fractures in small numbers of people at high risk does not reduce the burden of fractures in the community.\\nThe population burden of fractures\\x97the number of events, morbidity, mortality, and cost to the community\\x97arises from the vast numbers of people with risk factors that confer a modest attributable risk to the individual.17 For example, most fragility fractures in the community arise among women with osteopenia (bone mineral density T score \\x962.5 to \\x961 SD) because they form the largest segment of the community.18 Likewise, most fractures attributable to nutritional inadequacy arise among the great many people with intakes of calcium and protein that are below recommended levels.19 This nutritional inadequacy confers a small attributable risk to the individual but accounts for a large attributable fraction of the fracture burden in the community as a whole. This is the Geoffrey Rose prevention paradox\\x97a community based approach producing a small benefit to an individual may still confer a large benefit to the community.17 Safety is essential because most individuals treated may derive little or no benefit from the intervention. For example, the Dietary Approach to Stop Hypertension study reduced blood pressure by replacing a \\x93western\\x94 diet with a diet rich in fruit, vegetables, and low fat dairy foods\\x97an approach associated with fewer cardiovascular events.20\\nComparison with other studies\\nMost nutrition based studies assessing antifracture efficacy in aged care residents and people in the community used pharmacological doses of calcium with or without vitamin D.21 In a meta-analysis of 17 of these studies, only two studies reported a reduction in fracture risk\\x97a study of nursing home residents with calcium intakes <600 mg/day and vitamin D concentrations <50 nmol/L and a community based study in women and men ?65 years of age with mean calcium intakes of 700 mg/day.21 In the remaining 15 studies, poor compliance, large numbers of dropouts, and a low prevalence of the risk factor may have contributed to the null findings.2223 Benefits are unlikely if the prevalence of a risk factor (such as inadequate calcium and protein intakes) is low.24 For example, in the meta-analysis, reduction in fracture risk reported with treatment was confined to the 7272 individuals with calcium intakes <700 mg/day (risk ratio 0.80, 95% confidence interval 0.71 to 0.89), not the 45 241 individuals with calcium intakes above 700 mg daily.21 The reduction in fracture risk observed with calcium and protein rich foods in this study may have been the result of attention to several of the above factors. Compliance was optimised by supervised provision and consumption of the foods. Participants lost to follow-up were replaced by newly admitted residents. We intentionally targeted a cohort at high risk for fracture in whom low calcium and protein intakes were common and so were likely to partly contribute to the already high fracture burden in this community.\\nMechanisms of fracture risk reduction\\nThis nutritional intervention produced two unanticipated novel observations. The risk reduction for falls and fractures was detected by three and five months, respectively, and the relative risk reduction for fractures was similar to that found in trials using potent antiresorptive therapy to treat people at high risk due to osteoporosis. The two most likely explanations for each of these observations is a risk reduction for falls and slowing progression of bone fragility. Insulin-like growth factor 1 increased in the intervention group only, whereas the decrease in appendicular lean mass was confined to controls, consistent with the notion that protein intakes of 1-1.5 g/kg/day is needed to prevent protein catabolism and preserve or increase muscle mass in older adults, particularly those at risk of malnutrition or frailty.25 The increase in serum C terminal telopeptide of type 1 collagen and deterioration in tibial and radial total volumetric bone mineral density in controls was not seen in the intervention group, consistent with slowing of bone loss and slowing of microstructural deterioration.26 These changes were modest, but slowing microstructural deterioration disproportionately reduces progression of bone fragility because fragility increases as a power function to the bone loss producing it.27\\nMortality did not differ between the groups. Some, but not all, studies suggest that milk consumption is associated with increased mortality but consumption of yoghurt and cheese (fermented foods) with reduced mortality and favourable blood lipid profiles.282930 Fermented and non-fermented dairy foods were used during the intervention. Milk consumption did not differ between the intervention and control groups (data not shown).\\nLimitations of study\\nThe study has several limitations. Less than half of the participants had follow-up longer than 15 months. However, the reduction in risk of fractures and falls was detected within six months. Measures of dietary intakes and causes of secondary osteoporosis were obtained from the subgroup of 716 consented participants, not all 7195 residents, so compliance was monitored in about 10% of residents. However, recorded intakes of 55 000 foods and beverages are likely to be representative of all residents as most, if not all, food is provided by facilities. Assessment of body composition, bone morphology, and biochemistry was confined to a subgroup of residents. Attrition of these participants limited the power to examine differences in body composition, bone morphology, and biochemistry between the groups. Therefore, our ability to make inferences concerning the role of this intervention in slowing microstructural deterioration and loss of muscle mass is limited. Serum parathyroid hormone remained unchanged, perhaps owing to administration of around 1100 mg of calcium throughout the day as food, not as a single supplement of elemental calcium.31 Moreover, this intervention used whole dairy foods, so any potential benefit of other components of the dairy matrix cannot be determined.\\nSummary and conclusions\\nIn summary, ageing of the population is associated with a greater number of older adults needing full time institutionalised care. These individuals are often malnourished.5 Although the risk of fracture attributable to undernutrition may be small in an individual, the large number of older adults in aged care confers a large fracture burden in the community; institutionalised people are the source of about 30% of all hip fractures.217 A high calcium and high protein nutritional intervention reduced the risk of falls and fractures. This intervention was tailored to the preferences of the residents and was successfully delivered through the food service using regular retail milk, yoghurt, and cheese incorporated into existing menus. In conclusion, this nutritional intervention has widespread implications as a public health measure for fracture prevention in the aged care setting and potentially in the wider community.',\n",
       "  'folder': 'text/2023-07-11 for db',\n",
       "  'system_role': 'You are a helpful assistant.',\n",
       "  'temperature': 0.7,\n",
       "  'max_tokens': 1000,\n",
       "  'model': 'gpt-3.5-turbo-16k-0613',\n",
       "  'qna': {'date': '2023-07-11 2131',\n",
       "   'folder': 'text/2023-07-11 for db',\n",
       "   'text_id': 2,\n",
       "   'article_title': 'Effect of dietary sources of calcium and protein on hip fractures and falls in older adults in residential care cluster randomised controlled trial',\n",
       "   'text': 'Longevity increases the proportion of older adults in the population. The accompanying increased prevalences of chronic illnesses, loss of musculoskeletal mass, frailty, and bone fragility increase the risk of falls and fractures.1 Loss of independence increases the number of people needing full time institutionalised care, the source of around 30% of all hip fractures in the community.2 Thus, targeting an intervention to all aged care residents is a rational approach to reducing the fracture burden in the whole community.\\nThe widespread use of antiresorptive therapy is unlikely to reduce this fracture burden because of a paucity of evidence of antifracture efficacy in people over 80 years of age, the common occurrence of adverse events, and high cost given the large numbers of people that must be treated.3 However, these people often have calcium intakes below 700 mg daily, an amount unlikely to offset obligatory loss of calcium.4 They also often have protein intakes below 1 g/kg body weight/day, predisposing to loss of lean muscle mass.5 Thus, an alternative approach is to target all institutionalised older adults with a non-pharmaceutical nutritional intervention.\\nFew studies have investigated the efficacy and safety of a nutritional approach to reduction of fracture risk in aged care residents. Chapuy and colleagues showed antifracture efficacy with pharmacological doses of calcium and vitamin D in female nursing home residents with low calcium intakes and vitamin D deficiency.6 No studies have examined the effects of protein supplementation on reduction of fracture risk, despite evidence of improved muscle function and reduced falls.7\\nConsumption of milk, yoghurt, and cheese, foods rich in calcium and protein, slows bone loss and improves insulin-like growth factor 1.89 These foods are widely available, palatable, and low cost and so are likely to be adhered to. Accordingly, we conducted a prospective, two year, cluster randomised controlled trial to test the hypothesis that achieving recommended intakes of 1300 mg/day calcium and 1 g protein/kg body weight will reduce the risk of fragility fractures and falls when targeted to institutionalised older adults replete in vitamin D but with intakes below these levels.\\nMethods\\nStudy design\\nThis two year, cluster randomised controlled trial involved recruitment of 60 residential aged care facilities housing 7195 older adults in metropolitan Melbourne and regional Victoria, Australia, between December 2013 and August 2016. To ensure similar standards of care, we recruited only facilities accredited with the Australian Aged-Care Accreditation Agency that housed predominantly ambulant residents. These facilities are similar to residential care in the UK and assisted care facilities in the US. Facilities recruited were representative of charitable, private, and religious organisations, with an even distribution of small (?50 beds), medium (51-100 beds), and large (>100 beds) facilities (supplementary figure C). The ratio of women to men and the age of residents were representative of the national average.10\\nInclusion criteria\\nRandomisation was by facility, not by individuals. For inclusion, facilities were required to provide no more than two servings of dairy foods daily, which was assessed from menu audits, as this level of provision is associated with dietary intakes of <1 g/kg body weight and 600 mg calcium daily.11 Vitamin D adequacy is maintained in residents through routine supplementation as foods are not fortified with vitamin D. We included only permanent residents in data analyses\\x97that is, we excluded data from respite residents.\\nRandomisation procedure\\nThe unit of randomisation was facility, as the intervention was delivered to all residents by the food service at each facility. Eligible facilities were randomly assigned in a 1:1 ratio to either intervention (n=30) or control (n=30), with the control facilities maintaining their existing menus. The randomisation was done with the use of a computer, with block sizes being varied according to organisation (to ensure similar procedures and policies), and was stratified by geographical location (to ensure similar socioeconomic status). A statistician independent of the study did the randomisation and provided the concealed group allocation to the principal investigator (SI) who, in turn, conveyed this allocation to the facility. SI was not involved in any data collection. An organisation may have between two and 10 facilities, and randomisation was done within an organisation.\\nConsent\\nFacility managers consented to provide de-identified details of age and sex of residents, as well as access to all incident reports including those for falls and fractures. Reporting of all incidents of any nature is a mandatory requirement of all accredited aged care providers. The accreditation agency regularly audits incident reports. Facilities are sanctioned if breaches are observed, with potential for accreditation to be revoked and government funding terminated. Falls (time, location, circumstances, and outcome/injury), fractures, and other adverse events were verified from these incident reports. These reports were maintained at all facilities. An independent medically trained person blinded to study allocation verified fractures by using hospital radiographs and radiographic reports. Residents and families were informed of the study during regular meetings. A subset of 371 residents from all facilities voluntarily consented to have dietary intake recorded, medical records reviewed, blood sampling, and measurement of body composition, bone mineral density, and bone microarchitecture performed. A next of kin consented for an additional 345 residents to allow dietary intake to be recorded and medical records reviewed.\\nIntervention\\nWe classified dairy foods by using the Australian Dietary Guidelines, with a \\x93serving\\x94 defined as 250 mL of milk, 200 g of yoghurt, and 40 g of cheese.12 Lactose-free options were provided to accommodate the few participants (<0.001%) with reported lactose intolerance. Butter, cream, and ice cream were not provided, as they contain little calcium or protein. All facilities prepared and cooked foods on site. We assigned intervention facilities a food service dietician to assist food service staff to increase the provision of dairy foods at all meals and snacks. Methods used to increase dairy foods included use of milk powder to fortify milk used in recipes and beverages. Dairy based desserts and snacks were offered in place of less nutritious foods such as cakes and biscuits. Foods provided were based on the preferences expressed by the residents at intervention sites.\\nDairy foods were provided in-kind by Fonterra International (New Zealand) and distributed by a commercial food distribution company not associated with the project (Bidfoods, Australia). Use of a single distributor ensured accurate recording of costs for all dairy foods provided, with invoices used to verify compliance data. During dietary assessments, foods and beverages were weighed on a food scale (1 g) (Sohnele Page Profi, Germany) at all facilities. During two days every three months, dieticians assessed compliance by using the validated visual estimation of plate waste, with data collected from 55 000 foods and beverages during the study.13 We used nutritional analysis software (FoodWorks, Australia) or the Australian food composition database NUTTAB 2010 to calculate nutrient intakes.\\nData monitoring\\nData safety monitoring was carried out by the Study Trial Review Board, which was provided with quarterly reports.\\nOutcomes\\nAs per the approved study protocol, all pre-specified primary and secondary outcomes have been reported. The primary outcome was time to fragility fracture. Secondary outcomes were time to fall and changes in bone morphology and biochemistry. The tertiary outcomes of all cause mortality and changes in body composition are also reported. Exploratory outcomes including quality of life and muscle function were not examined (see original and final study protocols). Fasting morning serum samples were obtained from 189 residents at baseline and 12 months for measurement of 25-hydroxy-vitamin D (baseline only), C terminal telopeptide of type 1 collagen (a measure of bone resorption), procollagen type 1 N propeptide (a measure of bone formation), parathyroid hormone (Roche Cobas E170), and insulin-like growth factor 1 (LIASON) (supplementary figure A).\\nBody composition and bone morphology were assessed at baseline and 12 months in 72 residents (supplementary figure B). Total and appendicular (arms and legs) lean mass and fat mass were determined from total body scans, and bone mineral density was measured at the lumbar spine and femoral neck using dual x-ray absorptiometry (Prodigy, GE Lunar, Madison, WI, CV=1%). Volumetric bone mineral density (the amount of bone contained within the external volume of bone, in g/cm3) was measured at the distal tibia and distal radius by using high resolution peripheral quantitative computed tomography (Scanco Medical AG, Switzerland, CV 0.5\\x964.0%).14 Cortical porosity was determined using automated image processing (StrAx1.0, Straxcorp, Melbourne, Australia).\\nBlinding and sample size\\nOnce a facility was randomised, only the principal investigator, food service research dieticians, facility managers, and food service staff were aware of the allocation. Data acquisition and analyses were carried out by staff blinded to group allocation (SP, XW, MB, AGZ, and TN). Residents were blinded to the study; permission to conduct the study was obtained from the aged-care provider and facility managers. Some of the intervention strategies were not visible\\x97for example, fortification of milk with milk powder or modification of recipes. Some residents may have been aware of some changes, such as provision of cheese and biscuits for snacks, but not the reason for the changes.\\nThe sample size was determined on the basis of a hypothesised effect size and intra-cluster correlation coefficient (r). Under the hypothesis that the intervention reduces the risk of fracture by 30%, based on previous antifracture calcium/vitamin D intervention in this setting, and that r ranges from 0.10 to 0.50, the sample size needed was 25 to 50 residents per facility and 25 facilities per arm to achieve the power of 80%.6 From falls data, we used an r of 0.20 to calculate the sample size.15 To account for approximately 20% annual attrition, we recruited 60 facilities with a minimum of 50 residents per facility.\\nAt the start of the study, 3980 permanent residents were living in the participating facilities. We refer to these residents as the inception cohort. Recruitment continued throughout the 24 months to ensure that the required sample size was maintained, so we included data from residents admitted to facilities that replaced initial residents lost to follow-up due to death or discharge in analyses. We refer to these residents as the replacement cohort. In total, an additional 3215 residents were admitted to facilities after the study had started. We obtained details of new residents and those lost to follow-up from admission and discharge records from each facility.\\nAnalyses\\nWe expressed baseline data as mean and standard deviation, with the unit of analysis being clusters. We expressed fracture incidence, falls, and deaths per 100 person years of follow-up. We used the product limit (Kaplan-Meier) method to determine the cumulative risk of an event. No data were missing for these primary and secondary outcomes. The duration of follow-up was based on date of study entry to date of an event. When no event occurred, duration of follow-up was date of study entry to date of study termination.\\nAs individuals were \\x93nested\\x94 within clusters (facilities), the primary analysis was based on the mixed effects Cox\\x92s proportional hazards model; effects of intervention, age, and sex were fixed effects, and the facility was considered the random effect (see supplementary methods for additional statistical analysis). We expressed the results as a hazard ratio with 95% confidence limits. We used the \\x93coxme\\x94 package to estimate model parameters. We also used the Fine-Gray sub-distribution method with the \\x93cmprsk\\x94 package to do mortality competing risk analysis.\\nWe tested between group differences in serum biomarkers and measurements of body composition and bone morphology at baseline with the weighted t test, with cluster being the unit of analysis. Biomarkers were log transformed if they were not normally distributed. We analysed effects of the intervention by using the mixed effects model in which the within person change in outcome was modelled as a function of treatment or control group, time of follow-up, age, weight, and sex. All analyses used the R Statistical Environment.\\nAmendments to protocol and statistical analysis plan\\nInitially, facilities were matched only by location to account for socioeconomic status. We also accounted for organisations, as they contributed varying numbers of facilities and had different policies and procedures. We included facilities providing less than two servings of dairy food daily, as dietary assessments for all residents was not feasible. We quantified two day instead of three day diets, as this was adequate to capture regular intakes.11 We did not assess osteocalcin, as sufficient information is obtained from C terminal telopeptide of type 1 collagen and procollagen type 1 N propeptide. We used only all cause mortality as a tertiary outcome, as cardiovascular events were not obtainable and causes of death were poorly documented. We did not examine exploratory outcomes related to quality of life and muscle function, as unanticipated attrition reduced the sample size resulting in insufficient power to detect an effect of treatment. We did not include bayesian analyses and imputations, as no values were missing for falls and fracture outcomes (original and final study protocols).\\nPatient and public involvement\\nWe consulted aged care residents, providers, and food service staff after the initial feasibility study that guided the design of this intervention.16 The manuscript was read by non-academics.\\nResults\\nOf the 60 facilities, 54 completed the 24 month intervention (fig 1). One control facility and three intervention facilities withdrew after randomisation. Two intervention facilities closed at months 15 and 20, but we included the data up to the date of closure. At baseline, the two groups had comparable demographics and were vitamin D replete. Daily baseline calcium and protein intakes were 689 (SD 266) mg and 57 (16) g respectively (table 1; supplementary figure D).\\nNutritional changes\\nDairy food intake increased from 2.0 to 3.5 servings daily in the intervention facilities (fig 2). The additional dairy foods, equivalent to 250 mL of milk plus 20 g cheese or 100 g yoghurt, provided 562 (166) mg calcium, achieving 1142 (353) mg calcium daily, and 12 (6) g protein, achieving an intake of 69 (15) g (1.1 g/kg body weight) daily. In control facilities, residents\\x92 dairy intakes remained at less than two servings daily providing 700 (247) mg calcium and 58 (14) g protein (0.9 g/kg body weight) daily. No adverse gastrointestinal events related to the intervention were reported. No detectable within or between group differences in energy intake were observed during follow-up (fig 2). However, we observed group differences for the change in body weight (table 2). In absolute terms, no weight change occurred in the intervention group (0.3 (95% confidence interval ?0.8 to 1.4) kg; P=0.56). In controls, a weight loss of 1.4 (0.6 to 2.1) kg, (P<0.001) was due to a 0.3 (?0.6 to 0.0) kg decline in appendicular lean mass (P=0.03) and 0.8 kg (?1.6 to ?0.2) decline in total body fat mass (P=0.02).\\nFractures, falls, and mortality\\nDuring 90 557 person months of follow-up (mean 12.6 (8.9) months), 324 fractures occurred: 121 (3.7%) in the intervention group and 203 (5.2%) in controls\\x97a 33% risk reduction (hazard ratio 0.67, 95% confidence interval 0.48 to 0.93; P=0.02). Post hoc analysis indicated that the incidence of hip fracture was 1.3% (n=42) in the intervention group and 2.4% (n=93) in controls\\x97a 46% risk reduction (hazard ratio 0.54, 0.35 to 0.83; P=0.005). The separation in cumulative incidence of fractures between the groups achieved significance at five months for all fractures (P=0.02) and hip fractures (P=0.02) (fig 3). Competing risk analysis adjusted for mortality showed that the intervention was associated with average reductions in fracture risk of 27% (hazard ratio 0.73, 0.58 to 0.92) for all fractures and 44% (0.56, 0.39 to 0.82) for hip fractures.\\nThe cumulative incidence of falls was 57% (n=1879) in the intervention group and 62% (n=2423) in controls\\x97an 11% relative risk reduction (hazard ratio 0.89, 0.78 to 0.98; P=0.04). The separation in the incidence of falls between groups achieved significance at three months (P=0.04) (fig 3). All but one fracture was the result of a fall. Mortality did not differ between the intervention and control groups (27% (n=900) v 28% (n=1074), respectively; hazard ratio 1.01, 0.43 to 3.08; P=0.91) (fig 3). The numbers needing treatment to prevent any fracture, hip fracture, or a fall were 52, 82, and 17, respectively.\\nTo explore the veracity of the observations made in the entire cohort (n=7195), we examined the effects of the intervention on fracture risk and falls relative to controls in a post hoc analyses of residents present at the start of the study (n=3980, the inception cohort) and residents added after its start (n=3215, the replacement cohort), separately. The inception cohort was older than the replacement cohort (mean 86.5 (8.1) v 85.2 (8.4) years; P<0.001). However, age did not differ between the intervention and control groups in either the inception cohort (mean 86.7 (8.2) v 86.4 (8.0) years, respectively; P=0.25) or the replacement cohort (85.1 (8.5) v 85.3 (8.2) years, respectively; P=0.601). We observed significant reductions in all fractures, hip fractures, and falls in the intervention group relative to the controls in both the inception and replacement cohorts (see supplementary figure E).\\nBiochemistry and bone morphology\\nThe subgroup providing data for biochemistry, body composition, and bone morphology did not differ from the entire cohort in age (mean 85.9 (8.2) v 85.6 (8.2) years), proportion of women to men (70% (n=74) v 69% (n=57)), and proportion with previous fractures (38% (n=40) v 39 (n=32)). As shown in table 2, at 12 months, we observed a 20.4% between group difference in serum C terminal telopeptide of type 1 collagen (P=0.002), the result of no change in the intervention group and a 13.1% increase in controls (P<0.05). We observed no between group difference in procollagen type 1 N propeptide or parathyroid hormone, but we observed a 7.9% between group difference in serum insulin-like growth factor 1 (P=0.04), the result of a 5.9% increase in the intervention group (P<0.05) and no change in controls.\\nWe observed a 1.8% between group difference in spine bone mineral density (P=0.04), the result of a 2.1% increase in the intervention group (P<0.001) and no change in controls. The 1.7% between group difference in femoral neck bone mineral density was not significant (P=0.09). The 3.3% between group difference in distal radius total volumetric bone mineral density (P=0.02) and 2.0% between group difference in distal tibial total volumetric bone mineral density (P=0.07) were the result of decreases at each site in controls (both P<0.05). We observed a 4.6% between group difference in distal radial trabecular volumetric bone mineral density (P=0.03) due to a non-significant decrease in controls and a 0.7% increase in distal tibia cortical porosity in controls (P<0.05).\\nDiscussion\\nThis nutritional approach using high calcium and high protein dairy foods to increase calcium and protein intakes in institutionalised older adults replete in vitamin D was associated with a 33% reduction in risk of fractures of any type, a 46% reduction in risk of hip fractures, and an 11% reduction in risk of falls relative to controls. We found no group difference in all cause mortality.\\nMost interventions aimed at reducing fracture risk target a drug therapy to people with osteoporosis because they are at high risk of fracture. This approach confers a large benefit to the individual and does so cost effectively, because few people need to be treated to avert one event. However, averting fractures in small numbers of people at high risk does not reduce the burden of fractures in the community.\\nThe population burden of fractures\\x97the number of events, morbidity, mortality, and cost to the community\\x97arises from the vast numbers of people with risk factors that confer a modest attributable risk to the individual.17 For example, most fragility fractures in the community arise among women with osteopenia (bone mineral density T score \\x962.5 to \\x961 SD) because they form the largest segment of the community.18 Likewise, most fractures attributable to nutritional inadequacy arise among the great many people with intakes of calcium and protein that are below recommended levels.19 This nutritional inadequacy confers a small attributable risk to the individual but accounts for a large attributable fraction of the fracture burden in the community as a whole. This is the Geoffrey Rose prevention paradox\\x97a community based approach producing a small benefit to an individual may still confer a large benefit to the community.17 Safety is essential because most individuals treated may derive little or no benefit from the intervention. For example, the Dietary Approach to Stop Hypertension study reduced blood pressure by replacing a \\x93western\\x94 diet with a diet rich in fruit, vegetables, and low fat dairy foods\\x97an approach associated with fewer cardiovascular events.20\\nComparison with other studies\\nMost nutrition based studies assessing antifracture efficacy in aged care residents and people in the community used pharmacological doses of calcium with or without vitamin D.21 In a meta-analysis of 17 of these studies, only two studies reported a reduction in fracture risk\\x97a study of nursing home residents with calcium intakes <600 mg/day and vitamin D concentrations <50 nmol/L and a community based study in women and men ?65 years of age with mean calcium intakes of 700 mg/day.21 In the remaining 15 studies, poor compliance, large numbers of dropouts, and a low prevalence of the risk factor may have contributed to the null findings.2223 Benefits are unlikely if the prevalence of a risk factor (such as inadequate calcium and protein intakes) is low.24 For example, in the meta-analysis, reduction in fracture risk reported with treatment was confined to the 7272 individuals with calcium intakes <700 mg/day (risk ratio 0.80, 95% confidence interval 0.71 to 0.89), not the 45 241 individuals with calcium intakes above 700 mg daily.21 The reduction in fracture risk observed with calcium and protein rich foods in this study may have been the result of attention to several of the above factors. Compliance was optimised by supervised provision and consumption of the foods. Participants lost to follow-up were replaced by newly admitted residents. We intentionally targeted a cohort at high risk for fracture in whom low calcium and protein intakes were common and so were likely to partly contribute to the already high fracture burden in this community.\\nMechanisms of fracture risk reduction\\nThis nutritional intervention produced two unanticipated novel observations. The risk reduction for falls and fractures was detected by three and five months, respectively, and the relative risk reduction for fractures was similar to that found in trials using potent antiresorptive therapy to treat people at high risk due to osteoporosis. The two most likely explanations for each of these observations is a risk reduction for falls and slowing progression of bone fragility. Insulin-like growth factor 1 increased in the intervention group only, whereas the decrease in appendicular lean mass was confined to controls, consistent with the notion that protein intakes of 1-1.5 g/kg/day is needed to prevent protein catabolism and preserve or increase muscle mass in older adults, particularly those at risk of malnutrition or frailty.25 The increase in serum C terminal telopeptide of type 1 collagen and deterioration in tibial and radial total volumetric bone mineral density in controls was not seen in the intervention group, consistent with slowing of bone loss and slowing of microstructural deterioration.26 These changes were modest, but slowing microstructural deterioration disproportionately reduces progression of bone fragility because fragility increases as a power function to the bone loss producing it.27\\nMortality did not differ between the groups. Some, but not all, studies suggest that milk consumption is associated with increased mortality but consumption of yoghurt and cheese (fermented foods) with reduced mortality and favourable blood lipid profiles.282930 Fermented and non-fermented dairy foods were used during the intervention. Milk consumption did not differ between the intervention and control groups (data not shown).\\nLimitations of study\\nThe study has several limitations. Less than half of the participants had follow-up longer than 15 months. However, the reduction in risk of fractures and falls was detected within six months. Measures of dietary intakes and causes of secondary osteoporosis were obtained from the subgroup of 716 consented participants, not all 7195 residents, so compliance was monitored in about 10% of residents. However, recorded intakes of 55 000 foods and beverages are likely to be representative of all residents as most, if not all, food is provided by facilities. Assessment of body composition, bone morphology, and biochemistry was confined to a subgroup of residents. Attrition of these participants limited the power to examine differences in body composition, bone morphology, and biochemistry between the groups. Therefore, our ability to make inferences concerning the role of this intervention in slowing microstructural deterioration and loss of muscle mass is limited. Serum parathyroid hormone remained unchanged, perhaps owing to administration of around 1100 mg of calcium throughout the day as food, not as a single supplement of elemental calcium.31 Moreover, this intervention used whole dairy foods, so any potential benefit of other components of the dairy matrix cannot be determined.\\nSummary and conclusions\\nIn summary, ageing of the population is associated with a greater number of older adults needing full time institutionalised care. These individuals are often malnourished.5 Although the risk of fracture attributable to undernutrition may be small in an individual, the large number of older adults in aged care confers a large fracture burden in the community; institutionalised people are the source of about 30% of all hip fractures.217 A high calcium and high protein nutritional intervention reduced the risk of falls and fractures. This intervention was tailored to the preferences of the residents and was successfully delivered through the food service using regular retail milk, yoghurt, and cheese incorporated into existing menus. In conclusion, this nutritional intervention has widespread implications as a public health measure for fracture prevention in the aged care setting and potentially in the wider community.',\n",
       "   'system_role': 'You are a helpful assistant.',\n",
       "   'model': 'gpt-3.5-turbo-16k-0613',\n",
       "   'prep step': 'In the summary, cover the following information:     \\n- Identify the key points and statistics from this text that would make interesting or helpful health content.     \\n- If available, include the effect sizes found in the research.     Otherwise, skip this step.     \\n- If applicable, get a brief description of the research participants,     such as age, sex, and health conditions. Otherwise, you can skip this step.    \\n- Think about why the general population should care about the research.',\n",
       "   'summarization task': '1. Summarize the text for a LinkedIn post.',\n",
       "   'edit task': 'Once you have written your text message:     \\nEvaluate your text message to see if it may be confusing or redundant.     \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same.     \\n2. Create an intriguing subject line for the text.',\n",
       "   'full summarization task': '1. Summarize the text for a LinkedIn post.\\n\\nIn the summary, cover the following information:     \\n- Identify the key points and statistics from this text that would make interesting or helpful health content.     \\n- If available, include the effect sizes found in the research.     Otherwise, skip this step.     \\n- If applicable, get a brief description of the research participants,     such as age, sex, and health conditions. Otherwise, you can skip this step.    \\n- Think about why the general population should care about the research.\\n\\n\\n    Once you have written your text message:     \\nEvaluate your text message to see if it may be confusing or redundant.     \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same.     \\n2. Create an intriguing subject line for the text.\\n    \\n\\n3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same.    Follow these steps to accomplish this:     \\na. Check if the content and language are appropriate for the audience.     \\nb. If it is suitable for the audience, keep it the same.     If not, rewrite using terms appropriate for the audience while keeping the news-worthy details. \\\\ \\n    \\nc. Return the final version of the summary to be shown to the audience.     \\n\\nYour audience is people without a science background\\n\\n4. Return your final response in a JSON format with the following format:     \\n{\"headline\": <subject line from step 2>,     \\n\"body\": <text from step 1>,\\n    \\n\"audience\": <rewritten text from step 3>}',\n",
       "   'summary': ['{\"headline\": \"New Study Shows Nutritional Intervention Reduces Fracture Risk in Older Adults\",\\n \"body\": \"A recent study found that a high calcium and high protein nutritional intervention can reduce the risk of falls and fractures in older adults. The intervention, which included increasing the intake of dairy foods, resulted in a 33% reduction in fracture risk and an 11% reduction in falls. This research was conducted with institutionalized older adults who were replete in vitamin D but had low calcium and protein intakes. The findings highlight the importance of proper nutrition in reducing the burden of fractures in the community.\",\\n \"audience\": \"A recent study has found that a simple nutritional intervention can significantly reduce the risk of falls and fractures in older adults. By increasing the intake of dairy foods, such as milk, yogurt, and cheese, the intervention resulted in a 33% reduction in fracture risk and an 11% reduction in falls. This research was conducted with older adults in aged care facilities who had low calcium and protein intakes. The findings emphasize the importance of a balanced diet in preventing fractures and improving overall health in older adults.\"}',\n",
       "    '{\"headline\": \"New Study Shows Nutritional Intervention Reduces Fracture Risk in Older Adults\",\\n\"body\": \"A recent study conducted on institutionalized older adults found that a high calcium and high protein nutritional intervention significantly reduced the risk of falls and fractures. The intervention involved increasing the intake of dairy foods, such as milk, yogurt, and cheese, which are rich in calcium and protein. The study showed a 33% reduction in the risk of fractures, including a 46% reduction in hip fractures. This research has important implications for fracture prevention in the aged care setting and potentially in the wider community.\",\\n\"audience\": \"A recent study has found that a simple nutritional intervention can significantly reduce the risk of falls and fractures in older adults. By increasing the intake of dairy foods like milk, yogurt, and cheese, which are rich in calcium and protein, the risk of fractures was reduced by 33%, including a 46% reduction in hip fractures. This research is important for anyone concerned about fracture prevention, especially in the aged care setting.\"}']},\n",
       "  'summaries_dict': {'response_01': '{\"headline\": \"New Study Shows Nutritional Intervention Reduces Fracture Risk in Older Adults\",\\n \"body\": \"A recent study found that a high calcium and high protein nutritional intervention can reduce the risk of falls and fractures in older adults. The intervention, which included increasing the intake of dairy foods, resulted in a 33% reduction in fracture risk and an 11% reduction in falls. This research was conducted with institutionalized older adults who were replete in vitamin D but had low calcium and protein intakes. The findings highlight the importance of proper nutrition in reducing the burden of fractures in the community.\",\\n \"audience\": \"A recent study has found that a simple nutritional intervention can significantly reduce the risk of falls and fractures in older adults. By increasing the intake of dairy foods, such as milk, yogurt, and cheese, the intervention resulted in a 33% reduction in fracture risk and an 11% reduction in falls. This research was conducted with older adults in aged care facilities who had low calcium and protein intakes. The findings emphasize the importance of a balanced diet in preventing fractures and improving overall health in older adults.\"}',\n",
       "   'response_02': '{\"headline\": \"New Study Shows Nutritional Intervention Reduces Fracture Risk in Older Adults\",\\n\"body\": \"A recent study conducted on institutionalized older adults found that a high calcium and high protein nutritional intervention significantly reduced the risk of falls and fractures. The intervention involved increasing the intake of dairy foods, such as milk, yogurt, and cheese, which are rich in calcium and protein. The study showed a 33% reduction in the risk of fractures, including a 46% reduction in hip fractures. This research has important implications for fracture prevention in the aged care setting and potentially in the wider community.\",\\n\"audience\": \"A recent study has found that a simple nutritional intervention can significantly reduce the risk of falls and fractures in older adults. By increasing the intake of dairy foods like milk, yogurt, and cheese, which are rich in calcium and protein, the risk of fractures was reduced by 33%, including a 46% reduction in hip fractures. This research is important for anyone concerned about fracture prevention, especially in the aged care setting.\"}'},\n",
       "  'article_title': 'Longevity increases the proportion of older adults in the population. The accompanying increased prevalences of chronic illnesses, loss of musculoskeletal mass, frailty, and bone fragility increase the risk of falls and fractures.1 Loss of independence increases the number of people needing full time institutionalised care, the source of around 30% of all hip fractures in the community.2 Thus, targeting an intervention to all aged care residents is a rational approach to reducing the fracture burden in the whole community.',\n",
       "  'response_regex': 'response_(.*)',\n",
       "  'simple_summary_dict': {},\n",
       "  'relevance_dict': {},\n",
       "  'n_previous_prompts': {},\n",
       "  'date_created': '2023-07-11_2135'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_instance_to_dict(chatbot_dict[iteration_id], description=f'batch_Chaining_attributes_initial', ext=None, json_path=folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 update time stamp; add to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-07-11 22:03:55'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytz\n",
    "datetime.now(pytz.timezone('Canada/Pacific')).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 22:05:01.330389-07:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2023-07-11 22:05:01.331881-07:00'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(datetime.now(pytz.timezone('Canada/Pacific')))\n",
    "str(datetime.now(pytz.timezone('Canada/Pacific')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from sources LIMIT 2\n",
      "**Text #1 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "**Text #2 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "1_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "2_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "An error occurred on line 193 in C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\\file_functions.py : Object of type datetime is not JSON serializable\n",
      "Unable to save chatbot dictionary to JSON\n",
      "Processing 1_prompt00...\n",
      "Processing 2_prompt00...\n",
      "Original summaries DataFrame shape: (4, 15)\n",
      "\tOriginal summaries Dataframe columns: Index(['choice', 'date', 'folder', 'reference_id', 'article_title', 'text',\n",
      "       'system_role', 'model', 'prep step', 'summarization task', 'edit task',\n",
      "       'full summarization task', 'summary', 'headline', 'simple_summary'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>folder</th>\n",
       "      <th>reference_id</th>\n",
       "      <th>choice</th>\n",
       "      <th>article_title</th>\n",
       "      <th>text</th>\n",
       "      <th>system_role</th>\n",
       "      <th>model</th>\n",
       "      <th>prep step</th>\n",
       "      <th>summarization task</th>\n",
       "      <th>edit task</th>\n",
       "      <th>full summarization task</th>\n",
       "      <th>summary</th>\n",
       "      <th>headline</th>\n",
       "      <th>simple_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-11 22:14:12.223997-07:00</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparisons in the Recovery Response From Resi...</td>\n",
       "      <td>Decreases in muscle mass, function, and neurom...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "      <td>New Study Shows Middle-Aged Adults Can Recover...</td>\n",
       "      <td>A recent study found that middle-aged adults w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-11 22:14:12.223997-07:00</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Comparisons in the Recovery Response From Resi...</td>\n",
       "      <td>Decreases in muscle mass, function, and neurom...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "      <td>New Research Shows That Middle-Aged Adults Can...</td>\n",
       "      <td>New research shows that middle-aged adults who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-11 22:14:18.041381-07:00</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Effect of dietary sources of calcium and prote...</td>\n",
       "      <td>Longevity increases the proportion of older ad...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>A recent study found that a high calcium and h...</td>\n",
       "      <td>New Study Shows Nutritional Intervention Reduc...</td>\n",
       "      <td>A recent study has shown that a simple nutriti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-11 22:14:18.041381-07:00</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Effect of dietary sources of calcium and prote...</td>\n",
       "      <td>Longevity increases the proportion of older ad...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>A recent study found that a high calcium and h...</td>\n",
       "      <td>Nutritional intervention reduces risk of falls...</td>\n",
       "      <td>A recent study showed that a simple dietary in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              date                  folder  reference_id  \\\n",
       "0 2023-07-11 22:14:12.223997-07:00  text/2023-07-11 for db             1   \n",
       "1 2023-07-11 22:14:12.223997-07:00  text/2023-07-11 for db             1   \n",
       "2 2023-07-11 22:14:18.041381-07:00  text/2023-07-11 for db             2   \n",
       "3 2023-07-11 22:14:18.041381-07:00  text/2023-07-11 for db             2   \n",
       "\n",
       "   choice                                      article_title  \\\n",
       "0       1  Comparisons in the Recovery Response From Resi...   \n",
       "1       2  Comparisons in the Recovery Response From Resi...   \n",
       "2       1  Effect of dietary sources of calcium and prote...   \n",
       "3       2  Effect of dietary sources of calcium and prote...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Decreases in muscle mass, function, and neurom...   \n",
       "1  Decreases in muscle mass, function, and neurom...   \n",
       "2  Longevity increases the proportion of older ad...   \n",
       "3  Longevity increases the proportion of older ad...   \n",
       "\n",
       "                    system_role                   model  \\\n",
       "0  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "1  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "2  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "3  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "\n",
       "                                           prep step  \\\n",
       "0  In the summary, cover the following informatio...   \n",
       "1  In the summary, cover the following informatio...   \n",
       "2  In the summary, cover the following informatio...   \n",
       "3  In the summary, cover the following informatio...   \n",
       "\n",
       "                           summarization task  \\\n",
       "0  1. Summarize the text for a LinkedIn post.   \n",
       "1  1. Summarize the text for a LinkedIn post.   \n",
       "2  1. Summarize the text for a LinkedIn post.   \n",
       "3  1. Summarize the text for a LinkedIn post.   \n",
       "\n",
       "                                           edit task  \\\n",
       "0  Once you have written your text message:     \\...   \n",
       "1  Once you have written your text message:     \\...   \n",
       "2  Once you have written your text message:     \\...   \n",
       "3  Once you have written your text message:     \\...   \n",
       "\n",
       "                             full summarization task  \\\n",
       "0  1. Summarize the text for a LinkedIn post.\\n\\n...   \n",
       "1  1. Summarize the text for a LinkedIn post.\\n\\n...   \n",
       "2  1. Summarize the text for a LinkedIn post.\\n\\n...   \n",
       "3  1. Summarize the text for a LinkedIn post.\\n\\n...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  A recent study compared the recovery response ...   \n",
       "1  A recent study compared the recovery response ...   \n",
       "2  A recent study found that a high calcium and h...   \n",
       "3  A recent study found that a high calcium and h...   \n",
       "\n",
       "                                            headline  \\\n",
       "0  New Study Shows Middle-Aged Adults Can Recover...   \n",
       "1  New Research Shows That Middle-Aged Adults Can...   \n",
       "2  New Study Shows Nutritional Intervention Reduc...   \n",
       "3  Nutritional intervention reduces risk of falls...   \n",
       "\n",
       "                                      simple_summary  \n",
       "0  A recent study found that middle-aged adults w...  \n",
       "1  New research shows that middle-aged adults who...  \n",
       "2  A recent study has shown that a simple nutriti...  \n",
       "3  A recent study showed that a simple dietary in...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    # created = mapped_column(TIMESTAMP(timezone=True))\n",
    "    full_template = mapped_column(Text)\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    original_headline = mapped_column(String(255))\n",
    "    simple_summary = mapped_column(Text)\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=5):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "@testing_session\n",
    "def get_session(session):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    return session\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries', engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    data = Sources(\n",
    "                        title=row['title'],\n",
    "                        text=row['text'],\n",
    "                        abstract=row['abstract'],\n",
    "                        publication=row['publication'],\n",
    "                        authors=row['authors'],\n",
    "                        year=row['year'],\n",
    "                        month=row['month'],\n",
    "                        pub_volume=row['pub_volume'],\n",
    "                        pub_issue=row['pub_issue'],\n",
    "                        start_page=row['start_page'],\n",
    "                        end_page=row['end_page'],\n",
    "                        doi=row['doi']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                elif table == 'summaries':\n",
    "                    # prompt = Prompts(\n",
    "                    #     full_template=row['full summarization task'],\n",
    "                    #     prep_steps=row['prep step'],\n",
    "                    #     task=row['summarization task'],\n",
    "                    #     edit_steps=row['edit task'],\n",
    "                    #     audience=row['audience']\n",
    "                    # )\n",
    "\n",
    "                    # SH 2023-07-11 22:09: Add code to check if prompt already exists in database\n",
    "                    # If it does, get the prompt_id and add it to the summary row\n",
    "                    # If it doesn't, add the prompt to the database and get the prompt_id\n",
    "                    # Add the prompt_id to the summary row\n",
    "                    # Add the summary row to the database\n",
    "\n",
    "                    # Check if prompt already exists in the database\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full summarization task'],\n",
    "                        # prep_steps=row['prep step'],\n",
    "                        # task=row['summarization task'],\n",
    "                        # edit_steps=row['edit task'],\n",
    "                        # audience=row['audience']\n",
    "                        ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full summarization task'],\n",
    "                            prep_steps=row['prep step'],\n",
    "                            task=row['summarization task'],\n",
    "                            edit_steps=row['edit task'],\n",
    "                            audience=row['audience']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['date'],\n",
    "                        original_summary=row['summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                print(f'\\t{row[\"title\"]}')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.reference_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['date'] = datetime.now(pytz.timezone('Canada/Pacific'))\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.qna['reference_id'] = self.reference_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model\n",
    "        self.qna['prep step'] = prep_step.strip()\n",
    "        self.qna['summarization task'] = task.strip()\n",
    "        self.qna['edit task'] = edit_task.strip()\n",
    "        self.qna['full summarization task'] = full_task.strip()\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "system_role = \"You are a helpful assistant.\"\n",
    "prep_step = [\n",
    "    # \"Tell your friend about the research in a text message.\",\n",
    "    \"In the summary, cover the following information: \\\n",
    "    \\n- Identify the key points and statistics from this text that would make interesting or helpful health content. \\\n",
    "    \\n- If available, include the effect sizes found in the research. \\\n",
    "    Otherwise, skip this step. \\\n",
    "    \\n- If applicable, get a brief description of the research participants, \\\n",
    "    such as age, sex, and health conditions. Otherwise, you can skip this step.\\\n",
    "    \\n- Think about why the general population should care about the research.\",\n",
    "]\n",
    "\n",
    "summarize_task = [\n",
    "    f\"1. Summarize the text for a LinkedIn post.\",\n",
    "]\n",
    "edit_task = [\n",
    "    \"\"\"\n",
    "    Once you have written your text message: \\\n",
    "    \\nEvaluate your text message to see if it may be confusing or redundant. \\\n",
    "    \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same. \\\n",
    "    \\n2. Create an intriguing subject line for the text.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "simplify_task = [\n",
    "    \"\"\"3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same.\\\n",
    "    Follow these steps to accomplish this: \\\n",
    "    \\na. Check if the content and language are appropriate for the audience. \\\n",
    "    \\nb. If it is suitable for the audience, keep it the same. \\\n",
    "    If not, rewrite using terms appropriate for the audience while keeping the news-worthy details. \\ \n",
    "    \\nc. Return the final version of the summary to be shown to the audience. \\\n",
    "    \\n\\nYour audience is\"\"\",\n",
    "]\n",
    "\n",
    "simplify_audience = [\n",
    "    \"people without a science background\",\n",
    "]\n",
    "\n",
    "format_task = [\n",
    "    \"\"\"4. Return your final response in a JSON format with the following format: \\\n",
    "    \\n{\"headline\": <subject line from step 2>, \\\n",
    "    \\n\"body\": <text from step 1>,\n",
    "    \\n\"audience\": <rewritten text from step 3>}\"\"\"\n",
    "]\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1.2\n",
    "n_choices = 2\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "save_outputs=True\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "## Add rows from references dataframe\n",
    "# bulk_append(references_df)\n",
    "\n",
    "\n",
    "# table_names = ['prompts', 'sources', 'summaries']\n",
    "# tables_dict = dict()\n",
    "\n",
    "# for table in table_names:\n",
    "#     tables_dict[table] = get_table(table=table)\n",
    "# tables_dict['sources']\n",
    "\n",
    "sources_df = get_table(table='sources', limit=2)\n",
    "\n",
    "chaining_dict = batch_summarize(\n",
    "    sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chatbot_dict,\n",
    "    system_role=system_role, model=model, max_tokens=1000,\n",
    "    n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "    iteration_id=iteration_id, save_outputs=save_outputs\n",
    "    )\n",
    "# chaining_dict[iteration_id]\n",
    "qna_dict = create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "    )\n",
    "qna_dict[iteration_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 4 rows to the database...\n",
      "Error adding data to the database: 'audience'\n"
     ]
    }
   ],
   "source": [
    "bulk_append(qna_dict[iteration_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 4 rows to the database...\n",
      "Error adding data to the database: 'simple summary'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    # created = mapped_column(TIMESTAMP(timezone=True))\n",
    "    full_template = mapped_column(Text)\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    original_headline = mapped_column(String(255))\n",
    "    simple_summary = mapped_column(Text)\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=5):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "@testing_session\n",
    "def get_session(session):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    return session\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries', engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    data = Sources(\n",
    "                        title=row['title'],\n",
    "                        text=row['text'],\n",
    "                        abstract=row['abstract'],\n",
    "                        publication=row['publication'],\n",
    "                        authors=row['authors'],\n",
    "                        year=row['year'],\n",
    "                        month=row['month'],\n",
    "                        pub_volume=row['pub_volume'],\n",
    "                        pub_issue=row['pub_issue'],\n",
    "                        start_page=row['start_page'],\n",
    "                        end_page=row['end_page'],\n",
    "                        doi=row['doi']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                elif table == 'summaries':\n",
    "\n",
    "                    # Check if prompt already exists in the database\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full summarization task'],\n",
    "                        # prep_steps=row['prep step'],\n",
    "                        # task=row['summarization task'],\n",
    "                        # edit_steps=row['edit task'],\n",
    "                        # audience=row['audience']\n",
    "                        ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full summarization task'],\n",
    "                            prep_steps=row['prep step'],\n",
    "                            task=row['summarization task'],\n",
    "                            edit_steps=row['edit task'],\n",
    "                            audience=row['simple summary']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                print(f'\\t{row[\"title\"]}')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.reference_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['timestamp'] = datetime.now(pytz.timezone('Canada/Pacific'))\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.qna['reference_id'] = self.reference_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model\n",
    "        self.qna['prep step'] = prep_step.strip()\n",
    "        self.qna['summarization task'] = task.strip()\n",
    "        self.qna['edit task'] = edit_task.strip()\n",
    "        self.qna['full summarization task'] = full_task.strip()\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "system_role = \"You are a helpful assistant.\"\n",
    "prep_step = [\n",
    "    # \"Tell your friend about the research in a text message.\",\n",
    "    \"In the summary, cover the following information: \\\n",
    "    \\n- Identify the key points and statistics from this text that would make interesting or helpful health content. \\\n",
    "    \\n- If available, include the effect sizes found in the research. \\\n",
    "    Otherwise, skip this step. \\\n",
    "    \\n- If applicable, get a brief description of the research participants, \\\n",
    "    such as age, sex, and health conditions. Otherwise, you can skip this step.\\\n",
    "    \\n- Think about why the general population should care about the research.\",\n",
    "]\n",
    "\n",
    "summarize_task = [\n",
    "    f\"1. Summarize the text for a LinkedIn post.\",\n",
    "]\n",
    "edit_task = [\n",
    "    \"\"\"\n",
    "    Once you have written your text message: \\\n",
    "    \\nEvaluate your text message to see if it may be confusing or redundant. \\\n",
    "    \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same. \\\n",
    "    \\n2. Create an intriguing subject line for the text.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "simplify_task = [\n",
    "    \"\"\"3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same.\\\n",
    "    Follow these steps to accomplish this: \\\n",
    "    \\na. Check if the content and language are appropriate for the audience. \\\n",
    "    \\nb. If it is suitable for the audience, keep it the same. \\\n",
    "    If not, rewrite using terms appropriate for the audience while keeping the news-worthy details. \\ \n",
    "    \\nc. Return the final version of the summary to be shown to the audience. \\\n",
    "    \\n\\nYour audience is\"\"\",\n",
    "]\n",
    "\n",
    "simplify_audience = [\n",
    "    \"people without a science background\",\n",
    "]\n",
    "\n",
    "format_task = [\n",
    "    \"\"\"4. Return your final response in a JSON format with the following format: \\\n",
    "    \\n{\"headline\": <subject line from step 2>, \\\n",
    "    \\n\"body\": <text from step 1>,\n",
    "    \\n\"audience\": <rewritten text from step 3>}\"\"\"\n",
    "]\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1.2\n",
    "n_choices = 2\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "save_outputs=True\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "## Add rows from references dataframe\n",
    "# bulk_append(references_df)\n",
    "\n",
    "\n",
    "# sources_df = get_table(table='sources', limit=2)\n",
    "\n",
    "# chaining_dict = batch_summarize(\n",
    "#     sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "#     simplify_task, simplify_audience, format_task,\n",
    "#     chatbot_dict,\n",
    "#     system_role=system_role, model=model, max_tokens=1000,\n",
    "#     n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "#     iteration_id=iteration_id, save_outputs=save_outputs\n",
    "#     )\n",
    "# # chaining_dict[iteration_id]\n",
    "qna_dict = create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "    )\n",
    "qna_dict[iteration_id]\n",
    "\n",
    "# Add rows from results to summaries and prompts table\n",
    "bulk_append(qna_dict[iteration_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Update Chaining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from sources LIMIT 1\n",
      "**Text #1 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 1 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "1_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "An error occurred on line 193 in C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\\file_functions.py : Object of type datetime is not JSON serializable\n",
      "Unable to save chatbot dictionary to JSON\n",
      "Processing 1_prompt00...\n",
      "Original summaries DataFrame shape: (1, 18)\n",
      "\tOriginal summaries Dataframe columns: Index(['choice', 'timestamp', 'reference_id', 'article_title', 'text',\n",
      "       'system_role', 'model', 'prep_step', 'summarize_task', 'edit_task',\n",
      "       'simplify_task', 'simplify_audience', 'format_task',\n",
      "       'full_summarize task', 'folder', 'summary', 'headline',\n",
      "       'simple_summary'],\n",
      "      dtype='object')\n",
      "Adding 1 rows to the database...\n",
      "Error adding data to the database: 'full summarization task'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    # created = mapped_column(TIMESTAMP(timezone=True))\n",
    "    full_template = mapped_column(Text)\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    original_headline = mapped_column(String(255))\n",
    "    simple_summary = mapped_column(Text)\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=5):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "@testing_session\n",
    "def get_session(session):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    return session\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries', engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    data = Sources(\n",
    "                        title=row['title'],\n",
    "                        text=row['text'],\n",
    "                        abstract=row['abstract'],\n",
    "                        publication=row['publication'],\n",
    "                        authors=row['authors'],\n",
    "                        year=row['year'],\n",
    "                        month=row['month'],\n",
    "                        pub_volume=row['pub_volume'],\n",
    "                        pub_issue=row['pub_issue'],\n",
    "                        start_page=row['start_page'],\n",
    "                        end_page=row['end_page'],\n",
    "                        doi=row['doi']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                elif table == 'summaries':\n",
    "\n",
    "                    # Check if prompt already exists in the database\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full summarization task'],\n",
    "                        ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['ssummarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                print(f'\\t{row[\"title\"]}')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.reference_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['timestamp'] = datetime.now(pytz.timezone('Canada/Pacific'))\n",
    "        self.qna['reference_id'] = self.reference_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model\n",
    "        self.qna['prep_step'] = prep_step\n",
    "        self.qna['summarize_task'] = task\n",
    "        self.qna['edit_task'] = edit_task\n",
    "        self.qna['simplify_task'] = simplify_task\n",
    "        self.qna['simplify_audience'] = simplify_audience\n",
    "        self.qna['format_task'] = format_task\n",
    "        self.qna['full_summarize task'] = full_task\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "system_role = \"You are a helpful assistant.\"\n",
    "prep_step = [\n",
    "    # \"Tell your friend about the research in a text message.\",\n",
    "    \"In the summary, cover the following information: \\\n",
    "    \\n- Identify the key points and statistics from this text that would make interesting or helpful health content. \\\n",
    "    \\n- If available, include the effect sizes found in the research. \\\n",
    "    Otherwise, skip this step. \\\n",
    "    \\n- If applicable, get a brief description of the research participants, \\\n",
    "    such as age, sex, and health conditions. Otherwise, you can skip this step.\\\n",
    "    \\n- Think about why the general population should care about the research.\",\n",
    "]\n",
    "\n",
    "summarize_task = [\n",
    "    f\"1. Summarize the text for a LinkedIn post.\",\n",
    "]\n",
    "edit_task = [\n",
    "    \"\"\"\n",
    "    Once you have written your text message: \\\n",
    "    \\nEvaluate your text message to see if it may be confusing or redundant. \\\n",
    "    \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same. \\\n",
    "    \\n2. Create an intriguing subject line for the text.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "simplify_task = [\n",
    "    \"\"\"3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same.\\\n",
    "    Follow these steps to accomplish this: \\\n",
    "    \\na. Check if the content and language are appropriate for the audience. \\\n",
    "    \\nb. If it is suitable for the audience, keep it the same. \\\n",
    "    If not, rewrite using terms appropriate for the audience while keeping the news-worthy details. \\ \n",
    "    \\nc. Return the final version of the summary to be shown to the audience. \\\n",
    "    \\n\\nYour audience is\"\"\",\n",
    "]\n",
    "\n",
    "simplify_audience = [\n",
    "    \"people without a science background\",\n",
    "]\n",
    "\n",
    "format_task = [\n",
    "    \"\"\"4. Return your final response in a JSON format with the following format: \\\n",
    "    \\n{\"headline\": <subject line from step 2>, \\\n",
    "    \\n\"body\": <text from step 1>,\n",
    "    \\n\"audience\": <rewritten text from step 3>}\"\"\"\n",
    "]\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1.3\n",
    "n_choices = 1\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "save_outputs=True\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "## Add rows from references dataframe\n",
    "# bulk_append(references_df)\n",
    "\n",
    "\n",
    "sources_df = get_table(table='sources', limit=1)\n",
    "\n",
    "chaining_dict = batch_summarize(\n",
    "    sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chatbot_dict,\n",
    "    system_role=system_role, model=model, max_tokens=1000,\n",
    "    n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "    iteration_id=iteration_id, save_outputs=save_outputs\n",
    "    )\n",
    "# # chaining_dict[iteration_id]\n",
    "qna_dict = create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "    )\n",
    "qna_dict[iteration_id]\n",
    "\n",
    "# Add rows from results to summaries and prompts table\n",
    "bulk_append(qna_dict[iteration_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from sources LIMIT 1\n",
      "**Text #1 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 1 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "1_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "Object saved as JSON: ../text/2023-07-11 for db//batch_Chaining_attributes_initial_2023-07-11_2304.json\n",
      "Processing 1_prompt00...\n",
      "Original summaries DataFrame shape: (1, 18)\n",
      "\tOriginal summaries Dataframe columns: Index(['choice', 'timestamp', 'reference_id', 'article_title', 'text',\n",
      "       'system_role', 'model', 'prep_step', 'summarize_task', 'edit_task',\n",
      "       'simplify_task', 'simplify_audience', 'format_task',\n",
      "       'full_summarize task', 'folder', 'summary', 'headline',\n",
      "       'simple_summary'],\n",
      "      dtype='object')\n",
      "Adding 1 rows to the database...\n",
      "Error adding data to the database: 'full summarization task'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    # created = mapped_column(TIMESTAMP(timezone=True))\n",
    "    full_template = mapped_column(Text)\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    original_headline = mapped_column(String(255))\n",
    "    simple_summary = mapped_column(Text)\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=5):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "@testing_session\n",
    "def get_session(session):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    return session\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries', engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    data = Sources(\n",
    "                        title=row['title'],\n",
    "                        text=row['text'],\n",
    "                        abstract=row['abstract'],\n",
    "                        publication=row['publication'],\n",
    "                        authors=row['authors'],\n",
    "                        year=row['year'],\n",
    "                        month=row['month'],\n",
    "                        pub_volume=row['pub_volume'],\n",
    "                        pub_issue=row['pub_issue'],\n",
    "                        start_page=row['start_page'],\n",
    "                        end_page=row['end_page'],\n",
    "                        doi=row['doi']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                elif table == 'summaries':\n",
    "\n",
    "                    # Check if prompt already exists in the database\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full summarization task'],\n",
    "                        ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['ssummarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                print(f'\\t{row[\"title\"]}')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.reference_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['timestamp'] = str(datetime.now(pytz.timezone('Canada/Pacific')))\n",
    "        self.qna['reference_id'] = self.reference_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model\n",
    "        self.qna['prep_step'] = prep_step\n",
    "        self.qna['summarize_task'] = task\n",
    "        self.qna['edit_task'] = edit_task\n",
    "        self.qna['simplify_task'] = simplify_task\n",
    "        self.qna['simplify_audience'] = simplify_audience\n",
    "        self.qna['format_task'] = format_task\n",
    "        self.qna['full_summarize task'] = full_task\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "system_role = \"You are a helpful assistant.\"\n",
    "prep_step = [\n",
    "    # \"Tell your friend about the research in a text message.\",\n",
    "    \"In the summary, cover the following information: \\\n",
    "    \\n- Identify the key points and statistics from this text that would make interesting or helpful health content. \\\n",
    "    \\n- If available, include the effect sizes found in the research. \\\n",
    "    Otherwise, skip this step. \\\n",
    "    \\n- If applicable, get a brief description of the research participants, \\\n",
    "    such as age, sex, and health conditions. Otherwise, you can skip this step.\\\n",
    "    \\n- Think about why the general population should care about the research.\",\n",
    "]\n",
    "\n",
    "summarize_task = [\n",
    "    f\"1. Summarize the text for a LinkedIn post.\",\n",
    "]\n",
    "edit_task = [\n",
    "    \"\"\"\n",
    "    Once you have written your text message: \\\n",
    "    \\nEvaluate your text message to see if it may be confusing or redundant. \\\n",
    "    \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same. \\\n",
    "    \\n2. Create an intriguing subject line for the text.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "simplify_task = [\n",
    "    \"\"\"3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same.\\\n",
    "    Follow these steps to accomplish this: \\\n",
    "    \\na. Check if the content and language are appropriate for the audience. \\\n",
    "    \\nb. If it is suitable for the audience, keep it the same. \\\n",
    "    If not, rewrite using terms appropriate for the audience while keeping the news-worthy details. \\ \n",
    "    \\nc. Return the final version of the summary to be shown to the audience. \\\n",
    "    \\n\\nYour audience is\"\"\",\n",
    "]\n",
    "\n",
    "simplify_audience = [\n",
    "    \"people without a science background\",\n",
    "]\n",
    "\n",
    "format_task = [\n",
    "    \"\"\"4. Return your final response in a JSON format with the following format: \\\n",
    "    \\n{\"headline\": <subject line from step 2>, \\\n",
    "    \\n\"body\": <text from step 1>,\n",
    "    \\n\"audience\": <rewritten text from step 3>}\"\"\"\n",
    "]\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1.3\n",
    "n_choices = 1\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "save_outputs=True\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "## Add rows from references dataframe\n",
    "# bulk_append(references_df)\n",
    "\n",
    "\n",
    "sources_df = get_table(table='sources', limit=1)\n",
    "\n",
    "chaining_dict = batch_summarize(\n",
    "    sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chatbot_dict,\n",
    "    system_role=system_role, model=model, max_tokens=1000,\n",
    "    n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "    iteration_id=iteration_id, save_outputs=save_outputs\n",
    "    )\n",
    "# # chaining_dict[iteration_id]\n",
    "qna_dict = create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "    )\n",
    "qna_dict[iteration_id]\n",
    "\n",
    "# Add rows from results to summaries and prompts table\n",
    "bulk_append(qna_dict[iteration_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>reference_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>choice</th>\n",
       "      <th>text</th>\n",
       "      <th>system_role</th>\n",
       "      <th>model</th>\n",
       "      <th>prep_step</th>\n",
       "      <th>summarize_task</th>\n",
       "      <th>edit_task</th>\n",
       "      <th>simplify_task</th>\n",
       "      <th>simplify_audience</th>\n",
       "      <th>format_task</th>\n",
       "      <th>full_summarize task</th>\n",
       "      <th>folder</th>\n",
       "      <th>summary</th>\n",
       "      <th>headline</th>\n",
       "      <th>simple_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-11 23:03:59.364176-07:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparisons in the Recovery Response From Resi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Decreases in muscle mass, function, and neurom...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>\\n    Once you have written your text message:...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "      <td>New Research Shows How Resistance Training Can...</td>\n",
       "      <td>New research suggests that resistance training...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          timestamp  reference_id  \\\n",
       "0  2023-07-11 23:03:59.364176-07:00             1   \n",
       "\n",
       "                                       article_title  choice  \\\n",
       "0  Comparisons in the Recovery Response From Resi...       1   \n",
       "\n",
       "                                                text  \\\n",
       "0  Decreases in muscle mass, function, and neurom...   \n",
       "\n",
       "                    system_role                   model  \\\n",
       "0  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "\n",
       "                                           prep_step  \\\n",
       "0  In the summary, cover the following informatio...   \n",
       "\n",
       "                               summarize_task  \\\n",
       "0  1. Summarize the text for a LinkedIn post.   \n",
       "\n",
       "                                           edit_task  \\\n",
       "0  \\n    Once you have written your text message:...   \n",
       "\n",
       "                                       simplify_task  \\\n",
       "0  3. If needed, rewrite the text using terms app...   \n",
       "\n",
       "                     simplify_audience  \\\n",
       "0  people without a science background   \n",
       "\n",
       "                                         format_task  \\\n",
       "0  4. Return your final response in a JSON format...   \n",
       "\n",
       "                                 full_summarize task                  folder  \\\n",
       "0  1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "\n",
       "                                             summary  \\\n",
       "0  A recent study compared the recovery response ...   \n",
       "\n",
       "                                            headline  \\\n",
       "0  New Research Shows How Resistance Training Can...   \n",
       "\n",
       "                                      simple_summary  \n",
       "0  New research suggests that resistance training...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_dict[iteration_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from sources LIMIT 1\n",
      "**Text #1 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 1 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "1_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "Object saved as JSON: ../text/2023-07-11 for db//batch_Chaining_attributes_initial_2023-07-11_2310.json\n",
      "Processing 1_prompt00...\n",
      "Original summaries DataFrame shape: (1, 18)\n",
      "\tOriginal summaries Dataframe columns: Index(['choice', 'timestamp', 'reference_id', 'article_title', 'text',\n",
      "       'system_role', 'model', 'prep_step', 'summarize_task', 'edit_task',\n",
      "       'simplify_task', 'simplify_audience', 'format_task',\n",
      "       'full_summarize_task', 'folder', 'summary', 'headline',\n",
      "       'simple_summary'],\n",
      "      dtype='object')\n",
      "Adding 1 rows to the database...\n",
      "Error adding data to the database: 'full_summarizate_task'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>reference_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>choice</th>\n",
       "      <th>text</th>\n",
       "      <th>system_role</th>\n",
       "      <th>model</th>\n",
       "      <th>prep_step</th>\n",
       "      <th>summarize_task</th>\n",
       "      <th>edit_task</th>\n",
       "      <th>simplify_task</th>\n",
       "      <th>simplify_audience</th>\n",
       "      <th>format_task</th>\n",
       "      <th>full_summarize_task</th>\n",
       "      <th>folder</th>\n",
       "      <th>summary</th>\n",
       "      <th>headline</th>\n",
       "      <th>simple_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-11 23:10:09.148801-07:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparisons in the Recovery Response From Resi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Decreases in muscle mass, function, and neurom...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>\\n    Once you have written your text message:...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study compared muscle function and re...</td>\n",
       "      <td>New Research on Muscle Function and Recovery i...</td>\n",
       "      <td>A recent study investigated the effects of exe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          timestamp  reference_id  \\\n",
       "0  2023-07-11 23:10:09.148801-07:00             1   \n",
       "\n",
       "                                       article_title  choice  \\\n",
       "0  Comparisons in the Recovery Response From Resi...       1   \n",
       "\n",
       "                                                text  \\\n",
       "0  Decreases in muscle mass, function, and neurom...   \n",
       "\n",
       "                    system_role                   model  \\\n",
       "0  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "\n",
       "                                           prep_step  \\\n",
       "0  In the summary, cover the following informatio...   \n",
       "\n",
       "                               summarize_task  \\\n",
       "0  1. Summarize the text for a LinkedIn post.   \n",
       "\n",
       "                                           edit_task  \\\n",
       "0  \\n    Once you have written your text message:...   \n",
       "\n",
       "                                       simplify_task  \\\n",
       "0  3. If needed, rewrite the text using terms app...   \n",
       "\n",
       "                     simplify_audience  \\\n",
       "0  people without a science background   \n",
       "\n",
       "                                         format_task  \\\n",
       "0  4. Return your final response in a JSON format...   \n",
       "\n",
       "                                 full_summarize_task                  folder  \\\n",
       "0  1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "\n",
       "                                             summary  \\\n",
       "0  A recent study compared muscle function and re...   \n",
       "\n",
       "                                            headline  \\\n",
       "0  New Research on Muscle Function and Recovery i...   \n",
       "\n",
       "                                      simple_summary  \n",
       "0  A recent study investigated the effects of exe...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    # created = mapped_column(TIMESTAMP(timezone=True))\n",
    "    full_template = mapped_column(Text)\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    original_headline = mapped_column(String(255))\n",
    "    simple_summary = mapped_column(Text)\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=5):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "@testing_session\n",
    "def get_session(session):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    return session\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries', engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    data = Sources(\n",
    "                        title=row['title'],\n",
    "                        text=row['text'],\n",
    "                        abstract=row['abstract'],\n",
    "                        publication=row['publication'],\n",
    "                        authors=row['authors'],\n",
    "                        year=row['year'],\n",
    "                        month=row['month'],\n",
    "                        pub_volume=row['pub_volume'],\n",
    "                        pub_issue=row['pub_issue'],\n",
    "                        start_page=row['start_page'],\n",
    "                        end_page=row['end_page'],\n",
    "                        doi=row['doi']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                elif table == 'summaries':\n",
    "\n",
    "                    # Check if prompt already exists in the database\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full_summarizate_task'],\n",
    "                        ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['ssummarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                print(f'\\t{row[\"title\"]}')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.reference_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['timestamp'] = str(datetime.now(pytz.timezone('Canada/Pacific')))\n",
    "        self.qna['reference_id'] = self.reference_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model\n",
    "        self.qna['prep_step'] = prep_step\n",
    "        self.qna['summarize_task'] = task\n",
    "        self.qna['edit_task'] = edit_task\n",
    "        self.qna['simplify_task'] = simplify_task\n",
    "        self.qna['simplify_audience'] = simplify_audience\n",
    "        self.qna['format_task'] = format_task\n",
    "        self.qna['full_summarize_task'] = full_task\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "system_role = \"You are a helpful assistant.\"\n",
    "prep_step = [\n",
    "    # \"Tell your friend about the research in a text message.\",\n",
    "    \"In the summary, cover the following information: \\\n",
    "    \\n- Identify the key points and statistics from this text that would make interesting or helpful health content. \\\n",
    "    \\n- If available, include the effect sizes found in the research. \\\n",
    "    Otherwise, skip this step. \\\n",
    "    \\n- If applicable, get a brief description of the research participants, \\\n",
    "    such as age, sex, and health conditions. Otherwise, you can skip this step.\\\n",
    "    \\n- Think about why the general population should care about the research.\",\n",
    "]\n",
    "\n",
    "summarize_task = [\n",
    "    f\"1. Summarize the text for a LinkedIn post.\",\n",
    "]\n",
    "edit_task = [\n",
    "    \"\"\"\n",
    "    Once you have written your text message: \\\n",
    "    \\nEvaluate your text message to see if it may be confusing or redundant. \\\n",
    "    \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same. \\\n",
    "    \\n2. Create an intriguing subject line for the text.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "simplify_task = [\n",
    "    \"\"\"3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same.\\\n",
    "    Follow these steps to accomplish this: \\\n",
    "    \\na. Check if the content and language are appropriate for the audience. \\\n",
    "    \\nb. If it is suitable for the audience, keep it the same. \\\n",
    "    If not, rewrite using terms appropriate for the audience while keeping the news-worthy details. \\ \n",
    "    \\nc. Return the final version of the summary to be shown to the audience. \\\n",
    "    \\n\\nYour audience is\"\"\",\n",
    "]\n",
    "\n",
    "simplify_audience = [\n",
    "    \"people without a science background\",\n",
    "]\n",
    "\n",
    "format_task = [\n",
    "    \"\"\"4. Return your final response in a JSON format with the following format: \\\n",
    "    \\n{\"headline\": <subject line from step 2>, \\\n",
    "    \\n\"body\": <text from step 1>,\n",
    "    \\n\"audience\": <rewritten text from step 3>}\"\"\"\n",
    "]\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1.4\n",
    "n_choices = 1\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "save_outputs=True\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "## Add rows from references dataframe\n",
    "# bulk_append(references_df)\n",
    "\n",
    "\n",
    "sources_df = get_table(table='sources', limit=1)\n",
    "\n",
    "chaining_dict = batch_summarize(\n",
    "    sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chatbot_dict,\n",
    "    system_role=system_role, model=model, max_tokens=1000,\n",
    "    n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "    iteration_id=iteration_id, save_outputs=save_outputs\n",
    "    )\n",
    "# # chaining_dict[iteration_id]\n",
    "qna_dict = create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "    )\n",
    "# Add rows from results to summaries and prompts table\n",
    "bulk_append(qna_dict[iteration_id])\n",
    "qna_dict[iteration_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from sources LIMIT 1\n",
      "**Text #1 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 1 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "1_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "Object saved as JSON: ../text/2023-07-11 for db//batch_Chaining_attributes_initial_2023-07-11_2312.json\n",
      "Processing 1_prompt00...\n",
      "Original summaries DataFrame shape: (1, 18)\n",
      "\tOriginal summaries Dataframe columns: Index(['choice', 'timestamp', 'reference_id', 'article_title', 'text',\n",
      "       'system_role', 'model', 'prep_step', 'summarize_task', 'edit_task',\n",
      "       'simplify_task', 'simplify_audience', 'format_task',\n",
      "       'full_summarize_task', 'folder', 'summary', 'headline',\n",
      "       'simple_summary'],\n",
      "      dtype='object')\n",
      "Adding 1 rows to the database...\n",
      "Error adding data to the database: 'title'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>reference_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>choice</th>\n",
       "      <th>text</th>\n",
       "      <th>system_role</th>\n",
       "      <th>model</th>\n",
       "      <th>prep_step</th>\n",
       "      <th>summarize_task</th>\n",
       "      <th>edit_task</th>\n",
       "      <th>simplify_task</th>\n",
       "      <th>simplify_audience</th>\n",
       "      <th>format_task</th>\n",
       "      <th>full_summarize_task</th>\n",
       "      <th>folder</th>\n",
       "      <th>summary</th>\n",
       "      <th>headline</th>\n",
       "      <th>simple_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-11 23:12:32.880097-07:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparisons in the Recovery Response From Resi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Decreases in muscle mass, function, and neurom...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>\\n    Once you have written your text message:...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "      <td>New Study Shows How Exercise Impacts Muscle Re...</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          timestamp  reference_id  \\\n",
       "0  2023-07-11 23:12:32.880097-07:00             1   \n",
       "\n",
       "                                       article_title  choice  \\\n",
       "0  Comparisons in the Recovery Response From Resi...       1   \n",
       "\n",
       "                                                text  \\\n",
       "0  Decreases in muscle mass, function, and neurom...   \n",
       "\n",
       "                    system_role                   model  \\\n",
       "0  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "\n",
       "                                           prep_step  \\\n",
       "0  In the summary, cover the following informatio...   \n",
       "\n",
       "                               summarize_task  \\\n",
       "0  1. Summarize the text for a LinkedIn post.   \n",
       "\n",
       "                                           edit_task  \\\n",
       "0  \\n    Once you have written your text message:...   \n",
       "\n",
       "                                       simplify_task  \\\n",
       "0  3. If needed, rewrite the text using terms app...   \n",
       "\n",
       "                     simplify_audience  \\\n",
       "0  people without a science background   \n",
       "\n",
       "                                         format_task  \\\n",
       "0  4. Return your final response in a JSON format...   \n",
       "\n",
       "                                 full_summarize_task                  folder  \\\n",
       "0  1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "\n",
       "                                             summary  \\\n",
       "0  A recent study compared the recovery response ...   \n",
       "\n",
       "                                            headline  \\\n",
       "0  New Study Shows How Exercise Impacts Muscle Re...   \n",
       "\n",
       "                                      simple_summary  \n",
       "0  A recent study compared the recovery response ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    # created = mapped_column(TIMESTAMP(timezone=True))\n",
    "    full_template = mapped_column(Text)\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    original_headline = mapped_column(String(255))\n",
    "    simple_summary = mapped_column(Text)\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=5):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "@testing_session\n",
    "def get_session(session):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    return session\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries', engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    data = Sources(\n",
    "                        title=row['title'],\n",
    "                        text=row['text'],\n",
    "                        abstract=row['abstract'],\n",
    "                        publication=row['publication'],\n",
    "                        authors=row['authors'],\n",
    "                        year=row['year'],\n",
    "                        month=row['month'],\n",
    "                        pub_volume=row['pub_volume'],\n",
    "                        pub_issue=row['pub_issue'],\n",
    "                        start_page=row['start_page'],\n",
    "                        end_page=row['end_page'],\n",
    "                        doi=row['doi']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                elif table == 'summaries':\n",
    "\n",
    "                    # Check if prompt already exists in the database\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full_summarize_task'],\n",
    "                        ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['summarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                print(f'\\t{row[\"title\"]}')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.reference_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['timestamp'] = str(datetime.now(pytz.timezone('Canada/Pacific')))\n",
    "        self.qna['reference_id'] = self.reference_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model\n",
    "        self.qna['prep_step'] = prep_step\n",
    "        self.qna['summarize_task'] = task\n",
    "        self.qna['edit_task'] = edit_task\n",
    "        self.qna['simplify_task'] = simplify_task\n",
    "        self.qna['simplify_audience'] = simplify_audience\n",
    "        self.qna['format_task'] = format_task\n",
    "        self.qna['full_summarize_task'] = full_task\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "system_role = \"You are a helpful assistant.\"\n",
    "prep_step = [\n",
    "    # \"Tell your friend about the research in a text message.\",\n",
    "    \"In the summary, cover the following information: \\\n",
    "    \\n- Identify the key points and statistics from this text that would make interesting or helpful health content. \\\n",
    "    \\n- If available, include the effect sizes found in the research. \\\n",
    "    Otherwise, skip this step. \\\n",
    "    \\n- If applicable, get a brief description of the research participants, \\\n",
    "    such as age, sex, and health conditions. Otherwise, you can skip this step.\\\n",
    "    \\n- Think about why the general population should care about the research.\",\n",
    "]\n",
    "\n",
    "summarize_task = [\n",
    "    f\"1. Summarize the text for a LinkedIn post.\",\n",
    "]\n",
    "edit_task = [\n",
    "    \"\"\"\n",
    "    Once you have written your text message: \\\n",
    "    \\nEvaluate your text message to see if it may be confusing or redundant. \\\n",
    "    \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same. \\\n",
    "    \\n2. Create an intriguing subject line for the text.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "simplify_task = [\n",
    "    \"\"\"3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same.\\\n",
    "    Follow these steps to accomplish this: \\\n",
    "    \\na. Check if the content and language are appropriate for the audience. \\\n",
    "    \\nb. If it is suitable for the audience, keep it the same. \\\n",
    "    If not, rewrite using terms appropriate for the audience while keeping the news-worthy details. \\ \n",
    "    \\nc. Return the final version of the summary to be shown to the audience. \\\n",
    "    \\n\\nYour audience is\"\"\",\n",
    "]\n",
    "\n",
    "simplify_audience = [\n",
    "    \"people without a science background\",\n",
    "]\n",
    "\n",
    "format_task = [\n",
    "    \"\"\"4. Return your final response in a JSON format with the following format: \\\n",
    "    \\n{\"headline\": <subject line from step 2>, \\\n",
    "    \\n\"body\": <text from step 1>,\n",
    "    \\n\"audience\": <rewritten text from step 3>}\"\"\"\n",
    "]\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1.41\n",
    "n_choices = 1\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "save_outputs=True\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "## Add rows from references dataframe\n",
    "# bulk_append(references_df)\n",
    "\n",
    "\n",
    "sources_df = get_table(table='sources', limit=1)\n",
    "\n",
    "chaining_dict = batch_summarize(\n",
    "    sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chatbot_dict,\n",
    "    system_role=system_role, model=model, max_tokens=1000,\n",
    "    n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "    iteration_id=iteration_id, save_outputs=save_outputs\n",
    "    )\n",
    "# # chaining_dict[iteration_id]\n",
    "qna_dict = create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "    )\n",
    "# Add rows from results to summaries and prompts table\n",
    "bulk_append(qna_dict[iteration_id])\n",
    "qna_dict[iteration_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 1 rows to the database...\n",
      "Data added successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>reference_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>choice</th>\n",
       "      <th>text</th>\n",
       "      <th>system_role</th>\n",
       "      <th>model</th>\n",
       "      <th>prep_step</th>\n",
       "      <th>summarize_task</th>\n",
       "      <th>edit_task</th>\n",
       "      <th>simplify_task</th>\n",
       "      <th>simplify_audience</th>\n",
       "      <th>format_task</th>\n",
       "      <th>full_summarize_task</th>\n",
       "      <th>folder</th>\n",
       "      <th>summary</th>\n",
       "      <th>headline</th>\n",
       "      <th>simple_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-11 23:12:32.880097-07:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparisons in the Recovery Response From Resi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Decreases in muscle mass, function, and neurom...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>\\n    Once you have written your text message:...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "      <td>New Study Shows How Exercise Impacts Muscle Re...</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          timestamp  reference_id  \\\n",
       "0  2023-07-11 23:12:32.880097-07:00             1   \n",
       "\n",
       "                                       article_title  choice  \\\n",
       "0  Comparisons in the Recovery Response From Resi...       1   \n",
       "\n",
       "                                                text  \\\n",
       "0  Decreases in muscle mass, function, and neurom...   \n",
       "\n",
       "                    system_role                   model  \\\n",
       "0  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "\n",
       "                                           prep_step  \\\n",
       "0  In the summary, cover the following informatio...   \n",
       "\n",
       "                               summarize_task  \\\n",
       "0  1. Summarize the text for a LinkedIn post.   \n",
       "\n",
       "                                           edit_task  \\\n",
       "0  \\n    Once you have written your text message:...   \n",
       "\n",
       "                                       simplify_task  \\\n",
       "0  3. If needed, rewrite the text using terms app...   \n",
       "\n",
       "                     simplify_audience  \\\n",
       "0  people without a science background   \n",
       "\n",
       "                                         format_task  \\\n",
       "0  4. Return your final response in a JSON format...   \n",
       "\n",
       "                                 full_summarize_task                  folder  \\\n",
       "0  1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "\n",
       "                                             summary  \\\n",
       "0  A recent study compared the recovery response ...   \n",
       "\n",
       "                                            headline  \\\n",
       "0  New Study Shows How Exercise Impacts Muscle Re...   \n",
       "\n",
       "                                      simple_summary  \n",
       "0  A recent study compared the recovery response ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    # created = mapped_column(TIMESTAMP(timezone=True))\n",
    "    full_template = mapped_column(Text)\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    original_headline = mapped_column(String(255))\n",
    "    simple_summary = mapped_column(Text)\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=5):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "@testing_session\n",
    "def get_session(session):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    return session\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries', engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                # if table == 'sources':\n",
    "                #     data = Sources(\n",
    "                #         title=row['title'],\n",
    "                #         text=row['text'],\n",
    "                #         abstract=row['abstract'],\n",
    "                #         publication=row['publication'],\n",
    "                #         authors=row['authors'],\n",
    "                #         year=row['year'],\n",
    "                #         month=row['month'],\n",
    "                #         pub_volume=row['pub_volume'],\n",
    "                #         pub_issue=row['pub_issue'],\n",
    "                #         start_page=row['start_page'],\n",
    "                #         end_page=row['end_page'],\n",
    "                #         doi=row['doi']\n",
    "                #     )\n",
    "                #     session.add(data)\n",
    "                # elif table == 'summaries':\n",
    "                if table == 'summaries':\n",
    "\n",
    "                    # Check if prompt already exists in the database\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full_summarize_task'],\n",
    "                        ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['summarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.reference_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['timestamp'] = str(datetime.now(pytz.timezone('Canada/Pacific')))\n",
    "        self.qna['reference_id'] = self.reference_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model\n",
    "        self.qna['prep_step'] = prep_step\n",
    "        self.qna['summarize_task'] = task\n",
    "        self.qna['edit_task'] = edit_task\n",
    "        self.qna['simplify_task'] = simplify_task\n",
    "        self.qna['simplify_audience'] = simplify_audience\n",
    "        self.qna['format_task'] = format_task\n",
    "        self.qna['full_summarize_task'] = full_task\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "system_role = \"You are a helpful assistant.\"\n",
    "prep_step = [\n",
    "    # \"Tell your friend about the research in a text message.\",\n",
    "    \"In the summary, cover the following information: \\\n",
    "    \\n- Identify the key points and statistics from this text that would make interesting or helpful health content. \\\n",
    "    \\n- If available, include the effect sizes found in the research. \\\n",
    "    Otherwise, skip this step. \\\n",
    "    \\n- If applicable, get a brief description of the research participants, \\\n",
    "    such as age, sex, and health conditions. Otherwise, you can skip this step.\\\n",
    "    \\n- Think about why the general population should care about the research.\",\n",
    "]\n",
    "\n",
    "summarize_task = [\n",
    "    f\"1. Summarize the text for a LinkedIn post.\",\n",
    "]\n",
    "edit_task = [\n",
    "    \"\"\"\n",
    "    Once you have written your text message: \\\n",
    "    \\nEvaluate your text message to see if it may be confusing or redundant. \\\n",
    "    \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same. \\\n",
    "    \\n2. Create an intriguing subject line for the text.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "simplify_task = [\n",
    "    \"\"\"3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same.\\\n",
    "    Follow these steps to accomplish this: \\\n",
    "    \\na. Check if the content and language are appropriate for the audience. \\\n",
    "    \\nb. If it is suitable for the audience, keep it the same. \\\n",
    "    If not, rewrite using terms appropriate for the audience while keeping the news-worthy details. \\ \n",
    "    \\nc. Return the final version of the summary to be shown to the audience. \\\n",
    "    \\n\\nYour audience is\"\"\",\n",
    "]\n",
    "\n",
    "simplify_audience = [\n",
    "    \"people without a science background\",\n",
    "]\n",
    "\n",
    "format_task = [\n",
    "    \"\"\"4. Return your final response in a JSON format with the following format: \\\n",
    "    \\n{\"headline\": <subject line from step 2>, \\\n",
    "    \\n\"body\": <text from step 1>,\n",
    "    \\n\"audience\": <rewritten text from step 3>}\"\"\"\n",
    "]\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1.41\n",
    "n_choices = 1\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "save_outputs=True\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "## Add rows from references dataframe\n",
    "# bulk_append(references_df)\n",
    "\n",
    "\n",
    "# sources_df = get_table(table='sources', limit=1)\n",
    "\n",
    "# chaining_dict = batch_summarize(\n",
    "#     sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "#     simplify_task, simplify_audience, format_task,\n",
    "#     chatbot_dict,\n",
    "#     system_role=system_role, model=model, max_tokens=1000,\n",
    "#     n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "#     iteration_id=iteration_id, save_outputs=save_outputs\n",
    "#     )\n",
    "# # # chaining_dict[iteration_id]\n",
    "# qna_dict = create_summaries_df(\n",
    "#     qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "#     )\n",
    "# Add rows from results to summaries and prompts table\n",
    "bulk_append(qna_dict[iteration_id])\n",
    "qna_dict[iteration_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from sources LIMIT 3\n",
      "**Text #1 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "**Text #2 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "**Text #3 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "1_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "2_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "3_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "Object saved as JSON: ../text/2023-07-11 for db//batch_Chaining_attributes_initial_2023-07-11_2335.json\n",
      "Processing 1_prompt00...\n",
      "Processing 2_prompt00...\n",
      "Processing 3_prompt00...\n",
      "Original summaries DataFrame shape: (6, 18)\n",
      "\tOriginal summaries Dataframe columns: Index(['choice', 'timestamp', 'reference_id', 'article_title', 'text',\n",
      "       'system_role', 'model', 'prep_step', 'summarize_task', 'edit_task',\n",
      "       'simplify_task', 'simplify_audience', 'format_task',\n",
      "       'full_summarize_task', 'folder', 'summary', 'headline',\n",
      "       'simple_summary'],\n",
      "      dtype='object')\n",
      "Adding 6 rows to the database...\n",
      "\tReference #1: New Research on Recovery from Exercise in Middle-Aged Adults\n",
      "\tReference #1: New Research Shows How Recreational Training Can Mitigate Age-Related Decline in Muscle Function\n",
      "\tReference #2: New Study Shows Nutritional Intervention Reduces Risk of Falls and Fractures in Older Adults\n",
      "\tReference #2: New Study Shows Nutritional Intervention Can Reduce Fracture Risk in Older Adults\n",
      "\tReference #3: New Research Shows Exercise Snacks Improve Health and Fitness\n",
      "\tReference #3: Exercise Snacks: A Time-Efficient Way to Improve Fitness and Health\n",
      "Data added successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>reference_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>choice</th>\n",
       "      <th>text</th>\n",
       "      <th>system_role</th>\n",
       "      <th>model</th>\n",
       "      <th>prep_step</th>\n",
       "      <th>summarize_task</th>\n",
       "      <th>edit_task</th>\n",
       "      <th>simplify_task</th>\n",
       "      <th>simplify_audience</th>\n",
       "      <th>format_task</th>\n",
       "      <th>full_summarize_task</th>\n",
       "      <th>folder</th>\n",
       "      <th>summary</th>\n",
       "      <th>headline</th>\n",
       "      <th>simple_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-11 23:35:24.458418-07:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparisons in the Recovery Response From Resi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Decreases in muscle mass, function, and neurom...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>\\n    Once you have written your text message:...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "      <td>New Research on Recovery from Exercise in Midd...</td>\n",
       "      <td>Check out this new research that shows how par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-11 23:35:24.458418-07:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparisons in the Recovery Response From Resi...</td>\n",
       "      <td>2</td>\n",
       "      <td>Decreases in muscle mass, function, and neurom...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>\\n    Once you have written your text message:...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "      <td>New Research Shows How Recreational Training C...</td>\n",
       "      <td>New research suggests that engaging in regular...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-11 23:35:28.609483-07:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Effect of dietary sources of calcium and prote...</td>\n",
       "      <td>1</td>\n",
       "      <td>Longevity increases the proportion of older ad...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>\\n    Once you have written your text message:...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study found that a high calcium and h...</td>\n",
       "      <td>New Study Shows Nutritional Intervention Reduc...</td>\n",
       "      <td>A recent study has shown that a simple dietary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-11 23:35:28.609483-07:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Effect of dietary sources of calcium and prote...</td>\n",
       "      <td>2</td>\n",
       "      <td>Longevity increases the proportion of older ad...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>\\n    Once you have written your text message:...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study found that a high calcium and h...</td>\n",
       "      <td>New Study Shows Nutritional Intervention Can R...</td>\n",
       "      <td>A recent study has found that a simple dietary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-07-11 23:35:33.993335-07:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Exercise Snacks A Novel Strategy to Improve Ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>We define exercise snacks as isolated ?1-min b...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>\\n    Once you have written your text message:...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>Exercise snacks, which are short bouts of vigo...</td>\n",
       "      <td>New Research Shows Exercise Snacks Improve Hea...</td>\n",
       "      <td>New research has found that short bursts of vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-07-11 23:35:33.993335-07:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Exercise Snacks A Novel Strategy to Improve Ca...</td>\n",
       "      <td>2</td>\n",
       "      <td>We define exercise snacks as isolated ?1-min b...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>\\n    Once you have written your text message:...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>Exercise snacks, short bursts of vigorous exer...</td>\n",
       "      <td>Exercise Snacks: A Time-Efficient Way to Impro...</td>\n",
       "      <td>Exercise snacks are a convenient and time-effi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          timestamp  reference_id  \\\n",
       "0  2023-07-11 23:35:24.458418-07:00             1   \n",
       "1  2023-07-11 23:35:24.458418-07:00             1   \n",
       "2  2023-07-11 23:35:28.609483-07:00             2   \n",
       "3  2023-07-11 23:35:28.609483-07:00             2   \n",
       "4  2023-07-11 23:35:33.993335-07:00             3   \n",
       "5  2023-07-11 23:35:33.993335-07:00             3   \n",
       "\n",
       "                                       article_title  choice  \\\n",
       "0  Comparisons in the Recovery Response From Resi...       1   \n",
       "1  Comparisons in the Recovery Response From Resi...       2   \n",
       "2  Effect of dietary sources of calcium and prote...       1   \n",
       "3  Effect of dietary sources of calcium and prote...       2   \n",
       "4  Exercise Snacks A Novel Strategy to Improve Ca...       1   \n",
       "5  Exercise Snacks A Novel Strategy to Improve Ca...       2   \n",
       "\n",
       "                                                text  \\\n",
       "0  Decreases in muscle mass, function, and neurom...   \n",
       "1  Decreases in muscle mass, function, and neurom...   \n",
       "2  Longevity increases the proportion of older ad...   \n",
       "3  Longevity increases the proportion of older ad...   \n",
       "4  We define exercise snacks as isolated ?1-min b...   \n",
       "5  We define exercise snacks as isolated ?1-min b...   \n",
       "\n",
       "                    system_role                   model  \\\n",
       "0  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "1  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "2  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "3  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "4  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "5  You are a helpful assistant.  gpt-3.5-turbo-16k-0613   \n",
       "\n",
       "                                           prep_step  \\\n",
       "0  In the summary, cover the following informatio...   \n",
       "1  In the summary, cover the following informatio...   \n",
       "2  In the summary, cover the following informatio...   \n",
       "3  In the summary, cover the following informatio...   \n",
       "4  In the summary, cover the following informatio...   \n",
       "5  In the summary, cover the following informatio...   \n",
       "\n",
       "                               summarize_task  \\\n",
       "0  1. Summarize the text for a LinkedIn post.   \n",
       "1  1. Summarize the text for a LinkedIn post.   \n",
       "2  1. Summarize the text for a LinkedIn post.   \n",
       "3  1. Summarize the text for a LinkedIn post.   \n",
       "4  1. Summarize the text for a LinkedIn post.   \n",
       "5  1. Summarize the text for a LinkedIn post.   \n",
       "\n",
       "                                           edit_task  \\\n",
       "0  \\n    Once you have written your text message:...   \n",
       "1  \\n    Once you have written your text message:...   \n",
       "2  \\n    Once you have written your text message:...   \n",
       "3  \\n    Once you have written your text message:...   \n",
       "4  \\n    Once you have written your text message:...   \n",
       "5  \\n    Once you have written your text message:...   \n",
       "\n",
       "                                       simplify_task  \\\n",
       "0  3. If needed, rewrite the text using terms app...   \n",
       "1  3. If needed, rewrite the text using terms app...   \n",
       "2  3. If needed, rewrite the text using terms app...   \n",
       "3  3. If needed, rewrite the text using terms app...   \n",
       "4  3. If needed, rewrite the text using terms app...   \n",
       "5  3. If needed, rewrite the text using terms app...   \n",
       "\n",
       "                     simplify_audience  \\\n",
       "0  people without a science background   \n",
       "1  people without a science background   \n",
       "2  people without a science background   \n",
       "3  people without a science background   \n",
       "4  people without a science background   \n",
       "5  people without a science background   \n",
       "\n",
       "                                         format_task  \\\n",
       "0  4. Return your final response in a JSON format...   \n",
       "1  4. Return your final response in a JSON format...   \n",
       "2  4. Return your final response in a JSON format...   \n",
       "3  4. Return your final response in a JSON format...   \n",
       "4  4. Return your final response in a JSON format...   \n",
       "5  4. Return your final response in a JSON format...   \n",
       "\n",
       "                                 full_summarize_task                  folder  \\\n",
       "0  1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "1  1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "2  1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "3  1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "4  1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "5  1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "\n",
       "                                             summary  \\\n",
       "0  A recent study compared the recovery response ...   \n",
       "1  A recent study compared the recovery response ...   \n",
       "2  A recent study found that a high calcium and h...   \n",
       "3  A recent study found that a high calcium and h...   \n",
       "4  Exercise snacks, which are short bouts of vigo...   \n",
       "5  Exercise snacks, short bursts of vigorous exer...   \n",
       "\n",
       "                                            headline  \\\n",
       "0  New Research on Recovery from Exercise in Midd...   \n",
       "1  New Research Shows How Recreational Training C...   \n",
       "2  New Study Shows Nutritional Intervention Reduc...   \n",
       "3  New Study Shows Nutritional Intervention Can R...   \n",
       "4  New Research Shows Exercise Snacks Improve Hea...   \n",
       "5  Exercise Snacks: A Time-Efficient Way to Impro...   \n",
       "\n",
       "                                      simple_summary  \n",
       "0  Check out this new research that shows how par...  \n",
       "1  New research suggests that engaging in regular...  \n",
       "2  A recent study has shown that a simple dietary...  \n",
       "3  A recent study has found that a simple dietary...  \n",
       "4  New research has found that short bursts of vi...  \n",
       "5  Exercise snacks are a convenient and time-effi...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    full_template = mapped_column(Text)\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    original_headline = mapped_column(String(255))\n",
    "    simple_summary = mapped_column(Text)\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=5):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "@testing_session\n",
    "def get_session(session):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    return session\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries', engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    data = Sources(\n",
    "                        title=row['title'],\n",
    "                        text=row['text'],\n",
    "                        abstract=row['abstract'],\n",
    "                        publication=row['publication'],\n",
    "                        authors=row['authors'],\n",
    "                        year=row['year'],\n",
    "                        month=row['month'],\n",
    "                        pub_volume=row['pub_volume'],\n",
    "                        pub_issue=row['pub_issue'],\n",
    "                        start_page=row['start_page'],\n",
    "                        end_page=row['end_page'],\n",
    "                        doi=row['doi']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                    print(f'\\t{row[\"title\"]}')\n",
    "                elif table == 'summaries':\n",
    "                # if table == 'summaries':\n",
    "\n",
    "                    # Check if prompt already exists in the database\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full_summarize_task'],\n",
    "                        ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['summarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                    print(f'\\tReference #{row[\"reference_id\"]}: {row[\"headline\"]}')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.reference_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['timestamp'] = str(datetime.now(pytz.timezone('Canada/Pacific')))\n",
    "        self.qna['reference_id'] = self.reference_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model\n",
    "        self.qna['prep_step'] = prep_step\n",
    "        self.qna['summarize_task'] = task\n",
    "        self.qna['edit_task'] = edit_task\n",
    "        self.qna['simplify_task'] = simplify_task\n",
    "        self.qna['simplify_audience'] = simplify_audience\n",
    "        self.qna['format_task'] = format_task\n",
    "        self.qna['full_summarize_task'] = full_task\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "system_role = \"You are a helpful assistant.\"\n",
    "prep_step = [\n",
    "    # \"Tell your friend about the research in a text message.\",\n",
    "    \"In the summary, cover the following information: \\\n",
    "    \\n- Identify the key points and statistics from this text that would make interesting or helpful health content. \\\n",
    "    \\n- If available, include the effect sizes found in the research. \\\n",
    "    Otherwise, skip this step. \\\n",
    "    \\n- If applicable, get a brief description of the research participants, \\\n",
    "    such as age, sex, and health conditions. Otherwise, you can skip this step.\\\n",
    "    \\n- Think about why the general population should care about the research.\",\n",
    "]\n",
    "\n",
    "summarize_task = [\n",
    "    f\"1. Summarize the text for a LinkedIn post.\",\n",
    "]\n",
    "edit_task = [\n",
    "    \"\"\"\n",
    "    Once you have written your text message: \\\n",
    "    \\nEvaluate your text message to see if it may be confusing or redundant. \\\n",
    "    \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same. \\\n",
    "    \\n2. Create an intriguing subject line for the text.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "simplify_task = [\n",
    "    \"\"\"3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same.\\\n",
    "    Follow these steps to accomplish this: \\\n",
    "    \\na. Check if the content and language are appropriate for the audience. \\\n",
    "    \\nb. If it is suitable for the audience, keep it the same. \\\n",
    "    If not, rewrite using terms appropriate for the audience while keeping the news-worthy details. \\ \n",
    "    \\nc. Return the final version of the summary to be shown to the audience. \\\n",
    "    \\n\\nYour audience is\"\"\",\n",
    "]\n",
    "\n",
    "simplify_audience = [\n",
    "    \"people without a science background\",\n",
    "]\n",
    "\n",
    "format_task = [\n",
    "    \"\"\"4. Return your final response in a JSON format with the following format: \\\n",
    "    \\n{\"headline\": <subject line from step 2>, \\\n",
    "    \\n\"body\": <text from step 1>,\n",
    "    \\n\"audience\": <rewritten text from step 3>}\"\"\"\n",
    "]\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1.5\n",
    "n_choices = 2\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "save_outputs=True\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "## Add rows from references dataframe\n",
    "# bulk_append(references_df)\n",
    "\n",
    "\n",
    "sources_df = get_table(table='sources', limit=3)\n",
    "\n",
    "chaining_dict = batch_summarize(\n",
    "    sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chatbot_dict,\n",
    "    system_role=system_role, model=model, max_tokens=1000,\n",
    "    n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "    iteration_id=iteration_id, save_outputs=save_outputs\n",
    "    )\n",
    "# # chaining_dict[iteration_id]\n",
    "qna_dict = create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "    )\n",
    "# Add rows from results to summaries and prompts table\n",
    "bulk_append(qna_dict[iteration_id])\n",
    "qna_dict[iteration_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
