{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "from silvhua import *\n",
    "# from datetime import datetime\n",
    "# sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\src\")\n",
    "# import json\n",
    "# from pandas import json_normalize  \n",
    "# from plotly.subplots import make_subplots\n",
    "# import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the option to wrap text within cells\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wrangling import filter_df_all_conditions, filter_df_any_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.datacamp.com/courses/all'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('http://www.DataCamp.com/courses/all')\n",
    "response.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 971\u001b[0m     \u001b[39mreturn\u001b[39;00m complexjson\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[39mexcept\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[39m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[39m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\notebooks\\2023-06-06 web scraping.ipynb Cell 6\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/Ginkgo%20coding/content-summarization/notebooks/2023-06-06%20web%20scraping.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/Ginkgo%20coding/content-summarization/notebooks/2023-06-06%20web%20scraping.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m response\u001b[39m.\u001b[39;49mjson()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[39mreturn\u001b[39;00m complexjson\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[39mexcept\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[39m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[39m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m     \u001b[39mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[39m.\u001b[39mmsg, e\u001b[39m.\u001b[39mdoc, e\u001b[39m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('http://www.DataCamp.com')\n",
    "response.status_code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 16:44:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scrapybot)\n",
      "2023-06-06 16:44:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.0.0 (OpenSSL 1.1.1t  7 Feb 2023), cryptography 39.0.1, Platform Windows-10-10.0.22621-SP0\n",
      "2023-06-06 16:44:25 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-06-06 16:44:25 [py.warnings] WARNING: c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\utils\\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-06-06 16:44:25 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-06-06 16:44:25 [scrapy.extensions.telnet] INFO: Telnet Password: a153621678cb55c6\n",
      "2023-06-06 16:44:26 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n"
     ]
    }
   ],
   "source": [
    "class DCspider(scrapy.Spider):\n",
    "    def start_requests(self):\n",
    "        urls = [ 'https://www.datacamp.com/courses/all' ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request( url = url, callback = self.parse )\n",
    "    def parse(self, response):\n",
    "        print(response.body)\n",
    "\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DCspider)\n",
    "process.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unhandled error in Deferred:\n",
      "2023-06-06 21:30:49 [twisted] CRITICAL: Unhandled error in Deferred:\n",
      "2023-06-06 21:30:49 [twisted] CRITICAL: Unhandled error in Deferred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\crawler.py\", line 233, in crawl\n",
      "    return self._crawl(crawler, *args, **kwargs)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\crawler.py\", line 237, in _crawl\n",
      "    d = crawler.crawl(*args, **kwargs)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1947, in unwindGenerator\n",
      "    return _cancellableInlineCallbacks(gen)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1857, in _cancellableInlineCallbacks\n",
      "    _inlineCallbacks(None, gen, status, _copy_context())\n",
      "--- <exception caught here> ---\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1697, in _inlineCallbacks\n",
      "    result = context.run(gen.send, result)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\crawler.py\", line 121, in crawl\n",
      "    self.spider = self._create_spider(*args, **kwargs)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\crawler.py\", line 133, in _create_spider\n",
      "    return self.spidercls.from_crawler(self, *args, **kwargs)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\spiders\\__init__.py\", line 53, in from_crawler\n",
      "    spider = cls(*args, **kwargs)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\spiders\\__init__.py\", line 32, in __init__\n",
      "    raise ValueError(f\"{type(self).__name__} must have a name\")\n",
      "builtins.ValueError: DCspider must have a name\n",
      "\n",
      "2023-06-06 21:30:49 [twisted] CRITICAL: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1697, in _inlineCallbacks\n",
      "    result = context.run(gen.send, result)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\crawler.py\", line 121, in crawl\n",
      "    self.spider = self._create_spider(*args, **kwargs)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\crawler.py\", line 133, in _create_spider\n",
      "    return self.spidercls.from_crawler(self, *args, **kwargs)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\spiders\\__init__.py\", line 53, in from_crawler\n",
      "    spider = cls(*args, **kwargs)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\spiders\\__init__.py\", line 32, in __init__\n",
      "    raise ValueError(f\"{type(self).__name__} must have a name\")\n",
      "ValueError: DCspider must have a name\n",
      "2023-06-06 21:30:49 [twisted] CRITICAL: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1697, in _inlineCallbacks\n",
      "    result = context.run(gen.send, result)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\crawler.py\", line 121, in crawl\n",
      "    self.spider = self._create_spider(*args, **kwargs)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\crawler.py\", line 133, in _create_spider\n",
      "    return self.spidercls.from_crawler(self, *args, **kwargs)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\spiders\\__init__.py\", line 53, in from_crawler\n",
      "    spider = cls(*args, **kwargs)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\spiders\\__init__.py\", line 32, in __init__\n",
      "    raise ValueError(f\"{type(self).__name__} must have a name\")\n",
      "ValueError: DCspider must have a name\n",
      "2023-06-06 21:30:49 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scrapybot)\n",
      "2023-06-06 21:30:49 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.0.0 (OpenSSL 1.1.1t  7 Feb 2023), cryptography 39.0.1, Platform Windows-10-10.0.22621-SP0\n",
      "2023-06-06 21:30:49 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-06-06 21:30:49 [py.warnings] WARNING: c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\utils\\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-06-06 21:30:49 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-06-06 21:30:49 [scrapy.extensions.telnet] INFO: Telnet Password: 200ba463b3b3e9f5\n",
      "2023-06-06 21:30:49 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "class DCspider(scrapy.Spider):\n",
    "    def start_requests(self):\n",
    "        urls = [ 'https://www.datacamp.com/courses/all' ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request( url = url, callback = self.parse )\n",
    "    def parse(self, response):\n",
    "        links = response.css('div.course-block > a::attr(href)').extract()\n",
    "        for link in links:\n",
    "            yield response.follow( url = link, callback = self.parse2 )\n",
    "    def parse2( self, response ):\n",
    "        print(response)\n",
    "        \n",
    "process = CrawlerProcess()\n",
    "process.crawl(DCspider)\n",
    "process.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 21:36:00 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scrapybot)\n",
      "2023-06-06 21:36:00 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.0.0 (OpenSSL 1.1.1t  7 Feb 2023), cryptography 39.0.1, Platform Windows-10-10.0.22621-SP0\n",
      "2023-06-06 21:36:00 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-06-06 21:36:00 [py.warnings] WARNING: c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\utils\\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-06-06 21:36:00 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-06-06 21:36:00 [scrapy.extensions.telnet] INFO: Telnet Password: 09f0947eca0ec30f\n",
      "2023-06-06 21:36:00 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "class DCspider(scrapy.Spider):\n",
    "    def start_requests(self):\n",
    "        urls = [ 'https://www.datacamp.com/courses/all' ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request( url = url, callback = self.parse )\n",
    "    def parse_front(self, response):\n",
    "        course_blocks = response. css('div.course-block')\n",
    "        course_links = course_blocks.xpath('./a/@href')\n",
    "        links_to_follow = course_links.extract()\n",
    "        for url in links_to_follow:\n",
    "            yield response.follow(url=url, callback=self.parse_pages)\n",
    "\n",
    "    def parse_pages(self, response):\n",
    "        crs_title = response.xpath('//h1[contains(@class, \"title\")]/text()')\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "        ch_titles = response.css('h4.chapter__title::text')\n",
    "        ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "        dc_dict[crs_title_ext] = ch_titles_ext\n",
    "        \n",
    "dc_dict = dict()\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DCspider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 22:03:27 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scrapybot)\n",
      "2023-06-06 22:03:27 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.0.0 (OpenSSL 1.1.1t  7 Feb 2023), cryptography 39.0.1, Platform Windows-10-10.0.22621-SP0\n",
      "2023-06-06 22:03:27 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-06-06 22:03:27 [py.warnings] WARNING: c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\utils\\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-06-06 22:03:27 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-06-06 22:03:27 [scrapy.extensions.telnet] INFO: Telnet Password: 7e6159bbc0f227b4\n",
      "2023-06-06 22:03:27 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "class DCspider(scrapy.Spider):\n",
    "    def start_requests(self):\n",
    "        urls = [ 'https://emails.bmj.com/q/1fnLH65XUsNn7Iiph6kELOM/wv' ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request( url = url, callback = self.parse )\n",
    "    def parse_front(self, response):\n",
    "        # article_url = response.xpath('//a/@href=\"art-title\"').extract()\n",
    "        article_url = response.css('td//a::attr(href)').extract()\n",
    "        for index, url in enumerate(article_url):\n",
    "    #         yield response.follow(url=url, callback=self.parse_pages)\n",
    "            dc_dict[index] = url\n",
    "\n",
    "    # def parse_pages(self, response):\n",
    "    #     crs_title = response.xpath('//h1[contains(@class, \"title\")]/text()')\n",
    "    #     crs_title_ext = crs_title.extract_first().strip()\n",
    "    #     ch_titles = response.css('h4.chapter__title::text')\n",
    "    #     ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    #     dc_dict[crs_title_ext] = ch_titles_ext\n",
    "        \n",
    "dc_dict = dict()\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DCspider)\n",
    "process.start()\n",
    "\n",
    "dc_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 this worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 23:27:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scrapybot)\n",
      "2023-06-06 23:27:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.0.0 (OpenSSL 1.1.1t  7 Feb 2023), cryptography 39.0.1, Platform Windows-10-10.0.22621-SP0\n",
      "2023-06-06 23:27:45 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-06-06 23:27:45 [py.warnings] WARNING: c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\utils\\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-06-06 23:27:45 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-06-06 23:27:45 [scrapy.extensions.telnet] INFO: Telnet Password: f2c69d2ad1f48d13\n",
      "2023-06-06 23:27:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-06-06 23:27:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-06-06 23:27:46 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-06-06 23:27:46 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-06-06 23:27:46 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-06-06 23:27:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-06-06 23:27:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-06-06 23:27:46 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\tldextract\\.suffix_cache/publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\gingko2\\\\lib\\\\site-packages\\\\tldextract\\\\.suffix_cache'\n",
      "2023-06-06 23:27:46 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): publicsuffix.org:443\n",
      "2023-06-06 23:27:46 [urllib3.connectionpool] DEBUG: https://publicsuffix.org:443 \"GET /list/public_suffix_list.dat HTTP/1.1\" 200 77466\n",
      "2023-06-06 23:27:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikiquote.org/wiki/Maynard_James_Keenan> (referer: None)\n",
      "2023-06-06 23:27:47 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-06-06 23:27:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 245,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 12847,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.405144,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 6, 7, 6, 27, 47, 184117),\n",
      " 'httpcompression/response_bytes': 40707,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/DEBUG': 4,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 2,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2023, 6, 7, 6, 27, 46, 778973)}\n",
      "2023-06-06 23:27:47 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: '<li>Tool is not Slayer. I went to art school. I spent three years in the military. There\\'s more to me than throwing devil horns.\\n<ul><li>Aidin Vaziri (January 2, 2009) \"Maynard James Keenan: Hard rocker, winemaker. 5 Questions.\", <i><a href=\"https://en.wikipedia.org/wiki/San_Francisco_Chronicle\" class=\"extiw\" title=\"w:San Francisco Chronicle\">San Francisco Chronicle</a></i>, p. E3.</li></ul></li>',\n",
       " 1: '<li>I think there’s a reason why wine figures into so many religions. There’s something transcendent about it. It’s sort of the way that music is more than the sum of its parts. You have all these elements that make up the terroir that wine can communicate.\\n<ul><li>Matthew Lickona (December 24, 2008) <a rel=\"nofollow\" class=\"external text\" href=\"http://www.sandiegoreader.com/news/2008/dec/24/Hedonistic/\">\"Hedonistic\"</a>, <i>San Diego Reader</i>.</li></ul></li>',\n",
       " 2: '<li>You can grow grapes in almost any part of the world. You just have to develop your palate enough to realize wine is an expression of the place where you make it. You don\\'t have to take over the world; just be an artist and express your area.\\n<ul><li>George Varga (November 7, 2008) \"Tool lead singer hits the right notes in the winemaking community\", <i><a href=\"https://en.wikipedia.org/wiki/The_San_Diego_Union-Tribune\" class=\"extiw\" title=\"w:The San Diego Union-Tribune\">The San Diego Union-Tribune</a></i>, p. E-1.</li></ul></li>',\n",
       " 3: '<li>For the music, it’s not about the individual — so the more you let the music speak for yourself, the more powerful the music will be.\\n<ul><li>Carl Kozlowski (September 11, 2008) <a rel=\"nofollow\" class=\"external text\" href=\"http://www.pasadenaweekly.com/cms/story/detail/taste_in_the_making/6378/\">\"Taste in the making: Tool’s Maynard James Keenan shifts his focus from writing dark lyrics to creating zesty wines\"</a>, <i>Pasadena Weekly</i>. Southland Publishing.</li></ul></li>',\n",
       " 4: '<li>It’s in my blood. My great-grandfather made wine and it’s a tradition I want to pass on to my son.\\n<ul><li>On his work with his vineyard in Northern Arizona and wine label of the same name, <i>Caduceus</i> — reported in Jon Dolan (August 2006) <a rel=\"nofollow\" class=\"external text\" href=\"http://www.blender.com/guide/articles.aspx?id=2002\">\"33 Things You Should Know About Tool\"</a>, <i>Blender</i>, Alpha Media Group Inc.</li></ul></li>',\n",
       " 5: '<li>It\\'s the role of us to run our government, the government by the people, for the people, and I don\\'t think our government is listening to the people. It\\'s our role as patriots to question them, because we elected them. And if they\\'re not fairly and accurately representing us, it\\'s the job of the people, the patriots, to take their country back.\\n<ul><li>George Varga (October 31, 2004) \"Fired up and emoting on the state of politics, and more\", <i><a href=\"https://en.wikipedia.org/wiki/The_San_Diego_Union-Tribune\" class=\"extiw\" title=\"w:The San Diego Union-Tribune\">The San Diego Union-Tribune</a></i>, p. F-5.</li></ul></li>',\n",
       " 6: '<li>Every now and then, you get people who tend to forget what this country is about, which is a melting pot of races and cultures and freedom of speech.\\n<ul><li>George Varga (October 25, 2001) \"The Tool Man: Blistering band\\'s frontman puts his mind to the world\\'s problems\", <i><a href=\"https://en.wikipedia.org/wiki/The_San_Diego_Union-Tribune\" class=\"extiw\" title=\"w:The San Diego Union-Tribune\">The San Diego Union-Tribune</a></i>, p. 4.</li></ul></li>',\n",
       " 7: '<li>The army influences everything I do. Certainly it teaches you discipline, which is a necessary element of development. I think there\\'s more of a collaborative understanding in the band because of that.\\n<ul><li>Iain Shedden (July 20, 2001) \"Tool a bigger `threat\\' than any rapper\", <i><a href=\"https://en.wikipedia.org/wiki/The_Australian\" class=\"extiw\" title=\"w:The Australian\">The Australian</a></i>, p. 10.</li></ul></li>',\n",
       " 8: '<li>The process that we go through in recording with Tool is very organic, but at the same time it is very thought out. There is a very left-brain process of dissecting what we\\'re doing and drawing from source material; it\\'s very research oriented and esoteric.\\n<ul><li>Neil Strauss (March 29, 2000) \"A brain comes full circle: Rock musician Maynard James Keenan, of the bands Tool and A Perfect Circle\", <i><a href=\"https://en.wikipedia.org/wiki/The_New_York_Times\" class=\"extiw\" title=\"w:The New York Times\">The New York Times</a></i>, p. B3.</li></ul></li>',\n",
       " 9: '<li>One of my biggest heroes in music has been David Bowie. He\\'s said, `I\\'m going to be a painter now, or I\\'m going to do some films,\\' and his audience is very forgiving, because they understand him as an artist. Whether you agree or like the result, you respect that he\\'s expressing his artistic feelings.\\n<ul><li>The Record staff (March 22, 2000) \"Music News &amp; Notes\", <i><a href=\"https://en.wikipedia.org/wiki/New_Jersey_Record\" class=\"extiw\" title=\"w:New Jersey Record\">The Record</a></i>, p. Y4.</li></ul></li>',\n",
       " 10: '<li>It\\'s a purging of sorts. Like, when you\\'re all done doing your laundry and it\\'s fresh and bright, but washing the clothes, you wouldn\\'t want to get in while it\\'s spinning around.\\n<ul><li>On why his band Tool is like a washing machine — reported in Jim Sullivan (July 4, 1997) \"Lollapalooza Still Packs A Wallop\", <i><a href=\"https://en.wikipedia.org/wiki/Boston_Globe\" class=\"extiw\" title=\"w:Boston Globe\">Boston Globe</a></i>, p. D1.</li></ul></li>',\n",
       " 11: '<li>You really should be able to feel the higher power of music and be moved by it, rather than listening to me waffle on and having to explain it.\\n<ul><li>Steve Morse ( November 15, 1996) \"Sonic Evolution With the Use of Tool\", <i><a href=\"https://en.wikipedia.org/wiki/Boston_Globe\" class=\"extiw\" title=\"w:Boston Globe\">Boston Globe</a></i>, p. D14.</li></ul></li>',\n",
       " 12: '<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.maynardjameskeenan.com/\">MaynardJamesKeenan.com</a>, currently redirecting to the <a href=\"https://en.wikipedia.org/wiki/Puscifer\" class=\"extiw\" title=\"w:Puscifer\">Puscifer</a> website.</li>',\n",
       " 13: '<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.winespectator.com/Wine/Blogs/Blog_Main/0,4210,321,00.html\">Wine Spectator Online</a>, where Maynard maintained a blog about wine collecting and winemaking from 2006 to 2008.</li>',\n",
       " 14: '<li><a rel=\"nofollow\" class=\"external text\" href=\"https://www.imdb.com/name/nm0444665/\">Maynard James Keenan</a> on <a href=\"https://en.wikipedia.org/wiki/IMDb\" class=\"extiw\" title=\"w:IMDb\">IMDb</a></li>'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "class DCspider(scrapy.Spider):\n",
    "    name = \"dcspinder\"\n",
    "    def start_requests(self):\n",
    "        urls = [ 'https://en.wikiquote.org/wiki/Maynard_James_Keenan' ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request( url = url, callback = self.parse_front )\n",
    "    def parse_front(self, response):\n",
    "        # article_url = response.xpath('//a/@href=\"art-title\"').extract()\n",
    "        article_url = response.css('div.mw-parser-output > ul > li')\n",
    "        for index, url in enumerate(article_url):\n",
    "    #         yield response.follow(url=url, callback=self.parse_pages)\n",
    "            dc_dict[index] = url.extract()\n",
    "\n",
    "    # def parse_pages(self, response):\n",
    "    #     crs_title = response.xpath('//h1[contains(@class, \"title\")]/text()')\n",
    "    #     crs_title_ext = crs_title.extract_first().strip()\n",
    "    #     ch_titles = response.css('h4.chapter__title::text')\n",
    "    #     ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    #     dc_dict[crs_title_ext] = ch_titles_ext\n",
    "        \n",
    "dc_dict = dict()\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DCspider)\n",
    "process.start()\n",
    "\n",
    "dc_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 22:53:12 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scrapybot)\n",
      "2023-06-06 22:53:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.0.0 (OpenSSL 1.1.1t  7 Feb 2023), cryptography 39.0.1, Platform Windows-10-10.0.22621-SP0\n",
      "2023-06-06 22:53:12 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-06-06 22:53:12 [py.warnings] WARNING: c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\utils\\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-06-06 22:53:12 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-06-06 22:53:12 [scrapy.extensions.telnet] INFO: Telnet Password: e04908382c887cbc\n",
      "2023-06-06 22:53:12 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-06-06 22:53:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-06-06 22:53:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-06-06 22:53:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-06-06 22:53:12 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-06-06 22:53:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-06-06 22:53:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-06-06 22:53:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://emails.bmj.com/q/1fnLH65XUsNn7Iiph6kELOM/wv> (referer: None)\n",
      "2023-06-06 22:53:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://emails.bmj.com/q/1fnLH65XUsNn7Iiph6kELOM/wv> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\twisted\\internet\\defer.py\", line 892, in _runCallbacks\n",
      "    current.result = callback(  # type: ignore[misc]\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\spiders\\__init__.py\", line 76, in parse\n",
      "    raise NotImplementedError(\n",
      "NotImplementedError: DCspider.parse callback is not defined\n",
      "2023-06-06 22:53:13 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-06-06 22:53:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 246,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 51868,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.669686,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 6, 7, 5, 53, 13, 532560),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'spider_exceptions/NotImplementedError': 1,\n",
      " 'start_time': datetime.datetime(2023, 6, 7, 5, 53, 12, 862874)}\n",
      "2023-06-06 22:53:13 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "class DCspider(scrapy.Spider):\n",
    "    name=\"dcspider\"\n",
    "    def start_requests(self):\n",
    "        urls = [ 'https://emails.bmj.com/q/1fnLH65XUsNn7Iiph6kELOM/wv']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request( url = url, callback = self.parse )\n",
    "    def parse_front(self, response):\n",
    "        # article_url = response.xpath('//a/@href=\"art-title\"').extract()\n",
    "        article_url = response.css('a::attr(href)').extract()\n",
    "        for index, url in enumerate(article_url):\n",
    "            # yield response.follow(url=url, callback=self.parse_pages)\n",
    "            dc_dict[index] = url\n",
    "\n",
    "    # def parse_pages(self, response):\n",
    "    #     crs_title = response.xpath('//h1[contains(@class, \"title\")]/text()')\n",
    "    #     crs_title_ext = crs_title.extract_first().strip()\n",
    "    #     ch_titles = response.css('h4.chapter__title::text')\n",
    "    #     ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    #     dc_dict[crs_title_ext] = ch_titles_ext\n",
    "        \n",
    "dc_dict = dict()\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DCspider)\n",
    "process.start()\n",
    "\n",
    "dc_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 22:46:29 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scrapybot)\n",
      "2023-06-06 22:46:29 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.0.0 (OpenSSL 1.1.1t  7 Feb 2023), cryptography 39.0.1, Platform Windows-10-10.0.22621-SP0\n",
      "2023-06-06 22:46:29 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-06-06 22:46:29 [py.warnings] WARNING: c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\utils\\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-06-06 22:46:29 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-06-06 22:46:29 [scrapy.extensions.telnet] INFO: Telnet Password: 9c0dccb11f5f9e37\n",
      "2023-06-06 22:46:29 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-06-06 22:46:30 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-06-06 22:46:30 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-06-06 22:46:30 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-06-06 22:46:30 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-06-06 22:46:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-06-06 22:46:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-06-06 22:46:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
      "2023-06-06 22:46:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n",
      "2023-06-06 22:46:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
      "{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}\n",
      "2023-06-06 22:46:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
      "{'text': \"“This life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them. Also remember, sisters make the best friends in the world. As for lovers, well, they'll come and go too. And baby, I hate to say it, most of them - actually pretty much all of them are going to break your heart, but you can't give up because if you give up, you'll never find your soulmate. You'll never find that half who makes you whole and that goes for everything. Just because you fail once, doesn't mean you're gonna fail at everything. Keep trying, hold on, and always, always, always believe in yourself, because if you don't, then who will, sweetie? So keep your head high, keep your chin up, and most importantly, keep smiling, because life's a beautiful thing and there's so much to smile about.”\", 'author': 'Marilyn Monroe', 'tags': ['friends', 'heartbreak', 'inspirational', 'life', 'love', 'sisters']}\n",
      "2023-06-06 22:46:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
      "{'text': '“It takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.”', 'author': 'J.K. Rowling', 'tags': ['courage', 'friends']}\n",
      "2023-06-06 22:46:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
      "{'text': \"“If you can't explain it to a six year old, you don't understand it yourself.”\", 'author': 'Albert Einstein', 'tags': ['simplicity', 'understand']}\n",
      "2023-06-06 22:46:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
      "{'text': \"“You may not be her first, her last, or her only. She loved before she may love again. But if she loves you now, what else matters? She's not perfect—you aren't either, and the two of you may never be perfect together but if she can make you laugh, cause you to think twice, and admit to being human and making mistakes, hold onto her and give her the most you can. She may not be thinking about you every second of the day, but she will give you a part of her that she knows you can break—her heart. So don't hurt her, don't change her, don't analyze and don't expect more than she can give. Smile when she makes you happy, let her know when she makes you mad, and miss her when she's not there.”\", 'author': 'Bob Marley', 'tags': ['love']}\n",
      "2023-06-06 22:46:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
      "{'text': '“I like nonsense, it wakes up the brain cells. Fantasy is a necessary ingredient in living.”', 'author': 'Dr. Seuss', 'tags': ['fantasy']}\n",
      "2023-06-06 22:46:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
      "{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}\n",
      "2023-06-06 22:46:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
      "{'text': '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', 'author': 'Albert Einstein', 'tags': ['inspirational', 'life', 'live', 'miracle', 'miracles']}\n",
      "2023-06-06 22:46:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
      "{'text': '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', 'author': 'Jane Austen', 'tags': ['aliteracy', 'books', 'classic', 'humor']}\n",
      "2023-06-06 22:46:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
      "{'text': \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\", 'author': 'Marilyn Monroe', 'tags': ['be-yourself', 'inspirational']}\n",
      "2023-06-06 22:46:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
      "{'text': '“I may not have gone where I intended to go, but I think I have ended up where I needed to be.”', 'author': 'Douglas Adams', 'tags': ['life', 'navigation']}\n",
      "2023-06-06 22:46:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
      "{'text': \"“The opposite of love is not hate, it's indifference. The opposite of art is not ugliness, it's indifference. The opposite of faith is not heresy, it's indifference. And the opposite of life is not death, it's indifference.”\", 'author': 'Elie Wiesel', 'tags': ['activism', 'apathy', 'hate', 'indifference', 'inspirational', 'love', 'opposite', 'philosophy']}\n",
      "2023-06-06 22:46:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
      "{'text': '“It is not a lack of love, but a lack of friendship that makes unhappy marriages.”', 'author': 'Friedrich Nietzsche', 'tags': ['friendship', 'lack-of-friendship', 'lack-of-love', 'love', 'marriage', 'unhappy-marriage']}\n",
      "2023-06-06 22:46:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
      "{'text': '“Good friends, good books, and a sleepy conscience: this is the ideal life.”', 'author': 'Mark Twain', 'tags': ['books', 'contentment', 'friends', 'friendship', 'life']}\n",
      "2023-06-06 22:46:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
      "{'text': '“Life is what happens to us while we are making other plans.”', 'author': 'Allen Saunders', 'tags': ['fate', 'life', 'misattributed-john-lennon', 'planning', 'plans']}\n",
      "2023-06-06 22:46:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
      "{'text': '“Try not to become a man of success. Rather become a man of value.”', 'author': 'Albert Einstein', 'tags': ['adulthood', 'success', 'value']}\n",
      "2023-06-06 22:46:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
      "{'text': '“It is better to be hated for what you are than to be loved for what you are not.”', 'author': 'André Gide', 'tags': ['life', 'love']}\n",
      "2023-06-06 22:46:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
      "{'text': \"“I have not failed. I've just found 10,000 ways that won't work.”\", 'author': 'Thomas A. Edison', 'tags': ['edison', 'failure', 'inspirational', 'paraphrased']}\n",
      "2023-06-06 22:46:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
      "{'text': \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\", 'author': 'Eleanor Roosevelt', 'tags': ['misattributed-eleanor-roosevelt']}\n",
      "2023-06-06 22:46:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
      "{'text': '“A day without sunshine is like, you know, night.”', 'author': 'Steve Martin', 'tags': ['humor', 'obvious', 'simile']}\n",
      "2023-06-06 22:46:31 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-06-06 22:46:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 460,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 5183,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 0.481047,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 6, 7, 5, 46, 31, 37653),\n",
      " 'httpcompression/response_bytes': 24787,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 20,\n",
      " 'log_count/DEBUG': 23,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2023, 6, 7, 5, 46, 30, 556606)}\n",
      "2023-06-06 22:46:31 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "        \"https://quotes.toscrape.com/page/2/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"text\": quote.css(\"span.text::text\").get(),\n",
    "                \"author\": quote.css(\"small.author::text\").get(),\n",
    "                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n",
    "            }\n",
    "\n",
    "process = CrawlerProcess()\n",
    "process.crawl(QuotesSpider)\n",
    "process.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Towards Data Science](https://towardsdatascience.com/run-scrapy-code-from-jupyter-notebook-without-issues-69b7cb79530c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 23:11:35 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scrapybot)\n",
      "2023-06-06 23:11:35 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.0.0 (OpenSSL 1.1.1t  7 Feb 2023), cryptography 39.0.1, Platform Windows-10-10.0.22621-SP0\n",
      "2023-06-06 23:11:35 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-06-06 23:11:35 [py.warnings] WARNING: c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\utils\\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-06-06 23:11:36 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-06-06 23:11:36 [scrapy.extensions.telnet] INFO: Telnet Password: d24909908cfea7d4\n",
      "2023-06-06 23:11:36 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-06-06 23:11:36 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-06-06 23:11:36 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-06-06 23:11:36 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "['__main__.ExtractFirstLine']\n",
      "2023-06-06 23:11:36 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-06-06 23:11:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-06-06 23:11:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-06-06 23:11:37 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\tldextract\\.suffix_cache/publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\gingko2\\\\lib\\\\site-packages\\\\tldextract\\\\.suffix_cache'\n",
      "2023-06-06 23:11:37 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): publicsuffix.org:443\n",
      "2023-06-06 23:11:37 [urllib3.connectionpool] DEBUG: https://publicsuffix.org:443 \"GET /list/public_suffix_list.dat HTTP/1.1\" 200 77441\n",
      "2023-06-06 23:11:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikiquote.org/wiki/Maynard_James_Keenan> (referer: None)\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': \"Tool is not Slayer. I went to art school. I spent three years in the military. There's more to me than throwing devil horns.\"}\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': 'I think there’s a reason why wine figures into so many religions. There’s something transcendent about it. It’s sort of the way that music is more than the sum of its parts. You have all these elements that make up the terroir that wine can communicate.'}\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': \"You can grow grapes in almost any part of the world. You just have to develop your palate enough to realize wine is an expression of the place where you make it. You don't have to take over the world; just be an artist and express your area.\"}\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': 'For the music, it’s not about the individual — so the more you let the music speak for yourself, the more powerful the music will be.'}\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': 'It’s in my blood. My great-grandfather made wine and it’s a tradition I want to pass on to my son.'}\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': \"It's the role of us to run our government, the government by the people, for the people, and I don't think our government is listening to the people. It's our role as patriots to question them, because we elected them. And if they're not fairly and accurately representing us, it's the job of the people, the patriots, to take their country back.\"}\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': 'Every now and then, you get people who tend to forget what this country is about, which is a melting pot of races and cultures and freedom of speech.'}\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': \"The army influences everything I do. Certainly it teaches you discipline, which is a necessary element of development. I think there's more of a collaborative understanding in the band because of that.\"}\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': \"The process that we go through in recording with Tool is very organic, but at the same time it is very thought out. There is a very left-brain process of dissecting what we're doing and drawing from source material; it's very research oriented and esoteric.\"}\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': \"One of my biggest heroes in music has been David Bowie. He's said, `I'm going to be a painter now, or I'm going to do some films,' and his audience is very forgiving, because they understand him as an artist. Whether you agree or like the result, you respect that he's expressing his artistic feelings.\"}\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': \"It's a purging of sorts. Like, when you're all done doing your laundry and it's fresh and bright, but washing the clothes, you wouldn't want to get in while it's spinning around.\"}\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': 'You really should be able to feel the higher power of music and be moved by it, rather than listening to me waffle on and having to explain it.'}\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': 'MaynardJamesKeenan.com, currently redirecting to the Puscifer website.'}\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': 'Wine Spectator Online, where Maynard maintained a blog about wine collecting and winemaking from 2006 to 2008.'}\n",
      "2023-06-06 23:11:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikiquote.org/wiki/Maynard_James_Keenan>\n",
      "{'quote': 'Maynard James Keenan on IMDb'}\n",
      "2023-06-06 23:11:37 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-06-06 23:11:37 [scrapy.extensions.feedexport] INFO: Stored csv feed (15 items) in: quotes.csv\n",
      "2023-06-06 23:11:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 245,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 12847,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.43015,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 6, 7, 6, 11, 37, 492343),\n",
      " 'httpcompression/response_bytes': 40707,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'item_scraped_count': 15,\n",
      " 'log_count/DEBUG': 19,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 2,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2023, 6, 7, 6, 11, 37, 62193)}\n",
      "2023-06-06 23:11:37 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "# text cleaning\n",
    "import re\n",
    "\n",
    "class QuotesToCsv(scrapy.Spider):\n",
    "    \"\"\"scrape first line of  quotes from `wikiquote` by \n",
    "    Maynard James Keenan and save to json file\"\"\"\n",
    "    name = \"MJKQuotesToCsv\"\n",
    "    start_urls = [\n",
    "        'https://en.wikiquote.org/wiki/Maynard_James_Keenan',\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'ITEM_PIPELINES': {\n",
    "            '__main__.ExtractFirstLine': 1\n",
    "        },\n",
    "        'FEEDS': {\n",
    "            'quotes.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"parse data from urls\"\"\"\n",
    "        for quote in response.css('div.mw-parser-output > ul > li'):\n",
    "            yield {'quote': quote.extract()}\n",
    "\n",
    "class ExtractFirstLine(object):\n",
    "    def process_item(self, item, spider):\n",
    "        \"\"\"text processing\"\"\"\n",
    "        lines = dict(item)[\"quote\"].splitlines()\n",
    "        first_line = self.__remove_html_tags__(lines[0])\n",
    "\n",
    "        return {'quote': first_line}\n",
    "\n",
    "    def __remove_html_tags__(self, text):\n",
    "        \"\"\"remove html tags from string\"\"\"\n",
    "        html_tags = re.compile('<.*?>')\n",
    "        return re.sub(html_tags, '', text)\n",
    "    \n",
    "process = CrawlerProcess()\n",
    "process.crawl(QuotesToCsv)\n",
    "process.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without text `ExtractFirstLine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 23:15:48 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scrapybot)\n",
      "2023-06-06 23:15:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.0.0 (OpenSSL 1.1.1t  7 Feb 2023), cryptography 39.0.1, Platform Windows-10-10.0.22621-SP0\n",
      "2023-06-06 23:15:48 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-06-06 23:15:48 [py.warnings] WARNING: c:\\ProgramData\\Anaconda3\\envs\\gingko2\\lib\\site-packages\\scrapy\\utils\\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-06-06 23:15:48 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-06-06 23:15:48 [scrapy.extensions.telnet] INFO: Telnet Password: d18ea6d63c5591d9\n",
      "2023-06-06 23:15:48 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-06-06 23:15:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-06-06 23:15:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "# text cleaning\n",
    "import re\n",
    "\n",
    "class QuotesToCsv(scrapy.Spider):\n",
    "    \"\"\"scrape first line of  quotes from `wikiquote` by \n",
    "    Maynard James Keenan and save to json file\"\"\"\n",
    "    name = \"MJKQuotesToCsv\"\n",
    "    start_urls = [\n",
    "        'https://en.wikiquote.org/wiki/Maynard_James_Keenan',\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'ITEM_PIPELINES': {\n",
    "            '__main__.ExtractFirstLine': 1\n",
    "        },\n",
    "        'FEEDS': {\n",
    "            'quotes2.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"parse data from urls\"\"\"\n",
    "        for quote in response.css('div.mw-parser-output > ul > li'):\n",
    "            yield {'quote': quote.extract()}\n",
    "\n",
    "\n",
    "        return re.sub(html_tags, '', text)\n",
    "    \n",
    "process = CrawlerProcess()\n",
    "process.crawl(QuotesToCsv)\n",
    "process.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After installing crochet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "# text cleaning\n",
    "import re\n",
    "# Reactor restart\n",
    "from crochet import setup, wait_for\n",
    "setup()\n",
    "\n",
    "class QuotesToCsv(scrapy.Spider):\n",
    "    \"\"\"scrape first line of  quotes from `wikiquote` by \n",
    "    Maynard James Keenan and save to json file\"\"\"\n",
    "    name = \"MJKQuotesToCsv\"\n",
    "    start_urls = [\n",
    "        'https://en.wikiquote.org/wiki/Maynard_James_Keenan',\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'ITEM_PIPELINES': {\n",
    "            '__main__.ExtractFirstLine': 1\n",
    "        },\n",
    "        'FEEDS': {\n",
    "            'quotes.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"parse data from urls\"\"\"\n",
    "        for quote in response.css('div.mw-parser-output > ul > li'):\n",
    "            yield {'quote': quote.extract()}\n",
    "\n",
    "\n",
    "class ExtractFirstLine(object):\n",
    "    def process_item(self, item, spider):\n",
    "        \"\"\"text processing\"\"\"\n",
    "        lines = dict(item)[\"quote\"].splitlines()\n",
    "        first_line = self.__remove_html_tags__(lines[0])\n",
    "\n",
    "        return {'quote': first_line}\n",
    "\n",
    "    def __remove_html_tags__(self, text):\n",
    "        \"\"\"remove html tags from string\"\"\"\n",
    "        html_tags = re.compile('<.*?>')\n",
    "        return re.sub(html_tags, '', text)\n",
    "\n",
    "@wait_for(10)\n",
    "def run_spider():\n",
    "    \"\"\"run spider with MJKQuotesToCsv\"\"\"\n",
    "    crawler = CrawlerRunner()\n",
    "    d = crawler.crawl(QuotesToCsv)\n",
    "    return d\n",
    "\n",
    "run_spider()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<li>Tool is not Slayer. I went to art school. I spent three years in the military. There\\'s more to me than throwing devil horns.\\n<ul><li>Aidin Vaziri (January 2, 2009) \"Maynard James Keenan: Hard rocker, winemaker. 5 Questions.\", <i><a href=\"https://en.wikipedia.org/wiki/San_Francisco_Chronicle\" class=\"extiw\" title=\"w:San Francisco Chronicle\">San Francisco Chronicle</a></i>, p. E3.</li></ul></li>',\n",
       " 1: '<li>I think there’s a reason why wine figures into so many religions. There’s something transcendent about it. It’s sort of the way that music is more than the sum of its parts. You have all these elements that make up the terroir that wine can communicate.\\n<ul><li>Matthew Lickona (December 24, 2008) <a rel=\"nofollow\" class=\"external text\" href=\"http://www.sandiegoreader.com/news/2008/dec/24/Hedonistic/\">\"Hedonistic\"</a>, <i>San Diego Reader</i>.</li></ul></li>',\n",
       " 2: '<li>You can grow grapes in almost any part of the world. You just have to develop your palate enough to realize wine is an expression of the place where you make it. You don\\'t have to take over the world; just be an artist and express your area.\\n<ul><li>George Varga (November 7, 2008) \"Tool lead singer hits the right notes in the winemaking community\", <i><a href=\"https://en.wikipedia.org/wiki/The_San_Diego_Union-Tribune\" class=\"extiw\" title=\"w:The San Diego Union-Tribune\">The San Diego Union-Tribune</a></i>, p. E-1.</li></ul></li>',\n",
       " 3: '<li>For the music, it’s not about the individual — so the more you let the music speak for yourself, the more powerful the music will be.\\n<ul><li>Carl Kozlowski (September 11, 2008) <a rel=\"nofollow\" class=\"external text\" href=\"http://www.pasadenaweekly.com/cms/story/detail/taste_in_the_making/6378/\">\"Taste in the making: Tool’s Maynard James Keenan shifts his focus from writing dark lyrics to creating zesty wines\"</a>, <i>Pasadena Weekly</i>. Southland Publishing.</li></ul></li>',\n",
       " 4: '<li>It’s in my blood. My great-grandfather made wine and it’s a tradition I want to pass on to my son.\\n<ul><li>On his work with his vineyard in Northern Arizona and wine label of the same name, <i>Caduceus</i> — reported in Jon Dolan (August 2006) <a rel=\"nofollow\" class=\"external text\" href=\"http://www.blender.com/guide/articles.aspx?id=2002\">\"33 Things You Should Know About Tool\"</a>, <i>Blender</i>, Alpha Media Group Inc.</li></ul></li>',\n",
       " 5: '<li>It\\'s the role of us to run our government, the government by the people, for the people, and I don\\'t think our government is listening to the people. It\\'s our role as patriots to question them, because we elected them. And if they\\'re not fairly and accurately representing us, it\\'s the job of the people, the patriots, to take their country back.\\n<ul><li>George Varga (October 31, 2004) \"Fired up and emoting on the state of politics, and more\", <i><a href=\"https://en.wikipedia.org/wiki/The_San_Diego_Union-Tribune\" class=\"extiw\" title=\"w:The San Diego Union-Tribune\">The San Diego Union-Tribune</a></i>, p. F-5.</li></ul></li>',\n",
       " 6: '<li>Every now and then, you get people who tend to forget what this country is about, which is a melting pot of races and cultures and freedom of speech.\\n<ul><li>George Varga (October 25, 2001) \"The Tool Man: Blistering band\\'s frontman puts his mind to the world\\'s problems\", <i><a href=\"https://en.wikipedia.org/wiki/The_San_Diego_Union-Tribune\" class=\"extiw\" title=\"w:The San Diego Union-Tribune\">The San Diego Union-Tribune</a></i>, p. 4.</li></ul></li>',\n",
       " 7: '<li>The army influences everything I do. Certainly it teaches you discipline, which is a necessary element of development. I think there\\'s more of a collaborative understanding in the band because of that.\\n<ul><li>Iain Shedden (July 20, 2001) \"Tool a bigger `threat\\' than any rapper\", <i><a href=\"https://en.wikipedia.org/wiki/The_Australian\" class=\"extiw\" title=\"w:The Australian\">The Australian</a></i>, p. 10.</li></ul></li>',\n",
       " 8: '<li>The process that we go through in recording with Tool is very organic, but at the same time it is very thought out. There is a very left-brain process of dissecting what we\\'re doing and drawing from source material; it\\'s very research oriented and esoteric.\\n<ul><li>Neil Strauss (March 29, 2000) \"A brain comes full circle: Rock musician Maynard James Keenan, of the bands Tool and A Perfect Circle\", <i><a href=\"https://en.wikipedia.org/wiki/The_New_York_Times\" class=\"extiw\" title=\"w:The New York Times\">The New York Times</a></i>, p. B3.</li></ul></li>',\n",
       " 9: '<li>One of my biggest heroes in music has been David Bowie. He\\'s said, `I\\'m going to be a painter now, or I\\'m going to do some films,\\' and his audience is very forgiving, because they understand him as an artist. Whether you agree or like the result, you respect that he\\'s expressing his artistic feelings.\\n<ul><li>The Record staff (March 22, 2000) \"Music News &amp; Notes\", <i><a href=\"https://en.wikipedia.org/wiki/New_Jersey_Record\" class=\"extiw\" title=\"w:New Jersey Record\">The Record</a></i>, p. Y4.</li></ul></li>',\n",
       " 10: '<li>It\\'s a purging of sorts. Like, when you\\'re all done doing your laundry and it\\'s fresh and bright, but washing the clothes, you wouldn\\'t want to get in while it\\'s spinning around.\\n<ul><li>On why his band Tool is like a washing machine — reported in Jim Sullivan (July 4, 1997) \"Lollapalooza Still Packs A Wallop\", <i><a href=\"https://en.wikipedia.org/wiki/Boston_Globe\" class=\"extiw\" title=\"w:Boston Globe\">Boston Globe</a></i>, p. D1.</li></ul></li>',\n",
       " 11: '<li>You really should be able to feel the higher power of music and be moved by it, rather than listening to me waffle on and having to explain it.\\n<ul><li>Steve Morse ( November 15, 1996) \"Sonic Evolution With the Use of Tool\", <i><a href=\"https://en.wikipedia.org/wiki/Boston_Globe\" class=\"extiw\" title=\"w:Boston Globe\">Boston Globe</a></i>, p. D14.</li></ul></li>',\n",
       " 12: '<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.maynardjameskeenan.com/\">MaynardJamesKeenan.com</a>, currently redirecting to the <a href=\"https://en.wikipedia.org/wiki/Puscifer\" class=\"extiw\" title=\"w:Puscifer\">Puscifer</a> website.</li>',\n",
       " 13: '<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.winespectator.com/Wine/Blogs/Blog_Main/0,4210,321,00.html\">Wine Spectator Online</a>, where Maynard maintained a blog about wine collecting and winemaking from 2006 to 2008.</li>',\n",
       " 14: '<li><a rel=\"nofollow\" class=\"external text\" href=\"https://www.imdb.com/name/nm0444665/\">Maynard James Keenan</a> on <a href=\"https://en.wikipedia.org/wiki/IMDb\" class=\"extiw\" title=\"w:IMDb\">IMDb</a></li>'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "\n",
    "class DCspider(scrapy.Spider):\n",
    "    name = \"dcspider\"\n",
    "    def start_requests(self):\n",
    "        urls = [ 'https://en.wikiquote.org/wiki/Maynard_James_Keenan' ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request( url = url, callback = self.parse_front )\n",
    "    def parse_front(self, response):\n",
    "        # article_url = response.xpath('//a/@href=\"art-title\"').extract()\n",
    "        article_url = response.css('div.mw-parser-output > ul > li')\n",
    "        for index, url in enumerate(article_url):\n",
    "    #         yield response.follow(url=url, callback=self.parse_pages)\n",
    "            dc_dict[index] = url.extract()\n",
    "\n",
    "    # def parse_pages(self, response):\n",
    "    #     crs_title = response.xpath('//h1[contains(@class, \"title\")]/text()')\n",
    "    #     crs_title_ext = crs_title.extract_first().strip()\n",
    "    #     ch_titles = response.css('h4.chapter__title::text')\n",
    "    #     ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    #     dc_dict[crs_title_ext] = ch_titles_ext\n",
    "        \n",
    "@wait_for(10)\n",
    "def run_spider():\n",
    "    crawler = CrawlerRunner()\n",
    "    d = crawler.crawl(DCspider)\n",
    "    return d\n",
    "\n",
    "dc_dict = dict()\n",
    "\n",
    "run_spider()\n",
    "dc_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 This worked to create dict with title and URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Developing best practice principles for the provision of programs and services to people transitioning from custody to the community: study protocol for a modified Delphi consensus exercise': 'https://emails.bmj.com/c/1aCA1LBbD1bgnHOk8DYQwZ6K6zgq',\n",
       " 'Comparing the effects of mHealth application based on micro-learning method and face-to-face training on treatment adherence and perception in haemodialysis patients: a randomised clinical trial': 'https://emails.bmj.com/c/1aCA2fZlWmUfdHqbqnfQbrPfGeza',\n",
       " 'Two-step offer and return of multiple types of additional genomic findings to families after ultrarapid trio genomic testing in the acute care setting: a study protocol': 'https://emails.bmj.com/c/1aCA2ZzBpouItGPYlYakF8U12Jwh',\n",
       " 'Assessment of patients’ preferences for new anticancer drugs in China: a best–worst discrete choice experiment on three common cancer types': 'https://emails.bmj.com/c/1aCA3tXLIKdHjGrPDHrkjBCwCoP1',\n",
       " 'CT perfusion for Assessment of poor Neurological outcome in Comatose Cardiac Arrest Patients (CANCCAP): protocol for a prospective study': 'https://emails.bmj.com/c/1aCA3YlW25WG9G3GVqIjY4l2c47L',\n",
       " 'Involving adolescents in the design, implementation, evaluation and dissemination of health research: an umbrella review protocol': 'https://emails.bmj.com/c/1aCA4sK6lrFEZFFyd9ZjCx3xLJqv',\n",
       " 'How regional versus global thresholds for physical activity and grip strength influence physical frailty prevalence and mortality estimates in PURE: a prospective multinational cohort study of community-dwelling adults': 'https://emails.bmj.com/c/1aCA4X8gENoDPFhpuTgjgZM3loJf',\n",
       " 'MODIFI: protocol for randomised feasibility study of eye-movement desensitisation and reprocessing therapy (EMDR) for functional neurological disorder (FND)': 'https://emails.bmj.com/c/1aCA5GIw7OZ75EHcquaNKGQOHTGm',\n",
       " 'Common factors, Responsiveness and Outcome in Psychotherapy (CROP): study protocol for a naturalistic prospective cohort study of psychotherapy in Denmark': 'https://emails.bmj.com/c/1aCA6b6GraI5VEj3IdrNp9zkhyZ6',\n",
       " 'Randomised, controlled, open-label pragmatic trial evaluating changes in functional exercise capacity after primary care PUlmonary REhabilitation in patients with long COVID: protocol of the PuRe-COVID trial in Belgium': 'https://emails.bmj.com/c/1aCA6FuQKwr4LDUUZWIN3ChPRehQ',\n",
       " 'Prevalence and factors associated with the utilisation of modern contraceptive methods among married women of childbearing age in Yemen: a secondary analysis of national survey data': 'https://emails.bmj.com/c/1aCA79T13Sa3BDwMhFZMI50lqTAA',\n",
       " 'Dietary patterns in rural and metropolitan Australia: a cross-sectional study exploring dietary patterns, inflammation and association with cardiovascular disease risk factors': 'https://emails.bmj.com/c/1aCA7EhbndT2rD8DzpgMmxIR0yTk'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "\n",
    "class myspider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "    def start_requests(self):\n",
    "        urls = [ 'https://emails.bmj.com/q/1fnLH65XUsNn7Iiph6kELOM/wv' ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request( url = url, callback = self.parse_front )\n",
    "    def parse_front(self, response):\n",
    "        article_title = response.css('a.art-title > font::text').extract()\n",
    "        article_url = response.xpath('//a[@class=\"art-title\"]/@href').extract()\n",
    "        for index, url in enumerate(article_url):\n",
    "            yield response.follow(url=url, callback=self.parse_pages)\n",
    "            article_dict[article_title[index]] = url\n",
    "\n",
    "    def parse_pages(self, response):\n",
    "        crs_title = response.xpath('//h1[contains(@class, \"title\")]/text()')\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "        ch_titles = response.css('h4.chapter__title::text')\n",
    "        ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "        dc_dict[crs_title_ext] = ch_titles_ext\n",
    "        \n",
    "@wait_for(10)\n",
    "def run_spider():\n",
    "    crawler = CrawlerRunner()\n",
    "    d = crawler.crawl(myspider)\n",
    "    return d\n",
    "\n",
    "article_dict = dict()\n",
    "\n",
    "run_spider()\n",
    "article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
