{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "from silvhua import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the option to wrap text within cells\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_dict = dict()\n",
    "chatbot_dict = dict()\n",
    "simple_summaries_dict = dict()\n",
    "relevance_dict = dict()\n",
    "save = True\n",
    "# save_outputs = False\n",
    "save_outputs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copied from [previous notebook](2023-07-11%20create%20summaries%20table.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    full_template = mapped_column(Text)\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    original_headline = mapped_column(String(255))\n",
    "    simple_summary = mapped_column(Text)\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=5):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries', engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    data = Sources(\n",
    "                        title=row['title'],\n",
    "                        text=row['text'],\n",
    "                        abstract=row['abstract'],\n",
    "                        publication=row['publication'],\n",
    "                        authors=row['authors'],\n",
    "                        year=row['year'],\n",
    "                        month=row['month'],\n",
    "                        pub_volume=row['pub_volume'],\n",
    "                        pub_issue=row['pub_issue'],\n",
    "                        start_page=row['start_page'],\n",
    "                        end_page=row['end_page'],\n",
    "                        doi=row['doi']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                    print(f'\\t{row[\"title\"]}')\n",
    "                elif table == 'summaries':\n",
    "                # if table == 'summaries':\n",
    "\n",
    "                    # Check if prompt already exists in the database\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full_summarize_task'],\n",
    "                        ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['summarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                    print(f'\\tReference #{row[\"reference_id\"]}: {row[\"headline\"]}')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.reference_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['timestamp'] = str(datetime.now(pytz.timezone('Canada/Pacific')))\n",
    "        self.qna['reference_id'] = self.reference_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model\n",
    "        self.qna['prep_step'] = prep_step.strip()\n",
    "        self.qna['summarize_task'] = task.strip()\n",
    "        self.qna['edit_task'] = edit_task.strip()\n",
    "        self.qna['simplify_task'] = simplify_task.strip()\n",
    "        self.qna['simplify_audience'] = simplify_audience.strip()\n",
    "        self.qna['format_task'] = format_task.strip()\n",
    "        self.qna['full_summarize_task'] = full_task.strip()\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1.5\n",
    "n_choices = 2\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "save_outputs=True\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "## Add rows from references dataframe\n",
    "# bulk_append(references_df)\n",
    "\n",
    "\n",
    "sources_df = get_table(table='sources', limit=3)\n",
    "\n",
    "chaining_dict = batch_summarize(\n",
    "    sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chatbot_dict,\n",
    "    system_role=system_role, model=model, max_tokens=1000,\n",
    "    n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "    iteration_id=iteration_id, save_outputs=save_outputs\n",
    "    )\n",
    "# # chaining_dict[iteration_id]\n",
    "qna_dict = create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "    )\n",
    "# Add rows from results to summaries and prompts table\n",
    "bulk_append(qna_dict[iteration_id])\n",
    "qna_dict[iteration_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the SQLAlchemy objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from summaries\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP, Numeric\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    full_template = mapped_column(Text)\n",
    "    system_role = mapped_column(String(300))\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    rating_original_content = mapped_column(Integer) #\n",
    "    simple_summary = mapped_column(Text)\n",
    "    # rating_simple_content = mapped_column(Integer) #\n",
    "    original_headline = mapped_column(String(255))\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    model = mapped_column(String(70))\n",
    "    temperature = mapped_column(Numeric)\n",
    "\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=None):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries', engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    data = Sources(\n",
    "                        title=row['title'],\n",
    "                        text=row['text'],\n",
    "                        abstract=row['abstract'],\n",
    "                        publication=row['publication'],\n",
    "                        authors=row['authors'],\n",
    "                        year=row['year'],\n",
    "                        month=row['month'],\n",
    "                        pub_volume=row['pub_volume'],\n",
    "                        pub_issue=row['pub_issue'],\n",
    "                        start_page=row['start_page'],\n",
    "                        end_page=row['end_page'],\n",
    "                        doi=row['doi']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                    print(f'\\t{row[\"title\"]}')\n",
    "                elif table == 'summaries':\n",
    "                # if table == 'summaries':\n",
    "\n",
    "                    # Check if prompt already exists in the database\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full_summarize_task'],\n",
    "                        ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['summarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                    print(f'\\tReference #{row[\"reference_id\"]}: {row[\"headline\"]}')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.reference_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['timestamp'] = str(datetime.now(pytz.timezone('Canada/Pacific')))\n",
    "        self.qna['reference_id'] = self.reference_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model        \n",
    "        self.qna['temperature'] = self.temperature\n",
    "        self.qna['prep_step'] = prep_step.strip()\n",
    "        self.qna['summarize_task'] = task.strip()\n",
    "        self.qna['edit_task'] = edit_task.strip()\n",
    "        self.qna['simplify_task'] = simplify_task.strip()\n",
    "        self.qna['simplify_audience'] = simplify_audience.strip()\n",
    "        self.qna['format_task'] = format_task.strip()\n",
    "        self.qna['full_summarize_task'] = full_task.strip()\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1.5\n",
    "n_choices = 2\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "save_outputs=True\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "summaries = get_table(table='summaries')\n",
    "\n",
    "\n",
    "# sources_df = get_table(table='sources', limit=3)\n",
    "\n",
    "# chaining_dict = batch_summarize(\n",
    "#     sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "#     simplify_task, simplify_audience, format_task,\n",
    "#     chatbot_dict,\n",
    "#     system_role=system_role, model=model, max_tokens=1000,\n",
    "#     n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "#     iteration_id=iteration_id, save_outputs=save_outputs\n",
    "#     )\n",
    "# # # chaining_dict[iteration_id]\n",
    "# qna_dict = create_summaries_df(\n",
    "#     qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "#     )\n",
    "# # Add rows from results to summaries and prompts table\n",
    "# bulk_append(qna_dict[iteration_id])\n",
    "# qna_dict[iteration_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>original_summary</th>\n",
       "      <th>original_headline</th>\n",
       "      <th>simple_summary</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>reference_id</th>\n",
       "      <th>choice</th>\n",
       "      <th>rating_original_content</th>\n",
       "      <th>rating_simple_content</th>\n",
       "      <th>model</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-07-12 06:35:24.458418+00:00</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "      <td>New Research on Recovery from Exercise in Midd...</td>\n",
       "      <td>Check out this new research that shows how par...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-12 06:12:32.880097+00:00</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "      <td>New Study Shows How Exercise Impacts Muscle Re...</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-07-12 06:35:24.458418+00:00</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "      <td>New Research Shows How Recreational Training C...</td>\n",
       "      <td>New research suggests that engaging in regular...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-07-12 06:35:28.609483+00:00</td>\n",
       "      <td>A recent study found that a high calcium and h...</td>\n",
       "      <td>New Study Shows Nutritional Intervention Reduc...</td>\n",
       "      <td>A recent study has shown that a simple dietary...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-07-12 06:35:28.609483+00:00</td>\n",
       "      <td>A recent study found that a high calcium and h...</td>\n",
       "      <td>New Study Shows Nutritional Intervention Can R...</td>\n",
       "      <td>A recent study has found that a simple dietary...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2023-07-12 06:35:33.993335+00:00</td>\n",
       "      <td>Exercise snacks, which are short bouts of vigo...</td>\n",
       "      <td>New Research Shows Exercise Snacks Improve Hea...</td>\n",
       "      <td>New research has found that short bursts of vi...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2023-07-12 06:35:33.993335+00:00</td>\n",
       "      <td>Exercise snacks, short bursts of vigorous exer...</td>\n",
       "      <td>Exercise Snacks: A Time-Efficient Way to Impro...</td>\n",
       "      <td>Exercise snacks are a convenient and time-effi...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                        timestamp  \\\n",
       "0   2 2023-07-12 06:35:24.458418+00:00   \n",
       "1   1 2023-07-12 06:12:32.880097+00:00   \n",
       "2   3 2023-07-12 06:35:24.458418+00:00   \n",
       "3   4 2023-07-12 06:35:28.609483+00:00   \n",
       "4   5 2023-07-12 06:35:28.609483+00:00   \n",
       "5   6 2023-07-12 06:35:33.993335+00:00   \n",
       "6   7 2023-07-12 06:35:33.993335+00:00   \n",
       "\n",
       "                                    original_summary  \\\n",
       "0  A recent study compared the recovery response ...   \n",
       "1  A recent study compared the recovery response ...   \n",
       "2  A recent study compared the recovery response ...   \n",
       "3  A recent study found that a high calcium and h...   \n",
       "4  A recent study found that a high calcium and h...   \n",
       "5  Exercise snacks, which are short bouts of vigo...   \n",
       "6  Exercise snacks, short bursts of vigorous exer...   \n",
       "\n",
       "                                   original_headline  \\\n",
       "0  New Research on Recovery from Exercise in Midd...   \n",
       "1  New Study Shows How Exercise Impacts Muscle Re...   \n",
       "2  New Research Shows How Recreational Training C...   \n",
       "3  New Study Shows Nutritional Intervention Reduc...   \n",
       "4  New Study Shows Nutritional Intervention Can R...   \n",
       "5  New Research Shows Exercise Snacks Improve Hea...   \n",
       "6  Exercise Snacks: A Time-Efficient Way to Impro...   \n",
       "\n",
       "                                      simple_summary  prompt_id  reference_id  \\\n",
       "0  Check out this new research that shows how par...          5             1   \n",
       "1  A recent study compared the recovery response ...          5             1   \n",
       "2  New research suggests that engaging in regular...          5             1   \n",
       "3  A recent study has shown that a simple dietary...          5             2   \n",
       "4  A recent study has found that a simple dietary...          5             2   \n",
       "5  New research has found that short bursts of vi...          5             3   \n",
       "6  Exercise snacks are a convenient and time-effi...          5             3   \n",
       "\n",
       "   choice  rating_original_content  rating_simple_content  \\\n",
       "0       1                        3                      3   \n",
       "1       1                        3                      3   \n",
       "2       2                        4                      2   \n",
       "3       1                        4                      3   \n",
       "4       2                        4                      4   \n",
       "5       1                        3                      3   \n",
       "6       2                        3                      4   \n",
       "\n",
       "                    model temperature  \n",
       "0  gpt-3.5-turbo-16k-0613         0.7  \n",
       "1  gpt-3.5-turbo-16k-0613         0.7  \n",
       "2  gpt-3.5-turbo-16k-0613         0.7  \n",
       "3  gpt-3.5-turbo-16k-0613         0.7  \n",
       "4  gpt-3.5-turbo-16k-0613         0.7  \n",
       "5  gpt-3.5-turbo-16k-0613         0.7  \n",
       "6  gpt-3.5-turbo-16k-0613         0.7  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-0613\n",
      "gpt-4-0314\n",
      "gpt-3.5-turbo-16k-0613\n",
      "gpt-3.5-turbo-0613\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-4\n",
      "gpt-3.5-turbo-0301\n",
      "gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "from orm_summarize import openai_models\n",
    "models_available = openai_models(env=\"api_openai\", query='gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-0613\n",
      "gpt-4-0314\n",
      "gpt-4\n"
     ]
    }
   ],
   "source": [
    "models_available = openai_models(env=\"api_openai\", query='4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate summaries with prompts imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from sources LIMIT 3\n",
      "**Text #1 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-4\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-4\n",
      "\t\tRequesting 2 choices using gpt-4\n",
      "An error occurred on line 246 in C:\\Users\\silvh\\AppData\\Local\\Temp\\ipykernel_23984\\3007662155.py : Rate limit reached for 10KTPM-200RPM in organization org-4l8HUKDtXhH0T7iFErf1JSJg on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues.\n",
      "\t**API request failed for `.summarize()`**\n",
      "\t...Completed\n",
      "**Text #2 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-4\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-4\n",
      "\t\tRequesting 2 choices using gpt-4\n",
      "An error occurred on line 246 in C:\\Users\\silvh\\AppData\\Local\\Temp\\ipykernel_23984\\3007662155.py : Rate limit reached for 10KTPM-200RPM in organization org-4l8HUKDtXhH0T7iFErf1JSJg on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues.\n",
      "\t**API request failed for `.summarize()`**\n",
      "\t...Completed\n",
      "**Text #3 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-4\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-4\n",
      "\t\tRequesting 2 choices using gpt-4\n",
      "An error occurred on line 246 in C:\\Users\\silvh\\AppData\\Local\\Temp\\ipykernel_23984\\3007662155.py : Rate limit reached for 10KTPM-200RPM in organization org-4l8HUKDtXhH0T7iFErf1JSJg on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues.\n",
      "\t**API request failed for `.summarize()`**\n",
      "\t...Completed\n",
      "1_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "2_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "3_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "Object saved as JSON: ../text/2023-07-11 for db//batch_Chaining_attributes_initial_2023-07-12_1102.json\n",
      "Processing 1_prompt00...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 399\u001b[0m\n\u001b[0;32m    390\u001b[0m chaining_dict \u001b[39m=\u001b[39m batch_summarize(\n\u001b[0;32m    391\u001b[0m     sources_df, folder_path, prep_step, summarize_task, edit_task, \n\u001b[0;32m    392\u001b[0m     simplify_task, simplify_audience, format_task,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m     iteration_id\u001b[39m=\u001b[39miteration_id, save_outputs\u001b[39m=\u001b[39msave_outputs\n\u001b[0;32m    397\u001b[0m     )\n\u001b[0;32m    398\u001b[0m \u001b[39m# # chaining_dict[iteration_id]\u001b[39;00m\n\u001b[1;32m--> 399\u001b[0m qna_dict \u001b[39m=\u001b[39m create_summaries_df(\n\u001b[0;32m    400\u001b[0m     qna_dict, chatbot_dict, iteration_id, chatbot_id\u001b[39m=\u001b[39;49mchatbot_id\n\u001b[0;32m    401\u001b[0m     )\n\u001b[0;32m    402\u001b[0m \u001b[39m# Add rows from results to summaries and prompts table\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[39m# bulk_append(qna_dict[iteration_id])\u001b[39;00m\n\u001b[0;32m    404\u001b[0m qna_dict[iteration_id]\n",
      "Cell \u001b[1;32mIn[24], line 342\u001b[0m, in \u001b[0;36mcreate_summaries_df\u001b[1;34m(qna_dict, chatbot_dict, iteration_id, chatbot_id)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39mfor\u001b[39;00m chatbot_key \u001b[39min\u001b[39;00m chatbot_dict[chatbot_id]\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    339\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mProcessing \u001b[39m\u001b[39m{\u001b[39;00mchatbot_key\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    340\u001b[0m     dfs_list\u001b[39m.\u001b[39mappend(pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[0;32m    341\u001b[0m         chatbot_dict[chatbot_id][chatbot_key]\u001b[39m.\u001b[39mqna, \n\u001b[1;32m--> 342\u001b[0m         index\u001b[39m=\u001b[39m[choice \u001b[39mfor\u001b[39;00m choice \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(chatbot_dict[chatbot_id][chatbot_key]\u001b[39m.\u001b[39;49mqna[\u001b[39m'\u001b[39;49m\u001b[39msummary\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)])\n\u001b[0;32m    343\u001b[0m         )\n\u001b[0;32m    345\u001b[0m qna_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(dfs_list)\u001b[39m.\u001b[39mreset_index(names\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mchoice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    346\u001b[0m qna_df \u001b[39m=\u001b[39m extract_summary(qna_df, \u001b[39m'\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'summary'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP, Numeric\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    full_template = mapped_column(Text)\n",
    "    system_role = mapped_column(String(300))\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    rating_original_content = mapped_column(Integer) #\n",
    "    simple_summary = mapped_column(Text)\n",
    "    # rating_simple_content = mapped_column(Integer) #\n",
    "    original_headline = mapped_column(String(255))\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    model = mapped_column(String(70))\n",
    "    temperature = mapped_column(Numeric)\n",
    "\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=None):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries', engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    data = Sources(\n",
    "                        title=row['title'],\n",
    "                        text=row['text'],\n",
    "                        abstract=row['abstract'],\n",
    "                        publication=row['publication'],\n",
    "                        authors=row['authors'],\n",
    "                        year=row['year'],\n",
    "                        month=row['month'],\n",
    "                        pub_volume=row['pub_volume'],\n",
    "                        pub_issue=row['pub_issue'],\n",
    "                        start_page=row['start_page'],\n",
    "                        end_page=row['end_page'],\n",
    "                        doi=row['doi']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                    print(f'\\t{row[\"title\"]}')\n",
    "                elif table == 'summaries':\n",
    "                # if table == 'summaries':\n",
    "\n",
    "                    # Check if prompt already exists in the database\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full_summarize_task'],\n",
    "                        ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['summarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                    print(f'\\tReference #{row[\"reference_id\"]}: {row[\"headline\"]}')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "from prompts import *\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.reference_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['timestamp'] = str(datetime.now(pytz.timezone('Canada/Pacific')))\n",
    "        self.qna['reference_id'] = self.reference_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model        \n",
    "        self.qna['temperature'] = self.temperature\n",
    "        self.qna['prep_step'] = prep_step.strip()\n",
    "        self.qna['summarize_task'] = task.strip()\n",
    "        self.qna['edit_task'] = edit_task.strip()\n",
    "        self.qna['simplify_task'] = simplify_task.strip()\n",
    "        self.qna['simplify_audience'] = simplify_audience.strip()\n",
    "        self.qna['format_task'] = format_task.strip()\n",
    "        self.qna['full_summarize_task'] = full_task.strip()\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1\n",
    "n_choices = 2\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "# model = 'gpt-3.5-turbo-16k-0613'\n",
    "model = 'gpt-4'\n",
    "save_outputs=True\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "# summaries = get_table(table='summaries')\n",
    "\n",
    "\n",
    "sources_df = get_table(table='sources', limit=3)\n",
    "\n",
    "chaining_dict = batch_summarize(\n",
    "    sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chatbot_dict,\n",
    "    system_role=system_role, model=model, max_tokens=1000,\n",
    "    n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "    iteration_id=iteration_id, save_outputs=save_outputs\n",
    "    )\n",
    "# # chaining_dict[iteration_id]\n",
    "qna_dict = create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "    )\n",
    "# Add rows from results to summaries and prompts table\n",
    "# bulk_append(qna_dict[iteration_id])\n",
    "qna_dict[iteration_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 1: Increase temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from sources\n",
      "**Text #1 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "**Text #2 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "**Text #3 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "**Text #4 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "**Text #5 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "**Text #6 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "1_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "2_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "3_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "4_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "5_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "6_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "Object saved as JSON: ../text/2023-07-11 for db//batch_Chaining_attributes_initial_2023-07-12_1124.json\n",
      "Processing 1_prompt00...\n",
      "Processing 2_prompt00...\n",
      "Processing 3_prompt00...\n",
      "Processing 4_prompt00...\n",
      "Processing 5_prompt00...\n",
      "Processing 6_prompt00...\n",
      "Original summaries DataFrame shape: (12, 19)\n",
      "\tOriginal summaries Dataframe columns: Index(['choice', 'timestamp', 'reference_id', 'article_title', 'text',\n",
      "       'system_role', 'model', 'temperature', 'prep_step', 'summarize_task',\n",
      "       'edit_task', 'simplify_task', 'simplify_audience', 'format_task',\n",
      "       'full_summarize_task', 'folder', 'summary', 'headline',\n",
      "       'simple_summary'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>reference_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>choice</th>\n",
       "      <th>text</th>\n",
       "      <th>system_role</th>\n",
       "      <th>model</th>\n",
       "      <th>temperature</th>\n",
       "      <th>prep_step</th>\n",
       "      <th>summarize_task</th>\n",
       "      <th>edit_task</th>\n",
       "      <th>simplify_task</th>\n",
       "      <th>simplify_audience</th>\n",
       "      <th>format_task</th>\n",
       "      <th>full_summarize_task</th>\n",
       "      <th>folder</th>\n",
       "      <th>summary</th>\n",
       "      <th>headline</th>\n",
       "      <th>simple_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-12 11:23:53.713910-07:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparisons in the Recovery Response From Resi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Decreases in muscle mass, function, and neurom...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A new study compared young and middle-aged adu...</td>\n",
       "      <td>New Research on the Impact of Aging and Exerci...</td>\n",
       "      <td>Check out this new study comparing young and m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-12 11:23:53.713910-07:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparisons in the Recovery Response From Resi...</td>\n",
       "      <td>2</td>\n",
       "      <td>Decreases in muscle mass, function, and neurom...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study compared the recovery response ...</td>\n",
       "      <td>New Study on Aging and Exercise</td>\n",
       "      <td>A new study has found that regular resistance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-12 11:23:56.749327-07:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Effect of dietary sources of calcium and prote...</td>\n",
       "      <td>1</td>\n",
       "      <td>Longevity increases the proportion of older ad...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study found that a high calcium and h...</td>\n",
       "      <td>New study shows calcium and protein interventi...</td>\n",
       "      <td>A recent study has shown that a simple dietary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-12 11:23:56.749327-07:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Effect of dietary sources of calcium and prote...</td>\n",
       "      <td>2</td>\n",
       "      <td>Longevity increases the proportion of older ad...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study found promising results for a h...</td>\n",
       "      <td>Nutritional Intervention Reduces Fracture Risk...</td>\n",
       "      <td>A recent study found that a tailored high calc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-07-12 11:24:01.173618-07:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Exercise Snacks A Novel Strategy to Improve Ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>We define exercise snacks as isolated ?1-min b...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>Exercise snacks are short bouts of vigorous ex...</td>\n",
       "      <td>Discover the Benefits of Exercise Snacks: A Fa...</td>\n",
       "      <td>Exercise snacks are short bursts of vigorous e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-07-12 11:24:01.173618-07:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Exercise Snacks A Novel Strategy to Improve Ca...</td>\n",
       "      <td>2</td>\n",
       "      <td>We define exercise snacks as isolated ?1-min b...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>Key points: Exercise snacks are brief bouts of...</td>\n",
       "      <td>Improve your health with exercise snacks</td>\n",
       "      <td>Introducing exercise snacks- short and intense...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-07-12 11:24:08.390067-07:00</td>\n",
       "      <td>4</td>\n",
       "      <td>Food craving, cortisol and ghrelin responses i...</td>\n",
       "      <td>1</td>\n",
       "      <td>The United States is at the forefront of the g...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study in a controlled hospital-based ...</td>\n",
       "      <td>New Study Reveals Surprising Links Between Foo...</td>\n",
       "      <td>This study offers valuable information about h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-07-12 11:24:08.390067-07:00</td>\n",
       "      <td>4</td>\n",
       "      <td>Food craving, cortisol and ghrelin responses i...</td>\n",
       "      <td>2</td>\n",
       "      <td>The United States is at the forefront of the g...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study conducted in a controlled hospi...</td>\n",
       "      <td>New study examines the impact of food cues and...</td>\n",
       "      <td>Researchers have found that exposure to certai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-07-12 11:24:11.716996-07:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Hypohydration but not Menstrual Phase Influenc...</td>\n",
       "      <td>1</td>\n",
       "      <td>Pain is recognized as a public health problem ...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>Chronic pain affects a significant portion of ...</td>\n",
       "      <td>Study finds that mild dehydration increases pa...</td>\n",
       "      <td>A recent study discovered that not getting eno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-07-12 11:24:11.716996-07:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Hypohydration but not Menstrual Phase Influenc...</td>\n",
       "      <td>2</td>\n",
       "      <td>Pain is recognized as a public health problem ...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A study found that mild dehydration can increa...</td>\n",
       "      <td>New Research Reveals that Dehydration Increase...</td>\n",
       "      <td>Did you know that being properly hydrated can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-07-12 11:24:15.883109-07:00</td>\n",
       "      <td>6</td>\n",
       "      <td>Weight stigma and health behaviors: evidence f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Weight stigma is pervasive. Higher weight indi...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>Weight stigma, or the discrimination and mistr...</td>\n",
       "      <td>Weight Stigma Linked to Negative Health Behaviors</td>\n",
       "      <td>Weight stigma negatively affects people's heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-07-12 11:24:15.883109-07:00</td>\n",
       "      <td>6</td>\n",
       "      <td>Weight stigma and health behaviors: evidence f...</td>\n",
       "      <td>2</td>\n",
       "      <td>Weight stigma is pervasive. Higher weight indi...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following informatio...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\...</td>\n",
       "      <td>3. If needed, rewrite the text using terms app...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\n...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>This study found that weight stigma is associa...</td>\n",
       "      <td>New research links weight stigma to negative h...</td>\n",
       "      <td>New study reveals that weight stigma can lead ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           timestamp  reference_id  \\\n",
       "0   2023-07-12 11:23:53.713910-07:00             1   \n",
       "1   2023-07-12 11:23:53.713910-07:00             1   \n",
       "2   2023-07-12 11:23:56.749327-07:00             2   \n",
       "3   2023-07-12 11:23:56.749327-07:00             2   \n",
       "4   2023-07-12 11:24:01.173618-07:00             3   \n",
       "5   2023-07-12 11:24:01.173618-07:00             3   \n",
       "6   2023-07-12 11:24:08.390067-07:00             4   \n",
       "7   2023-07-12 11:24:08.390067-07:00             4   \n",
       "8   2023-07-12 11:24:11.716996-07:00             5   \n",
       "9   2023-07-12 11:24:11.716996-07:00             5   \n",
       "10  2023-07-12 11:24:15.883109-07:00             6   \n",
       "11  2023-07-12 11:24:15.883109-07:00             6   \n",
       "\n",
       "                                        article_title  choice  \\\n",
       "0   Comparisons in the Recovery Response From Resi...       1   \n",
       "1   Comparisons in the Recovery Response From Resi...       2   \n",
       "2   Effect of dietary sources of calcium and prote...       1   \n",
       "3   Effect of dietary sources of calcium and prote...       2   \n",
       "4   Exercise Snacks A Novel Strategy to Improve Ca...       1   \n",
       "5   Exercise Snacks A Novel Strategy to Improve Ca...       2   \n",
       "6   Food craving, cortisol and ghrelin responses i...       1   \n",
       "7   Food craving, cortisol and ghrelin responses i...       2   \n",
       "8   Hypohydration but not Menstrual Phase Influenc...       1   \n",
       "9   Hypohydration but not Menstrual Phase Influenc...       2   \n",
       "10  Weight stigma and health behaviors: evidence f...       1   \n",
       "11  Weight stigma and health behaviors: evidence f...       2   \n",
       "\n",
       "                                                 text  \\\n",
       "0   Decreases in muscle mass, function, and neurom...   \n",
       "1   Decreases in muscle mass, function, and neurom...   \n",
       "2   Longevity increases the proportion of older ad...   \n",
       "3   Longevity increases the proportion of older ad...   \n",
       "4   We define exercise snacks as isolated ?1-min b...   \n",
       "5   We define exercise snacks as isolated ?1-min b...   \n",
       "6   The United States is at the forefront of the g...   \n",
       "7   The United States is at the forefront of the g...   \n",
       "8   Pain is recognized as a public health problem ...   \n",
       "9   Pain is recognized as a public health problem ...   \n",
       "10  Weight stigma is pervasive. Higher weight indi...   \n",
       "11  Weight stigma is pervasive. Higher weight indi...   \n",
       "\n",
       "                     system_role                   model  temperature  \\\n",
       "0   You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "1   You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "2   You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "3   You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "4   You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "5   You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "6   You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "7   You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "8   You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "9   You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "10  You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "11  You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "\n",
       "                                            prep_step  \\\n",
       "0   In the summary, cover the following informatio...   \n",
       "1   In the summary, cover the following informatio...   \n",
       "2   In the summary, cover the following informatio...   \n",
       "3   In the summary, cover the following informatio...   \n",
       "4   In the summary, cover the following informatio...   \n",
       "5   In the summary, cover the following informatio...   \n",
       "6   In the summary, cover the following informatio...   \n",
       "7   In the summary, cover the following informatio...   \n",
       "8   In the summary, cover the following informatio...   \n",
       "9   In the summary, cover the following informatio...   \n",
       "10  In the summary, cover the following informatio...   \n",
       "11  In the summary, cover the following informatio...   \n",
       "\n",
       "                                summarize_task  \\\n",
       "0   1. Summarize the text for a LinkedIn post.   \n",
       "1   1. Summarize the text for a LinkedIn post.   \n",
       "2   1. Summarize the text for a LinkedIn post.   \n",
       "3   1. Summarize the text for a LinkedIn post.   \n",
       "4   1. Summarize the text for a LinkedIn post.   \n",
       "5   1. Summarize the text for a LinkedIn post.   \n",
       "6   1. Summarize the text for a LinkedIn post.   \n",
       "7   1. Summarize the text for a LinkedIn post.   \n",
       "8   1. Summarize the text for a LinkedIn post.   \n",
       "9   1. Summarize the text for a LinkedIn post.   \n",
       "10  1. Summarize the text for a LinkedIn post.   \n",
       "11  1. Summarize the text for a LinkedIn post.   \n",
       "\n",
       "                                            edit_task  \\\n",
       "0   Once you have written your text message:     \\...   \n",
       "1   Once you have written your text message:     \\...   \n",
       "2   Once you have written your text message:     \\...   \n",
       "3   Once you have written your text message:     \\...   \n",
       "4   Once you have written your text message:     \\...   \n",
       "5   Once you have written your text message:     \\...   \n",
       "6   Once you have written your text message:     \\...   \n",
       "7   Once you have written your text message:     \\...   \n",
       "8   Once you have written your text message:     \\...   \n",
       "9   Once you have written your text message:     \\...   \n",
       "10  Once you have written your text message:     \\...   \n",
       "11  Once you have written your text message:     \\...   \n",
       "\n",
       "                                        simplify_task  \\\n",
       "0   3. If needed, rewrite the text using terms app...   \n",
       "1   3. If needed, rewrite the text using terms app...   \n",
       "2   3. If needed, rewrite the text using terms app...   \n",
       "3   3. If needed, rewrite the text using terms app...   \n",
       "4   3. If needed, rewrite the text using terms app...   \n",
       "5   3. If needed, rewrite the text using terms app...   \n",
       "6   3. If needed, rewrite the text using terms app...   \n",
       "7   3. If needed, rewrite the text using terms app...   \n",
       "8   3. If needed, rewrite the text using terms app...   \n",
       "9   3. If needed, rewrite the text using terms app...   \n",
       "10  3. If needed, rewrite the text using terms app...   \n",
       "11  3. If needed, rewrite the text using terms app...   \n",
       "\n",
       "                      simplify_audience  \\\n",
       "0   people without a science background   \n",
       "1   people without a science background   \n",
       "2   people without a science background   \n",
       "3   people without a science background   \n",
       "4   people without a science background   \n",
       "5   people without a science background   \n",
       "6   people without a science background   \n",
       "7   people without a science background   \n",
       "8   people without a science background   \n",
       "9   people without a science background   \n",
       "10  people without a science background   \n",
       "11  people without a science background   \n",
       "\n",
       "                                          format_task  \\\n",
       "0   4. Return your final response in a JSON format...   \n",
       "1   4. Return your final response in a JSON format...   \n",
       "2   4. Return your final response in a JSON format...   \n",
       "3   4. Return your final response in a JSON format...   \n",
       "4   4. Return your final response in a JSON format...   \n",
       "5   4. Return your final response in a JSON format...   \n",
       "6   4. Return your final response in a JSON format...   \n",
       "7   4. Return your final response in a JSON format...   \n",
       "8   4. Return your final response in a JSON format...   \n",
       "9   4. Return your final response in a JSON format...   \n",
       "10  4. Return your final response in a JSON format...   \n",
       "11  4. Return your final response in a JSON format...   \n",
       "\n",
       "                                  full_summarize_task                  folder  \\\n",
       "0   1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "1   1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "2   1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "3   1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "4   1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "5   1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "6   1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "7   1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "8   1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "9   1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "10  1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "11  1. Summarize the text for a LinkedIn post.\\n\\n...  text/2023-07-11 for db   \n",
       "\n",
       "                                              summary  \\\n",
       "0   A new study compared young and middle-aged adu...   \n",
       "1   A recent study compared the recovery response ...   \n",
       "2   A recent study found that a high calcium and h...   \n",
       "3   A recent study found promising results for a h...   \n",
       "4   Exercise snacks are short bouts of vigorous ex...   \n",
       "5   Key points: Exercise snacks are brief bouts of...   \n",
       "6   A recent study in a controlled hospital-based ...   \n",
       "7   A recent study conducted in a controlled hospi...   \n",
       "8   Chronic pain affects a significant portion of ...   \n",
       "9   A study found that mild dehydration can increa...   \n",
       "10  Weight stigma, or the discrimination and mistr...   \n",
       "11  This study found that weight stigma is associa...   \n",
       "\n",
       "                                             headline  \\\n",
       "0   New Research on the Impact of Aging and Exerci...   \n",
       "1                     New Study on Aging and Exercise   \n",
       "2   New study shows calcium and protein interventi...   \n",
       "3   Nutritional Intervention Reduces Fracture Risk...   \n",
       "4   Discover the Benefits of Exercise Snacks: A Fa...   \n",
       "5            Improve your health with exercise snacks   \n",
       "6   New Study Reveals Surprising Links Between Foo...   \n",
       "7   New study examines the impact of food cues and...   \n",
       "8   Study finds that mild dehydration increases pa...   \n",
       "9   New Research Reveals that Dehydration Increase...   \n",
       "10  Weight Stigma Linked to Negative Health Behaviors   \n",
       "11  New research links weight stigma to negative h...   \n",
       "\n",
       "                                       simple_summary  \n",
       "0   Check out this new study comparing young and m...  \n",
       "1   A new study has found that regular resistance ...  \n",
       "2   A recent study has shown that a simple dietary...  \n",
       "3   A recent study found that a tailored high calc...  \n",
       "4   Exercise snacks are short bursts of vigorous e...  \n",
       "5   Introducing exercise snacks- short and intense...  \n",
       "6   This study offers valuable information about h...  \n",
       "7   Researchers have found that exposure to certai...  \n",
       "8   A recent study discovered that not getting eno...  \n",
       "9   Did you know that being properly hydrated can ...  \n",
       "10  Weight stigma negatively affects people's heal...  \n",
       "11  New study reveals that weight stigma can lead ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP, Numeric\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    full_template = mapped_column(Text)\n",
    "    system_role = mapped_column(String(300))\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    rating_original_content = mapped_column(Integer) #\n",
    "    simple_summary = mapped_column(Text)\n",
    "    # rating_simple_content = mapped_column(Integer) #\n",
    "    original_headline = mapped_column(String(255))\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    model = mapped_column(String(70))\n",
    "    temperature = mapped_column(Numeric)\n",
    "\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=None):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries', engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    data = Sources(\n",
    "                        title=row['title'],\n",
    "                        text=row['text'],\n",
    "                        abstract=row['abstract'],\n",
    "                        publication=row['publication'],\n",
    "                        authors=row['authors'],\n",
    "                        year=row['year'],\n",
    "                        month=row['month'],\n",
    "                        pub_volume=row['pub_volume'],\n",
    "                        pub_issue=row['pub_issue'],\n",
    "                        start_page=row['start_page'],\n",
    "                        end_page=row['end_page'],\n",
    "                        doi=row['doi']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                    print(f'\\t{row[\"title\"]}')\n",
    "                elif table == 'summaries':\n",
    "                # if table == 'summaries':\n",
    "\n",
    "                    # Check if prompt already exists in the database\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full_summarize_task'],\n",
    "                        ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['summarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                    print(f'\\tReference #{row[\"reference_id\"]}: {row[\"headline\"]}')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "from prompts import *\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.reference_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['timestamp'] = str(datetime.now(pytz.timezone('Canada/Pacific')))\n",
    "        self.qna['reference_id'] = self.reference_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model        \n",
    "        self.qna['temperature'] = self.temperature\n",
    "        self.qna['prep_step'] = prep_step.strip()\n",
    "        self.qna['summarize_task'] = task.strip()\n",
    "        self.qna['edit_task'] = edit_task.strip()\n",
    "        self.qna['simplify_task'] = simplify_task.strip()\n",
    "        self.qna['simplify_audience'] = simplify_audience.strip()\n",
    "        self.qna['format_task'] = format_task.strip()\n",
    "        self.qna['full_summarize_task'] = full_task.strip()\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1\n",
    "article_limit = None\n",
    "temperature = 1.5\n",
    "n_choices = 2\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "# model = 'gpt-4'\n",
    "save_outputs=True\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "# summaries = get_table(table='summaries')\n",
    "\n",
    "\n",
    "sources_df = get_table(table='sources', limit=article_limit)\n",
    "\n",
    "chaining_dict = batch_summarize(\n",
    "    sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chatbot_dict, temperature=temperature,\n",
    "    system_role=system_role, model=model, max_tokens=1000,\n",
    "    n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "    iteration_id=iteration_id, save_outputs=save_outputs\n",
    "    )\n",
    "# # chaining_dict[iteration_id]\n",
    "qna_dict = create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "    )\n",
    "# Add rows from results to summaries and prompts table\n",
    "# bulk_append(qna_dict[iteration_id])\n",
    "qna_dict[iteration_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 12 rows to the database...\n",
      "\tReference #1: New Research on the Impact of Aging and Exercise on Muscle Function and Recovery\n",
      "\tReference #1: New Study on Aging and Exercise\n",
      "\tReference #2: New study shows calcium and protein intervention reduces fracture and fall risk in older adults\n",
      "\tReference #2: Nutritional Intervention Reduces Fracture Risk in Older Adults\n",
      "\tReference #3: Discover the Benefits of Exercise Snacks: A Fast and Feasible Way to Improve Health\n",
      "\tReference #3: Improve your health with exercise snacks\n",
      "\tReference #4: New Study Reveals Surprising Links Between Food Cravings, Stress, and Weight\n",
      "\tReference #4: New study examines the impact of food cues and stress on cravings and intake of highly palatable foods\n",
      "\tReference #5: Study finds that mild dehydration increases pain sensitivity in women\n",
      "\tReference #5: New Research Reveals that Dehydration Increases Pain Sensitivity in Women\n",
      "\tReference #6: Weight Stigma Linked to Negative Health Behaviors\n",
      "\tReference #6: New research links weight stigma to negative health behaviors: [Highlight Key Points]\n",
      "Data added successfully!\n"
     ]
    }
   ],
   "source": [
    "bulk_append(qna_dict[iteration_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from sources\n",
      "**Text #4 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "**Text #5 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "**Text #6 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 2 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "4_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "5_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "6_prompt00\n",
      "\treference_id\n",
      "\ttitle\n",
      "\ttext\n",
      "\tfolder\n",
      "\tsystem_role\n",
      "\ttemperature\n",
      "\tmax_tokens\n",
      "\tmodel\n",
      "\tqna\n",
      "\tsummaries_dict\n",
      "\tarticle_title\n",
      "\tresponse_regex\n",
      "\tsimple_summary_dict\n",
      "\trelevance_dict\n",
      "\tn_previous_prompts\n",
      "Object saved as JSON: ../text/2023-07-11 for db//batch_Chaining_attributes_initial_2023-07-12_1410.json\n",
      "Processing 4_prompt00...\n",
      "Processing 5_prompt00...\n",
      "Processing 6_prompt00...\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 5 column 2 (char 848)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 405\u001b[0m\n\u001b[0;32m    396\u001b[0m chaining_dict \u001b[39m=\u001b[39m batch_summarize(\n\u001b[0;32m    397\u001b[0m     sources_df, folder_path, prep_step, summarize_task, edit_task, \n\u001b[0;32m    398\u001b[0m     simplify_task, simplify_audience, format_task,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    402\u001b[0m     iteration_id\u001b[39m=\u001b[39miteration_id, save_outputs\u001b[39m=\u001b[39msave_outputs\n\u001b[0;32m    403\u001b[0m     )\n\u001b[0;32m    404\u001b[0m \u001b[39m# # chaining_dict[iteration_id]\u001b[39;00m\n\u001b[1;32m--> 405\u001b[0m qna_dict \u001b[39m=\u001b[39m create_summaries_df(\n\u001b[0;32m    406\u001b[0m     qna_dict, chatbot_dict, iteration_id, chatbot_id\u001b[39m=\u001b[39;49mchatbot_id\n\u001b[0;32m    407\u001b[0m     )\n\u001b[0;32m    408\u001b[0m \u001b[39m# Add rows from results to summaries and prompts table\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[39m# bulk_append(qna_dict[iteration_id])\u001b[39;00m\n\u001b[0;32m    410\u001b[0m qna_dict[iteration_id]\n",
      "Cell \u001b[1;32mIn[28], line 350\u001b[0m, in \u001b[0;36mcreate_summaries_df\u001b[1;34m(qna_dict, chatbot_dict, iteration_id, chatbot_id)\u001b[0m\n\u001b[0;32m    344\u001b[0m     dfs_list\u001b[39m.\u001b[39mappend(pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[0;32m    345\u001b[0m         chatbot_dict[chatbot_id][chatbot_key]\u001b[39m.\u001b[39mqna, \n\u001b[0;32m    346\u001b[0m         index\u001b[39m=\u001b[39m[choice \u001b[39mfor\u001b[39;00m choice \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(chatbot_dict[chatbot_id][chatbot_key]\u001b[39m.\u001b[39mqna[\u001b[39m'\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)])\n\u001b[0;32m    347\u001b[0m         )\n\u001b[0;32m    349\u001b[0m qna_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(dfs_list)\u001b[39m.\u001b[39mreset_index(names\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mchoice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 350\u001b[0m qna_df \u001b[39m=\u001b[39m extract_summary(qna_df, \u001b[39m'\u001b[39;49m\u001b[39msummary\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    351\u001b[0m columns \u001b[39m=\u001b[39m qna_df\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m    352\u001b[0m columns\u001b[39m.\u001b[39mremove(\u001b[39m'\u001b[39m\u001b[39mchoice\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 368\u001b[0m, in \u001b[0;36mextract_summary\u001b[1;34m(df, summary_column)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_summary\u001b[39m(df, summary_column\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    367\u001b[0m     \u001b[39m# Convert the string to JSON\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     df[summary_column] \u001b[39m=\u001b[39m df[summary_column]\u001b[39m.\u001b[39;49mapply(json\u001b[39m.\u001b[39;49mloads)\n\u001b[0;32m    370\u001b[0m     \u001b[39m# Extract 'headline' and 'body' values\u001b[39;00m\n\u001b[0;32m    371\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mheadline\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[summary_column]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m'\u001b[39m\u001b[39mheadline\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\ginkgo\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\ginkgo\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\ginkgo\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\ginkgo\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\ginkgo\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\ginkgo\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\ginkgo\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[39ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 5 column 2 (char 848)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP, Numeric\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    full_template = mapped_column(Text)\n",
    "    system_role = mapped_column(String(300))\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    rating_original_content = mapped_column(Integer) \n",
    "    simple_summary = mapped_column(Text)\n",
    "    rating_simple_content = mapped_column(Integer) \n",
    "    original_headline = mapped_column(String(255))\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    model = mapped_column(String(70))\n",
    "    temperature = mapped_column(Numeric)\n",
    "\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=None):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries', engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    data = Sources(\n",
    "                        title=row['title'],\n",
    "                        text=row['text'],\n",
    "                        abstract=row['abstract'],\n",
    "                        publication=row['publication'],\n",
    "                        authors=row['authors'],\n",
    "                        year=row['year'],\n",
    "                        month=row['month'],\n",
    "                        pub_volume=row['pub_volume'],\n",
    "                        pub_issue=row['pub_issue'],\n",
    "                        start_page=row['start_page'],\n",
    "                        end_page=row['end_page'],\n",
    "                        doi=row['doi']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                    print(f'\\t{row[\"title\"]}')\n",
    "                elif table == 'summaries':\n",
    "                # if table == 'summaries':\n",
    "\n",
    "                    # Check if prompt already exists in the database\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full_summarize_task'],\n",
    "                        ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['summarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        rating_original_content=row['rating_original_content'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        rating_simple_content=row['rating_simple_content'],\n",
    "                        original_headline=row['headline'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice'],\n",
    "                        model=row['model'],\n",
    "                        temperature=row['temperature']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                    print(f'\\tReference #{row[\"reference_id\"]}: {row[\"headline\"]}')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "from prompts import *\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.reference_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['timestamp'] = str(datetime.now(pytz.timezone('Canada/Pacific')))\n",
    "        self.qna['reference_id'] = self.reference_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model        \n",
    "        self.qna['temperature'] = self.temperature\n",
    "        self.qna['prep_step'] = prep_step.strip()\n",
    "        self.qna['summarize_task'] = task.strip()\n",
    "        self.qna['edit_task'] = edit_task.strip()\n",
    "        self.qna['simplify_task'] = simplify_task.strip()\n",
    "        self.qna['simplify_audience'] = simplify_audience.strip()\n",
    "        self.qna['format_task'] = format_task.strip()\n",
    "        self.qna['full_summarize_task'] = full_task.strip()\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    df[summary_column] = df[summary_column].apply(json.loads)\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: x['headline'])\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: x['audience'])\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: x['body'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1.1\n",
    "article_limit = None\n",
    "temperature = 1.5\n",
    "n_choices = 2\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "# model = 'gpt-4'\n",
    "save_outputs=True\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "# summaries = get_table(table='summaries')\n",
    "\n",
    "\n",
    "sources_df = get_table(table='sources', limit=article_limit).tail(3)\n",
    "\n",
    "chaining_dict = batch_summarize(\n",
    "    sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chatbot_dict, temperature=temperature,\n",
    "    system_role=system_role, model=model, max_tokens=1000,\n",
    "    n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "    iteration_id=iteration_id, save_outputs=save_outputs\n",
    "    )\n",
    "# # chaining_dict[iteration_id]\n",
    "qna_dict = create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "    )\n",
    "# Add rows from results to summaries and prompts table\n",
    "# bulk_append(qna_dict[iteration_id])\n",
    "qna_dict[iteration_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1.1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qna_dict[iteration_id]\n",
      "\u001b[1;31mKeyError\u001b[0m: 1.1"
     ]
    }
   ],
   "source": [
    "qna_dict[iteration_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 4_prompt00...\n",
      "Processing 5_prompt00...\n",
      "Processing 6_prompt00...\n",
      "Error converting summary column to JSON: Expecting property name enclosed in double quotes: line 5 column 2 (char 848); will do row by row\n",
      "Error converting summary 5 to JSON: Expecting property name enclosed in double quotes: line 5 column 2 (char 848)\n",
      "Original summaries DataFrame shape: (6, 19)\n",
      "\tOriginal summaries Dataframe columns: Index(['choice', 'timestamp', 'reference_id', 'article_title', 'text',\n",
      "       'system_role', 'model', 'temperature', 'prep_step', 'summarize_task',\n",
      "       'edit_task', 'simplify_task', 'simplify_audience', 'format_task',\n",
      "       'full_summarize_task', 'folder', 'summary', 'headline',\n",
      "       'simple_summary'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>reference_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>choice</th>\n",
       "      <th>text</th>\n",
       "      <th>system_role</th>\n",
       "      <th>model</th>\n",
       "      <th>temperature</th>\n",
       "      <th>prep_step</th>\n",
       "      <th>summarize_task</th>\n",
       "      <th>edit_task</th>\n",
       "      <th>simplify_task</th>\n",
       "      <th>simplify_audience</th>\n",
       "      <th>format_task</th>\n",
       "      <th>full_summarize_task</th>\n",
       "      <th>folder</th>\n",
       "      <th>summary</th>\n",
       "      <th>headline</th>\n",
       "      <th>simple_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-12 14:10:05.214512-07:00</td>\n",
       "      <td>4</td>\n",
       "      <td>Food craving, cortisol and ghrelin responses in modeling highly palatable snack intake in the la...</td>\n",
       "      <td>1</td>\n",
       "      <td>The United States is at the forefront of the global obesity epidemic with 67% of its population ...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following information:     \\n- Identify the key points and statistics ...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\nEvaluate your text message to see if it may be co...</td>\n",
       "      <td>3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format with the following format:     \\n{\"headline\": &lt;su...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\nIn the summary, cover the following information:  ...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study conducted in a controlled setting found that exposure to food cues and stress can...</td>\n",
       "      <td>New Study Reveals How Food Cues and Stress Increase Cravings for Unhealthy Foods</td>\n",
       "      <td>Discover how exposure to food cues and stress can affect our cravings for unhealthy foods: the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-12 14:10:05.214512-07:00</td>\n",
       "      <td>4</td>\n",
       "      <td>Food craving, cortisol and ghrelin responses in modeling highly palatable snack intake in the la...</td>\n",
       "      <td>2</td>\n",
       "      <td>The United States is at the forefront of the global obesity epidemic with 67% of its population ...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following information:     \\n- Identify the key points and statistics ...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\nEvaluate your text message to see if it may be co...</td>\n",
       "      <td>3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format with the following format:     \\n{\"headline\": &lt;su...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\nIn the summary, cover the following information:  ...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>In a recent study conducted in a controlled hospital setting, researchers found that exposure to...</td>\n",
       "      <td>New Study Shows That Food Cues and Stress Increase Cravings and Intake of Unhealthy Foods</td>\n",
       "      <td>A recent study conducted in a controlled hospital setting found that exposures to food cues and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-12 14:10:08.562822-07:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Hypohydration but not Menstrual Phase Influences Pain Perception in Healthy Women</td>\n",
       "      <td>1</td>\n",
       "      <td>Pain is recognized as a public health problem (1). Chronic pain [i.e., pain that persists for ?3...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following information:     \\n- Identify the key points and statistics ...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\nEvaluate your text message to see if it may be co...</td>\n",
       "      <td>3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format with the following format:     \\n{\"headline\": &lt;su...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\nIn the summary, cover the following information:  ...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study found that mild dehydration increases pain sensitivity in women, supporting previ...</td>\n",
       "      <td>New Research Shows Dehydration Can Affect Pain Sensitivity in Women</td>\n",
       "      <td>Dehydration can have a negative impact on pain response in women, according to new research. It'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-12 14:10:08.562822-07:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Hypohydration but not Menstrual Phase Influences Pain Perception in Healthy Women</td>\n",
       "      <td>2</td>\n",
       "      <td>Pain is recognized as a public health problem (1). Chronic pain [i.e., pain that persists for ?3...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following information:     \\n- Identify the key points and statistics ...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\nEvaluate your text message to see if it may be co...</td>\n",
       "      <td>3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format with the following format:     \\n{\"headline\": &lt;su...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\nIn the summary, cover the following information:  ...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>A recent study found that mild dehydration can increase experimental pain sensitivity in women, ...</td>\n",
       "      <td>New research shows that dehydration increases pain sensitivity in women</td>\n",
       "      <td>Have you ever considered the effects of dehydration on pain perception? Recent research shows th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-07-12 14:10:12.012633-07:00</td>\n",
       "      <td>6</td>\n",
       "      <td>Weight stigma and health behaviors: evidence from the Eating in America Study</td>\n",
       "      <td>1</td>\n",
       "      <td>Weight stigma is pervasive. Higher weight individuals are stigmatized across many contexts, incl...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following information:     \\n- Identify the key points and statistics ...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\nEvaluate your text message to see if it may be co...</td>\n",
       "      <td>3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format with the following format:     \\n{\"headline\": &lt;su...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\nIn the summary, cover the following information:  ...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>Weight stigma negatively affects health behaviors such as eating, physical activity, alcohol use...</td>\n",
       "      <td>Weight Stigma and its Impact on Health Behaviors</td>\n",
       "      <td>New study highlights the negative impact of weight stigma on eating, exercise, alcohol use, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-07-12 14:10:12.012633-07:00</td>\n",
       "      <td>6</td>\n",
       "      <td>Weight stigma and health behaviors: evidence from the Eating in America Study</td>\n",
       "      <td>2</td>\n",
       "      <td>Weight stigma is pervasive. Higher weight individuals are stigmatized across many contexts, incl...</td>\n",
       "      <td>You are a helpful assistant.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1.5</td>\n",
       "      <td>In the summary, cover the following information:     \\n- Identify the key points and statistics ...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.</td>\n",
       "      <td>Once you have written your text message:     \\nEvaluate your text message to see if it may be co...</td>\n",
       "      <td>3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same...</td>\n",
       "      <td>people without a science background</td>\n",
       "      <td>4. Return your final response in a JSON format with the following format:     \\n{\"headline\": &lt;su...</td>\n",
       "      <td>1. Summarize the text for a LinkedIn post.\\n\\nIn the summary, cover the following information:  ...</td>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>Weight discrimination is associated with negative effects on physical health and multiple health...</td>\n",
       "      <td>Weight stigma and health behaviors: Key findings from recent research</td>\n",
       "      <td>Weight discrimination can impact physical health and health behaviors like eating, alcohol use, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          timestamp  reference_id  \\\n",
       "0  2023-07-12 14:10:05.214512-07:00             4   \n",
       "1  2023-07-12 14:10:05.214512-07:00             4   \n",
       "2  2023-07-12 14:10:08.562822-07:00             5   \n",
       "3  2023-07-12 14:10:08.562822-07:00             5   \n",
       "4  2023-07-12 14:10:12.012633-07:00             6   \n",
       "5  2023-07-12 14:10:12.012633-07:00             6   \n",
       "\n",
       "                                                                                         article_title  \\\n",
       "0  Food craving, cortisol and ghrelin responses in modeling highly palatable snack intake in the la...   \n",
       "1  Food craving, cortisol and ghrelin responses in modeling highly palatable snack intake in the la...   \n",
       "2                    Hypohydration but not Menstrual Phase Influences Pain Perception in Healthy Women   \n",
       "3                    Hypohydration but not Menstrual Phase Influences Pain Perception in Healthy Women   \n",
       "4                        Weight stigma and health behaviors: evidence from the Eating in America Study   \n",
       "5                        Weight stigma and health behaviors: evidence from the Eating in America Study   \n",
       "\n",
       "   choice  \\\n",
       "0       1   \n",
       "1       2   \n",
       "2       1   \n",
       "3       2   \n",
       "4       1   \n",
       "5       2   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  The United States is at the forefront of the global obesity epidemic with 67% of its population ...   \n",
       "1  The United States is at the forefront of the global obesity epidemic with 67% of its population ...   \n",
       "2  Pain is recognized as a public health problem (1). Chronic pain [i.e., pain that persists for ?3...   \n",
       "3  Pain is recognized as a public health problem (1). Chronic pain [i.e., pain that persists for ?3...   \n",
       "4  Weight stigma is pervasive. Higher weight individuals are stigmatized across many contexts, incl...   \n",
       "5  Weight stigma is pervasive. Higher weight individuals are stigmatized across many contexts, incl...   \n",
       "\n",
       "                    system_role                   model  temperature  \\\n",
       "0  You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "1  You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "2  You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "3  You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "4  You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "5  You are a helpful assistant.  gpt-3.5-turbo-16k-0613          1.5   \n",
       "\n",
       "                                                                                             prep_step  \\\n",
       "0  In the summary, cover the following information:     \\n- Identify the key points and statistics ...   \n",
       "1  In the summary, cover the following information:     \\n- Identify the key points and statistics ...   \n",
       "2  In the summary, cover the following information:     \\n- Identify the key points and statistics ...   \n",
       "3  In the summary, cover the following information:     \\n- Identify the key points and statistics ...   \n",
       "4  In the summary, cover the following information:     \\n- Identify the key points and statistics ...   \n",
       "5  In the summary, cover the following information:     \\n- Identify the key points and statistics ...   \n",
       "\n",
       "                               summarize_task  \\\n",
       "0  1. Summarize the text for a LinkedIn post.   \n",
       "1  1. Summarize the text for a LinkedIn post.   \n",
       "2  1. Summarize the text for a LinkedIn post.   \n",
       "3  1. Summarize the text for a LinkedIn post.   \n",
       "4  1. Summarize the text for a LinkedIn post.   \n",
       "5  1. Summarize the text for a LinkedIn post.   \n",
       "\n",
       "                                                                                             edit_task  \\\n",
       "0  Once you have written your text message:     \\nEvaluate your text message to see if it may be co...   \n",
       "1  Once you have written your text message:     \\nEvaluate your text message to see if it may be co...   \n",
       "2  Once you have written your text message:     \\nEvaluate your text message to see if it may be co...   \n",
       "3  Once you have written your text message:     \\nEvaluate your text message to see if it may be co...   \n",
       "4  Once you have written your text message:     \\nEvaluate your text message to see if it may be co...   \n",
       "5  Once you have written your text message:     \\nEvaluate your text message to see if it may be co...   \n",
       "\n",
       "                                                                                         simplify_task  \\\n",
       "0  3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same...   \n",
       "1  3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same...   \n",
       "2  3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same...   \n",
       "3  3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same...   \n",
       "4  3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same...   \n",
       "5  3. If needed, rewrite the text using terms appropriate for the audience. If not keep it the same...   \n",
       "\n",
       "                     simplify_audience  \\\n",
       "0  people without a science background   \n",
       "1  people without a science background   \n",
       "2  people without a science background   \n",
       "3  people without a science background   \n",
       "4  people without a science background   \n",
       "5  people without a science background   \n",
       "\n",
       "                                                                                           format_task  \\\n",
       "0  4. Return your final response in a JSON format with the following format:     \\n{\"headline\": <su...   \n",
       "1  4. Return your final response in a JSON format with the following format:     \\n{\"headline\": <su...   \n",
       "2  4. Return your final response in a JSON format with the following format:     \\n{\"headline\": <su...   \n",
       "3  4. Return your final response in a JSON format with the following format:     \\n{\"headline\": <su...   \n",
       "4  4. Return your final response in a JSON format with the following format:     \\n{\"headline\": <su...   \n",
       "5  4. Return your final response in a JSON format with the following format:     \\n{\"headline\": <su...   \n",
       "\n",
       "                                                                                   full_summarize_task  \\\n",
       "0  1. Summarize the text for a LinkedIn post.\\n\\nIn the summary, cover the following information:  ...   \n",
       "1  1. Summarize the text for a LinkedIn post.\\n\\nIn the summary, cover the following information:  ...   \n",
       "2  1. Summarize the text for a LinkedIn post.\\n\\nIn the summary, cover the following information:  ...   \n",
       "3  1. Summarize the text for a LinkedIn post.\\n\\nIn the summary, cover the following information:  ...   \n",
       "4  1. Summarize the text for a LinkedIn post.\\n\\nIn the summary, cover the following information:  ...   \n",
       "5  1. Summarize the text for a LinkedIn post.\\n\\nIn the summary, cover the following information:  ...   \n",
       "\n",
       "                   folder  \\\n",
       "0  text/2023-07-11 for db   \n",
       "1  text/2023-07-11 for db   \n",
       "2  text/2023-07-11 for db   \n",
       "3  text/2023-07-11 for db   \n",
       "4  text/2023-07-11 for db   \n",
       "5  text/2023-07-11 for db   \n",
       "\n",
       "                                                                                               summary  \\\n",
       "0  A recent study conducted in a controlled setting found that exposure to food cues and stress can...   \n",
       "1  In a recent study conducted in a controlled hospital setting, researchers found that exposure to...   \n",
       "2  A recent study found that mild dehydration increases pain sensitivity in women, supporting previ...   \n",
       "3  A recent study found that mild dehydration can increase experimental pain sensitivity in women, ...   \n",
       "4  Weight stigma negatively affects health behaviors such as eating, physical activity, alcohol use...   \n",
       "5  Weight discrimination is associated with negative effects on physical health and multiple health...   \n",
       "\n",
       "                                                                                    headline  \\\n",
       "0           New Study Reveals How Food Cues and Stress Increase Cravings for Unhealthy Foods   \n",
       "1  New Study Shows That Food Cues and Stress Increase Cravings and Intake of Unhealthy Foods   \n",
       "2                        New Research Shows Dehydration Can Affect Pain Sensitivity in Women   \n",
       "3                    New research shows that dehydration increases pain sensitivity in women   \n",
       "4                                           Weight Stigma and its Impact on Health Behaviors   \n",
       "5                      Weight stigma and health behaviors: Key findings from recent research   \n",
       "\n",
       "                                                                                        simple_summary  \n",
       "0  Discover how exposure to food cues and stress can affect our cravings for unhealthy foods: the l...  \n",
       "1  A recent study conducted in a controlled hospital setting found that exposures to food cues and ...  \n",
       "2  Dehydration can have a negative impact on pain response in women, according to new research. It'...  \n",
       "3  Have you ever considered the effects of dehydration on pain perception? Recent research shows th...  \n",
       "4  New study highlights the negative impact of weight stigma on eating, exercise, alcohol use, and ...  \n",
       "5  Weight discrimination can impact physical health and health behaviors like eating, alcohol use, ...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP, Numeric\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    full_template = mapped_column(Text)\n",
    "    system_role = mapped_column(String(300))\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    rating_original_content = mapped_column(Integer) \n",
    "    simple_summary = mapped_column(Text)\n",
    "    rating_simple_content = mapped_column(Integer) \n",
    "    original_headline = mapped_column(String(255))\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    model = mapped_column(String(70))\n",
    "    temperature = mapped_column(Numeric)\n",
    "\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=None):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries', engine=None):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    data = Sources(\n",
    "                        title=row['title'],\n",
    "                        text=row['text'],\n",
    "                        abstract=row['abstract'],\n",
    "                        publication=row['publication'],\n",
    "                        authors=row['authors'],\n",
    "                        year=row['year'],\n",
    "                        month=row['month'],\n",
    "                        pub_volume=row['pub_volume'],\n",
    "                        pub_issue=row['pub_issue'],\n",
    "                        start_page=row['start_page'],\n",
    "                        end_page=row['end_page'],\n",
    "                        doi=row['doi']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                    print(f'\\t{row[\"title\"]}')\n",
    "                elif table == 'summaries':\n",
    "                # if table == 'summaries':\n",
    "\n",
    "                    # Check if prompt already exists in the database\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full_summarize_task'],\n",
    "                        ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['summarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        rating_original_content=row['rating_original_content'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        rating_simple_content=row['rating_simple_content'],\n",
    "                        original_headline=row['headline'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice'],\n",
    "                        model=row['model'],\n",
    "                        temperature=row['temperature']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                    print(f'\\tReference #{row[\"reference_id\"]}: {row[\"headline\"]}')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"Data added successfully!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "from response_processing import *\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "from prompts import *\n",
    "\n",
    "class Chaining:\n",
    "    def __init__(self, text_id, title, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=9000, \n",
    "        ):\n",
    "        self.reference_id = text_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*\\/.*)\\/?$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        print(f'***OpenAI model: {self.model}')\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature, model=None):\n",
    "        model = self.model if model == None else model\n",
    "        print(f'\\tSending request to {model}')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(\n",
    "            self, task, prep_step, edit_task, simplify_task, simplify_audience,\n",
    "            format_task,\n",
    "            n_choices=5, task_first=True):\n",
    "        if task_first == True:\n",
    "            full_task = f'{task}\\n\\n{prep_step}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        else:\n",
    "            full_task = f'{prep_step}\\n\\n{task}\\n\\n{edit_task}\\n\\n{simplify_task} {simplify_audience}\\n\\n{format_task}'\n",
    "        prompt = self.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['timestamp'] = str(datetime.now(pytz.timezone('Canada/Pacific')))\n",
    "        self.qna['reference_id'] = self.reference_id\n",
    "        self.qna['article_title'] = self.title\n",
    "        self.qna['text'] = self.text\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model        \n",
    "        self.qna['temperature'] = self.temperature\n",
    "        self.qna['prep_step'] = prep_step.strip()\n",
    "        self.qna['summarize_task'] = task.strip()\n",
    "        self.qna['edit_task'] = edit_task.strip()\n",
    "        self.qna['simplify_task'] = simplify_task.strip()\n",
    "        self.qna['simplify_audience'] = simplify_audience.strip()\n",
    "        self.qna['format_task'] = format_task.strip()\n",
    "        self.qna['full_summarize_task'] = full_task.strip()\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = self.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "    \n",
    "def batch_summarize(sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "    simplify_task, simplify_audience, format_task,\n",
    "    chaining_bot_dict, iteration_id, task_first=True,\n",
    "    system_role=None, model='gpt-3.5-turbo', max_tokens=1000, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False\n",
    "    ):\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task, simplify_task, simplify_audience, format_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task', 'simplify_task', 'simplify_audience', 'format_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    def summarize_from_df_row(text_id, title, text, chaining_bot_dict):\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{text_id} prompt #{index+1} of {prompts_df.index.max()+1}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            simplify_task = prompts_df.loc[index, 'simplify_task']\n",
    "            simplify_audience = prompts_df.loc[index, 'simplify_audience']\n",
    "            format_task = prompts_df.loc[index, 'format_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text_id, title, text, folder_path=folder_path, system_role=system_role, \n",
    "                    model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, \n",
    "                    simplify_task=simplify_task, simplify_audience=simplify_audience,\n",
    "                    format_task=format_task, n_choices=n_choices, task_first=task_first\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'{text_id}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Completed')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    sources_df.apply(lambda row: summarize_from_df_row(row['id'], row['title'], row['text'], chaining_bot_dict), axis=1)\n",
    "    \n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=folder_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    qna_df = extract_summary(qna_df, 'summary')\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_summary(df, summary_column='summary'):\n",
    "    # Convert the string to JSON\n",
    "    try:\n",
    "        df[summary_column] = df[summary_column].apply(json.loads)\n",
    "    except Exception as error:\n",
    "        print(f'Error converting {summary_column} column to JSON: {error}; will do row by row')\n",
    "        summary_list = []\n",
    "        for index, summary in df[summary_column].items():\n",
    "            try:\n",
    "                summary_list.append(json.loads(summary))\n",
    "            except Exception as error:\n",
    "                print(f'Error converting summary {index} to JSON: {error}')\n",
    "                summary_list.append(summary)\n",
    "    def extract_value_from_key(summary, key):\n",
    "        try:\n",
    "            return summary[key]\n",
    "        except Exception as error:\n",
    "            value = re.search(rf'\"{key}\":\\s*\"([^\"]+)\"', summary).group(1)\n",
    "            return value\n",
    "\n",
    "    # Extract 'headline' and 'body' values\n",
    "    df['headline'] = df[summary_column].apply(lambda x: extract_value_from_key(x, 'headline'))\n",
    "    df['simple_summary'] = df[summary_column].apply(lambda x: extract_value_from_key(x, 'audience'))\n",
    "    df[summary_column] = df[summary_column].apply(lambda x: extract_value_from_key(x, 'body'))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 1.1\n",
    "article_limit = None\n",
    "temperature = 1.5\n",
    "n_choices = 2\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "# model = 'gpt-4'\n",
    "save_outputs=True\n",
    "folder_path = '../text/2023-07-11 for db'\n",
    "\n",
    "# summaries = get_table(table='summaries')\n",
    "\n",
    "\n",
    "# sources_df = get_table(table='sources', limit=article_limit).tail(3)\n",
    "\n",
    "# chaining_dict = batch_summarize(\n",
    "#     sources_df, folder_path, prep_step, summarize_task, edit_task, \n",
    "#     simplify_task, simplify_audience, format_task,\n",
    "#     chatbot_dict, temperature=temperature,\n",
    "#     system_role=system_role, model=model, max_tokens=1000,\n",
    "#     n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "#     iteration_id=iteration_id, save_outputs=save_outputs\n",
    "#     )\n",
    "# # chaining_dict[iteration_id]\n",
    "qna_dict = create_summaries_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "    )\n",
    "# Add rows from results to summaries and prompts table\n",
    "# bulk_append(qna_dict[iteration_id])\n",
    "qna_dict[iteration_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>{\"headline\": \"New Study Reveals How Food Cues and Stress Increase Cravings for Unhealthy Foods\",\\n\"body\": \"A recent study conducted in a controlled setting found that exposure to food cues and stress can significantly increase cravings for highly palatable foods. The study also showed that these cravings directly predicted subsequent intake of unhealthy snacks. Furthermore, the hormone ghrelin was found to play a role in promoting food cravings, particularly in individuals who are overweight. The findings highlight the need for greater understanding of the biobehavioral processes that contribute to overeating and weight gain.\",\\n\"audience\": \"Discover how exposure to food cues and stress can affect our cravings for unhealthy foods: the latest research sheds light on the role of hormonal responses and the potential impact on weight management.\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>{\\n  \"headline\": \"New Study Shows That Food Cues and Stress Increase Cravings and Intake of Unhealthy Foods\",\\n  \"body\": \"In a recent study conducted in a controlled hospital setting, researchers found that exposure to food cues and stress both significantly increased cravings for highly palatable (HP) foods. These cravings, in turn, predicted greater intake of HP foods. Additionally, the study revealed that specific hormones, such as ghrelin and cortisol, may play a role in food motivation and intake. The findings highlight the potential factors influencing overeating and weight gain, and provide insights into the biobehavioral processes driving unhealthy food consumption.\",\\n  \"audience\": \"A recent study conducted in a controlled hospital setting found that exposures to food cues and stress may increase unhealthy food cravings and intake. The findings suggest important factors to consider in controlling food consumption and understanding the impact on weight gain.\" \\n}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>{\"headline\": \"New Research Shows Dehydration Can Affect Pain Sensitivity in Women\",\\n\"body\": \"A recent study found that mild dehydration increases pain sensitivity in women, supporting previous research in men. The study also investigated the effects of menstrual phase on pain sensitivity, but found no significant difference. Interestingly, acute water ingestion did not reduce pain sensitivity in hypohydrated participants. This research highlights the importance of staying properly hydrated to manage pain symptoms.\",\\n\"audience\": \"Dehydration can have a negative impact on pain response in women, according to new research. It's essential to focus on staying hydrated to help manage pain effectively.\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>{\"headline\": \"New research shows that dehydration increases pain sensitivity in women\", \\n\"body\": \"A recent study found that mild dehydration can increase experimental pain sensitivity in women, leading to decreased pain tolerance and increased pain intensity and unpleasantness. The study also examined the effects of menstrual phase on pain sensitivity, but found that it did not greatly impact pain perception. Additionally, acute water ingestion did not reduce pain sensitivity. This research highlights the importance of maintaining adequate hydration for women's wellbeing and suggests a link between dehydration and pain perception.\", \\n\"audience\": \"Have you ever considered the effects of dehydration on pain perception? Recent research shows that even mild dehydration can affect how we perceive pain. This is particularly important for women, as they may experience greater pain sensitivity under dehydrated conditions. Ensuring proper hydration can play a role in managing pain symptoms and supporting overall health.\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>{\"headline\": \"Weight Stigma and its Impact on Health Behaviors\",\\n \"body\": \"Weight stigma negatively affects health behaviors such as eating, physical activity, alcohol use, and sleep, according to a US study. The research highlights that weight stigma is associated with disordered eating, higher alcohol consumption, and poorer sleep quality. It also found that weight stigma affects health behaviors across different weights. This shows the need to reduce weight stigma and promote weight-inclusive health approaches.\",\\n \"audience\": \"New study highlights the negative impact of weight stigma on eating, exercise, alcohol use, and sleep. It affects people of all weights. Let's work towards promoting more inclusive health approaches to avoid detrimental effects.\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text/2023-07-11 for db</td>\n",
       "      <td>{\\n  \"headline\": \"Weight stigma and health behaviors: Key findings from recent research\",\\n  \"body\": \"Weight discrimination is associated with negative effects on physical health and multiple health behaviors including eating behavior, alcohol use, and sleep. Research found that weight stigma is significantly associated with poorer health behaviors, independent of BMI. These findings suggest the need to reduce weight stigma and employ more weight-inclusive approaches to health promotion.\",\\n  \"audience\": \"Weight discrimination can impact physical health and health behaviors like eating, alcohol use, and sleep. Research, in a diverse U.S. sample, found that weight stigma is associated with poorer health behaviors, regardless of body mass index. Addressing weight stigma is important for promoting healthy behaviors and overall well-being.\",\\n }</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   folder  \\\n",
       "0  text/2023-07-11 for db   \n",
       "1  text/2023-07-11 for db   \n",
       "2  text/2023-07-11 for db   \n",
       "3  text/2023-07-11 for db   \n",
       "4  text/2023-07-11 for db   \n",
       "5  text/2023-07-11 for db   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  summary  \n",
       "0                                                                                                                                                                                 {\"headline\": \"New Study Reveals How Food Cues and Stress Increase Cravings for Unhealthy Foods\",\\n\"body\": \"A recent study conducted in a controlled setting found that exposure to food cues and stress can significantly increase cravings for highly palatable foods. The study also showed that these cravings directly predicted subsequent intake of unhealthy snacks. Furthermore, the hormone ghrelin was found to play a role in promoting food cravings, particularly in individuals who are overweight. The findings highlight the need for greater understanding of the biobehavioral processes that contribute to overeating and weight gain.\",\\n\"audience\": \"Discover how exposure to food cues and stress can affect our cravings for unhealthy foods: the latest research sheds light on the role of hormonal responses and the potential impact on weight management.\"}  \n",
       "1                                               {\\n  \"headline\": \"New Study Shows That Food Cues and Stress Increase Cravings and Intake of Unhealthy Foods\",\\n  \"body\": \"In a recent study conducted in a controlled hospital setting, researchers found that exposure to food cues and stress both significantly increased cravings for highly palatable (HP) foods. These cravings, in turn, predicted greater intake of HP foods. Additionally, the study revealed that specific hormones, such as ghrelin and cortisol, may play a role in food motivation and intake. The findings highlight the potential factors influencing overeating and weight gain, and provide insights into the biobehavioral processes driving unhealthy food consumption.\",\\n  \"audience\": \"A recent study conducted in a controlled hospital setting found that exposures to food cues and stress may increase unhealthy food cravings and intake. The findings suggest important factors to consider in controlling food consumption and understanding the impact on weight gain.\" \\n}  \n",
       "2                                                                                                                                                                                                                                                                                                                                    {\"headline\": \"New Research Shows Dehydration Can Affect Pain Sensitivity in Women\",\\n\"body\": \"A recent study found that mild dehydration increases pain sensitivity in women, supporting previous research in men. The study also investigated the effects of menstrual phase on pain sensitivity, but found no significant difference. Interestingly, acute water ingestion did not reduce pain sensitivity in hypohydrated participants. This research highlights the importance of staying properly hydrated to manage pain symptoms.\",\\n\"audience\": \"Dehydration can have a negative impact on pain response in women, according to new research. It's essential to focus on staying hydrated to help manage pain effectively.\"}  \n",
       "3  {\"headline\": \"New research shows that dehydration increases pain sensitivity in women\", \\n\"body\": \"A recent study found that mild dehydration can increase experimental pain sensitivity in women, leading to decreased pain tolerance and increased pain intensity and unpleasantness. The study also examined the effects of menstrual phase on pain sensitivity, but found that it did not greatly impact pain perception. Additionally, acute water ingestion did not reduce pain sensitivity. This research highlights the importance of maintaining adequate hydration for women's wellbeing and suggests a link between dehydration and pain perception.\", \\n\"audience\": \"Have you ever considered the effects of dehydration on pain perception? Recent research shows that even mild dehydration can affect how we perceive pain. This is particularly important for women, as they may experience greater pain sensitivity under dehydrated conditions. Ensuring proper hydration can play a role in managing pain symptoms and supporting overall health.\"}  \n",
       "4                                                                                                                                                                                                                                                                        {\"headline\": \"Weight Stigma and its Impact on Health Behaviors\",\\n \"body\": \"Weight stigma negatively affects health behaviors such as eating, physical activity, alcohol use, and sleep, according to a US study. The research highlights that weight stigma is associated with disordered eating, higher alcohol consumption, and poorer sleep quality. It also found that weight stigma affects health behaviors across different weights. This shows the need to reduce weight stigma and promote weight-inclusive health approaches.\",\\n \"audience\": \"New study highlights the negative impact of weight stigma on eating, exercise, alcohol use, and sleep. It affects people of all weights. Let's work towards promoting more inclusive health approaches to avoid detrimental effects.\"}  \n",
       "5                                                                                                                                                                                   {\\n  \"headline\": \"Weight stigma and health behaviors: Key findings from recent research\",\\n  \"body\": \"Weight discrimination is associated with negative effects on physical health and multiple health behaviors including eating behavior, alcohol use, and sleep. Research found that weight stigma is significantly associated with poorer health behaviors, independent of BMI. These findings suggest the need to reduce weight stigma and employ more weight-inclusive approaches to health promotion.\",\\n  \"audience\": \"Weight discrimination can impact physical health and health behaviors like eating, alcohol use, and sleep. Research, in a diverse U.S. sample, found that weight stigma is associated with poorer health behaviors, regardless of body mass index. Addressing weight stigma is important for promoting healthy behaviors and overall well-being.\",\\n }  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_dict[iteration_id][['folder', 'summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"headline\": \"Weight stigma and health behaviors: Key findings from recent research\",\\n  \"body\": \"Weight discrimination is associated with negative effects on physical health and multiple health behaviors including eating behavior, alcohol use, and sleep. Research found that weight stigma is significantly associated with poorer health behaviors, independent of BMI. These findings suggest the need to reduce weight stigma and employ more weight-inclusive approaches to health promotion.\",\\n  \"audience\": \"Weight discrimination can impact physical health and health behaviors like eating, alcohol use, and sleep. Research, in a diverse U.S. sample, found that weight stigma is associated with poorer health behaviors, regardless of body mass index. Addressing weight stigma is important for promoting healthy behaviors and overall well-being.\",\\n }'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_dict[iteration_id].loc[5, 'summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"headline\": \"Weight stigma and health behaviors: Key findings from recent research\",\\n  \"body\": \"Weight discrimination is associated with negative effects on physical health and multiple health behaviors including eating behavior, alcohol use, and sleep. Research found that weight stigma is significantly associated with poorer health behaviors, independent of BMI. These findings suggest the need to reduce weight stigma and employ more weight-inclusive approaches to health promotion.\",\\n  \"audience\": \"Weight discrimination can impact physical health and health behaviors like eating, alcohol use, and sleep. Research, in a diverse U.S. sample, found that weight stigma is associated with poorer health behaviors, regardless of body mass index. Addressing weight stigma is important for promoting healthy behaviors and overall well-being.\",\\n }'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_dict[iteration_id].loc[5, 'summary'].replace(r'\",\\n', r'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"headline\": \"Weight stigma and health behaviors: Key findings from recent research\",\\n  \"body\": \"Weight discrimination is associated with negative effects on physical health and multiple health behaviors including eating behavior, alcohol use, and sleep. Research found that weight stigma is significantly associated with poorer health behaviors, independent of BMI. These findings suggest the need to reduce weight stigma and employ more weight-inclusive approaches to health promotion.\",\\n  \"audience\": \"Weight discrimination can impact physical health and health behaviors like eating, alcohol use, and sleep. Research, in a diverse U.S. sample, found that weight stigma is associated with poorer health behaviors, regardless of body mass index. Addressing weight stigma is important for promoting healthy behaviors and overall well-being.\",\\n }'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_dict[iteration_id].loc[5, 'summary'].rstrip(',\\n ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 5 column 2 (char 848)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m  \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mheadline\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWeight stigma and health behaviors: Key findings from recent research\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m  \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWeight discrimination is associated with negative effects on physical health and multiple health behaviors including eating behavior, alcohol use, and sleep. Research found that weight stigma is significantly associated with poorer health behaviors, independent of BMI. These findings suggest the need to reduce weight stigma and employ more weight-inclusive approaches to health promotion.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m  \u001b[39m\u001b[39m\"\u001b[39m\u001b[39maudience\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWeight discrimination can impact physical health and health behaviors like eating, alcohol use, and sleep. Research, in a diverse U.S. sample, found that weight stigma is associated with poorer health behaviors, regardless of body mass index. Addressing weight stigma is important for promoting healthy behaviors and overall well-being.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m }\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m json\u001b[39m.\u001b[39;49mloads(text)\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\ginkgo\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\ginkgo\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\ginkgo\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[39ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 5 column 2 (char 848)"
     ]
    }
   ],
   "source": [
    "text = '{\\n  \"headline\": \"Weight stigma and health behaviors: Key findings from recent research\",\\n  \"body\": \"Weight discrimination is associated with negative effects on physical health and multiple health behaviors including eating behavior, alcohol use, and sleep. Research found that weight stigma is significantly associated with poorer health behaviors, independent of BMI. These findings suggest the need to reduce weight stigma and employ more weight-inclusive approaches to health promotion.\",\\n  \"audience\": \"Weight discrimination can impact physical health and health behaviors like eating, alcohol use, and sleep. Research, in a diverse U.S. sample, found that weight stigma is associated with poorer health behaviors, regardless of body mass index. Addressing weight stigma is important for promoting healthy behaviors and overall well-being.\",\\n }'\n",
    "json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'headline': 'Weight stigma and health behaviors: Key findings from recent research',\n",
       " 'body': 'Weight discrimination is associated with negative effects on physical health and multiple health behaviors including eating behavior, alcohol use, and sleep. Research found that weight stigma is significantly associated with poorer health behaviors, independent of BMI. These findings suggest the need to reduce weight stigma and employ more weight-inclusive approaches to health promotion.',\n",
       " 'audience': 'Weight discrimination can impact physical health and health behaviors like eating, alcohol use, and sleep. Research, in a diverse U.S. sample, found that weight stigma is associated with poorer health behaviors, regardless of body mass index. Addressing weight stigma is important for promoting healthy behaviors and overall well-being.'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '{\\n  \"headline\": \"Weight stigma and health behaviors: Key findings from recent research\",\\n  \"body\": \"Weight discrimination is associated with negative effects on physical health and multiple health behaviors including eating behavior, alcohol use, and sleep. Research found that weight stigma is significantly associated with poorer health behaviors, independent of BMI. These findings suggest the need to reduce weight stigma and employ more weight-inclusive approaches to health promotion.\",\\n  \"audience\": \"Weight discrimination can impact physical health and health behaviors like eating, alcohol use, and sleep. Research, in a diverse U.S. sample, found that weight stigma is associated with poorer health behaviors, regardless of body mass index. Addressing weight stigma is important for promoting healthy behaviors and overall well-being.\"\\n }'\n",
    "json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {\"headline\": \"New Study Reveals How Food Cues and Stress Increase Cravings for Unhealthy Foods\",\n",
      "\"body\": \"A recent study conducted in a controlled setting found that exposure to food cues and stress can significantly increase cravings for highly palatable foods. The study also showed that these cravings directly predicted subsequent intake of unhealthy snacks. Furthermore, the hormone ghrelin was found to play a role in promoting food cravings, particularly in individuals who are overweight. The findings highlight the need for greater understanding of the biobehavioral processes that contribute to overeating and weight gain.\",\n",
      "\"audience\": \"Discover how exposure to food cues and stress can affect our cravings for unhealthy foods: the latest research sheds light on the role of hormonal responses and the potential impact on weight management.\"}\n",
      "\n",
      "1 {\n",
      "  \"headline\": \"New Study Shows That Food Cues and Stress Increase Cravings and Intake of Unhealthy Foods\",\n",
      "  \"body\": \"In a recent study conducted in a controlled hospital setting, researchers found that exposure to food cues and stress both significantly increased cravings for highly palatable (HP) foods. These cravings, in turn, predicted greater intake of HP foods. Additionally, the study revealed that specific hormones, such as ghrelin and cortisol, may play a role in food motivation and intake. The findings highlight the potential factors influencing overeating and weight gain, and provide insights into the biobehavioral processes driving unhealthy food consumption.\",\n",
      "  \"audience\": \"A recent study conducted in a controlled hospital setting found that exposures to food cues and stress may increase unhealthy food cravings and intake. The findings suggest important factors to consider in controlling food consumption and understanding the impact on weight gain.\" \n",
      "}\n",
      "\n",
      "2 {\"headline\": \"New Research Shows Dehydration Can Affect Pain Sensitivity in Women\",\n",
      "\"body\": \"A recent study found that mild dehydration increases pain sensitivity in women, supporting previous research in men. The study also investigated the effects of menstrual phase on pain sensitivity, but found no significant difference. Interestingly, acute water ingestion did not reduce pain sensitivity in hypohydrated participants. This research highlights the importance of staying properly hydrated to manage pain symptoms.\",\n",
      "\"audience\": \"Dehydration can have a negative impact on pain response in women, according to new research. It's essential to focus on staying hydrated to help manage pain effectively.\"}\n",
      "\n",
      "3 {\"headline\": \"New research shows that dehydration increases pain sensitivity in women\", \n",
      "\"body\": \"A recent study found that mild dehydration can increase experimental pain sensitivity in women, leading to decreased pain tolerance and increased pain intensity and unpleasantness. The study also examined the effects of menstrual phase on pain sensitivity, but found that it did not greatly impact pain perception. Additionally, acute water ingestion did not reduce pain sensitivity. This research highlights the importance of maintaining adequate hydration for women's wellbeing and suggests a link between dehydration and pain perception.\", \n",
      "\"audience\": \"Have you ever considered the effects of dehydration on pain perception? Recent research shows that even mild dehydration can affect how we perceive pain. This is particularly important for women, as they may experience greater pain sensitivity under dehydrated conditions. Ensuring proper hydration can play a role in managing pain symptoms and supporting overall health.\"}\n",
      "\n",
      "4 {\"headline\": \"Weight Stigma and its Impact on Health Behaviors\",\n",
      " \"body\": \"Weight stigma negatively affects health behaviors such as eating, physical activity, alcohol use, and sleep, according to a US study. The research highlights that weight stigma is associated with disordered eating, higher alcohol consumption, and poorer sleep quality. It also found that weight stigma affects health behaviors across different weights. This shows the need to reduce weight stigma and promote weight-inclusive health approaches.\",\n",
      " \"audience\": \"New study highlights the negative impact of weight stigma on eating, exercise, alcohol use, and sleep. It affects people of all weights. Let's work towards promoting more inclusive health approaches to avoid detrimental effects.\"}\n",
      "\n",
      "5 {\n",
      "  \"headline\": \"Weight stigma and health behaviors: Key findings from recent research\",\n",
      "  \"body\": \"Weight discrimination is associated with negative effects on physical health and multiple health behaviors including eating behavior, alcohol use, and sleep. Research found that weight stigma is significantly associated with poorer health behaviors, independent of BMI. These findings suggest the need to reduce weight stigma and employ more weight-inclusive approaches to health promotion.\",\n",
      "  \"audience\": \"Weight discrimination can impact physical health and health behaviors like eating, alcohol use, and sleep. Research, in a diverse U.S. sample, found that weight stigma is associated with poorer health behaviors, regardless of body mass index. Addressing weight stigma is important for promoting healthy behaviors and overall well-being.\",\n",
      " }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in qna_dict[iteration_id]['summary'].items():\n",
    "    print(index, row)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
