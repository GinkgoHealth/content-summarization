{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "import time\n",
    "import re\n",
    "from itertools import product\n",
    "import openai\n",
    "\n",
    "from response_processing import *\n",
    "from article_processing import create_text_dict_from_folder\n",
    "import traceback\n",
    "from file_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the option to wrap text within cells\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_dict = dict()\n",
    "simple_summaries_dict = dict()\n",
    "relevance_dict = dict()\n",
    "chain_results_dict = dict()\n",
    "qna_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keys for text_dict: dict_keys([1, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create text dictionary\n",
    "folder_path = '../text/2023-06-12 1' # ** UPDATE REQUIRED**\n",
    "\n",
    "encoding='ISO-8859-1'\n",
    "subset=None\n",
    "\n",
    "text_dict = create_text_dict_from_folder(folder_path, encoding=encoding, subset=subset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load most recent response for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/batch_Chaining_attributes_2023-06-12_2110.sav'\n",
    "\n",
    "# loaded_pickle = loadpickle(filename, folder_path)\n",
    "# chatbot_dict[0] = revive_chatbot_dict(loaded_pickle)\n",
    "# sample_Chaining_attr(iteration_id=0)\n",
    "\n",
    "with open(folder_path+filename) as file:\n",
    "    jsonfile = json.load(file)\n",
    "\n",
    "chatbot_dict[0] = revive_chatbot_dict(jsonfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_step = [\n",
    "    \"Think about why this might be relevant for the audience in the grand scheme of things.\\\n",
    "    \\nIdentify 1 or 2 key concepts from this article that would make interesting or helpful health content. \\\n",
    "    Exclude details that do not add value to the audience.\\\n",
    "    \\nBased on the key concepts from the previous steps, extract the key points and numerical descriptors to\",\n",
    "]\n",
    "\n",
    "summarize_task = [\n",
    "    \"summarize for a LinkedIn post.\",\n",
    "    # \"Describe the interesting points to your coworker at the water cooler\",\n",
    "    # \"Create an Instagram post without hashtags.\",\n",
    "]\n",
    "edit_task = [\n",
    "    \"\\nIf applicable, include a brief description of the research participants, such as age and sex.\\\n",
    "    Otherwise, you can skip this step.\\\n",
    "    \\nEvaluate whether or not your writing may be confusing or redundant. \\\n",
    "    \\nIf so, re-write it so it is clear and concise. Otherwise, keep it the same. \\\n",
    "    \\nCreate a journalistic headline to hook the audience.\\\n",
    "    \\nReturn your response in this format:\\\n",
    "    \\n<headline>\\n\\n<summary>\\\n",
    "    \\nwhere the summary is in paragraph form.\\\n",
    "    \\nDo not label the headline and summary.\",\n",
    "]\n",
    "\n",
    "system_role = \"You are a journalist writing content based on science research articles.\"\n",
    "prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task), \n",
    "    columns=['prep_step', 'summarize_task', 'edit_task'])\n",
    "\n",
    "user_simplify_task = [\n",
    "    \"\"\"If needed, rewrite the text using terms appropriate for the audience. If not keep it the same.\\\n",
    "    Follow these steps to accomplish this: \\\n",
    "    \\n1. Check if the content and language are appropriate for the audience. \\\n",
    "    \\n2. If it is suitable for the audience, keep it the same. If not, rewrite using terms appropriate for the audience. \\ \n",
    "    \\n3. Return the final version of the summary to be shown to the audience. \\\n",
    "    \\n\\nYour audience is\"\"\",\n",
    "]\n",
    "\n",
    "simplify_audience = [\n",
    "    # \"a lay audience\",\n",
    "    \"people who are not science experts\",\n",
    "]\n",
    "\n",
    "user_relevance_task = [\n",
    "    \"\"\"Rewrite this summary to include a statement of how it is relevant for the audience. \\\n",
    "        Follow these steps to accomplish this: \\\n",
    "        \\n1. Think about why this might be relevant for the audience in the grand scheme of things.\\\n",
    "        \\n2. If it is not evident why the text is relevant for the audience in the grand scheme of things, \\\n",
    "        add a sentence to inform the audience. Otherwise, keep it the same. \\\n",
    "        \\n3. Modify the summary if needed to reduce redundancy. \\\n",
    "        \\n4. Check if the content and language are appropriate for the audience. \\\n",
    "        If it is suitable for the audience, keep it the same. If not, rewrite using terms appropriate for the audience. \\ \n",
    "        \\n5. Return the final version of the summary to be shown to the audience. \\\n",
    "        \\n6. Remove the backticks.\n",
    "        \\n\\nYour audience consists of\"\"\",\n",
    "]\n",
    "\n",
    "relevance_audience = [\n",
    "    \"seniors\",\n",
    "    \"people who enjoy sports\",\n",
    "    # \"people new to resistance training\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 1: Carry over code from previous notebook to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text1_prompt00...\n",
      "\tNo previous simple prompts for text1_prompt00.\n",
      "\tNo previous relevance prompts for text1_prompt00.\n",
      "Processing text2_prompt00...\n",
      "\tNo previous simple prompts for text2_prompt00.\n",
      "\tNo previous relevance prompts for text2_prompt00.\n",
      "Processing text1_prompt00...\n",
      "Processing text2_prompt00...\n",
      "Original summaries DataFrame shape: (4, 12)\n",
      "\tOriginal summaries Dataframe columns: Index(['choice', 'date', 'folder', 'article_title', 'system_role', 'model',\n",
      "       'text', 'prep step', 'summarization task', 'edit task',\n",
      "       'full summarization task', 'summary'],\n",
      "      dtype='object')\n",
      "Simple summaries DataFrame shape: (4, 6)\n",
      "\tSimple summaries DataFrame columns: ['audience', 'full simplify task', 'original summary', 'simple summary', 'simple summary choice', 'simplify task']\n",
      "Relevance summaries DataFrame shape: (8, 6)\n",
      "\tRelevance summaries DataFrame columns: ['audience', 'full relevance task', 'preceding summary', 'relevance choice', 'relevance statement', 'relevance task']\n",
      "Unique relevance audience values: ['people who enjoy sports', 'seniors']\n",
      "\n",
      "original summaries df columns: Index(['date', 'folder', 'article_title', 'choice', 'system_role', 'model',\n",
      "       'text', 'prep step', 'summarization task', 'edit task',\n",
      "       'full summarization task', 'summary'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "** Merged dataframe shape: (4, 20)\n",
      "['article_title', 'choice', 'system_role', 'model', 'text', 'prep step', 'summarization task', 'full summarization task', 'summary', 'simple summary choice', 'audience', 'simplify task', 'full simplify task', 'simple summary', 'relevance task', 'full relevance task', 'people who enjoy sports', 'add relevance task (seniors)', 'full add relevance task (seniors)', 'seniors']\n",
      "\n",
      "Completed merge_all_chaining_results!:)\n"
     ]
    }
   ],
   "source": [
    "class Chaining:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Text to feed to GPT for summarization.\n",
    "\n",
    "    Optional parameters:\n",
    "    --------------------\n",
    "    system_role : str\n",
    "        The role of the ChatGPT system in the conversation. Default is \"You are an expert at science communication.\"\n",
    "    temperature : float\n",
    "        Controls the randomness of responses. Lower values result in more predictable responses. Default is 0.7.\n",
    "    n_choices : int\n",
    "        Number of ChatGPT responses to generate. Default is 5.\n",
    "    max_tokens : int\n",
    "        Token limit for ChatGPT response. Default is 1000.\n",
    "    model : str\n",
    "        ChatGPT model to use. Default is \"gpt-3.5-turbo\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text, folder_path, system_role=\"You are a helpful assistant.\", \n",
    "            model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=1000, \n",
    "        ):\n",
    "        self.text = text\n",
    "        self.folder = re.sub(r'(?:.*\\/)?(.*)$', r'\\1', folder_path)\n",
    "        self.system_role = system_role\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "\n",
    "    def create_prompt(self, task, text):\n",
    "        \"\"\"\n",
    "        Creates a prompt for ChatGPT with the given task and text.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        task : str\n",
    "            The task to include in the ChatGPT prompt.\n",
    "        text : str\n",
    "            The text to include in the ChatGPT prompt.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        messages : list\n",
    "            A list of dictionaries representing the system and user messages in the prompt.\n",
    "        \"\"\"\n",
    "        system_role = f'{self.system_role}'\n",
    "        user_input = f\"\"\"Given the following text delimited by triple backticks: ```{text}``` \\n {task}\"\"\"\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_input},]\n",
    "\n",
    "        print('\\tDone creating prompt')\n",
    "        return messages\n",
    "\n",
    "    def gpt(self, messages, n_choices, temperature):\n",
    "        \"\"\"\n",
    "        Sends a request to the ChatGPT API with the given messages.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        messages : list\n",
    "            A list of dictionaries representing the system and user messages in the prompt.\n",
    "        n_choices : int\n",
    "            Number of ChatGPT responses to generate.\n",
    "        temperature : float\n",
    "            Controls the randomness of responses. Lower values result in more predictable responses.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        response : dict\n",
    "            A dictionary representing the ChatGPT response.\n",
    "        \"\"\"\n",
    "        print('\\tSending request to GPT-3')\n",
    "        print(f'\\t\\tRequesting {n_choices} choices using {self.model}')\n",
    "        openai.api_key = os.getenv('api_openai')\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model, messages=messages, \n",
    "            temperature=temperature, \n",
    "            max_tokens=self.max_tokens,\n",
    "            n=n_choices\n",
    "            )\n",
    "        print('\\tDone sending request to GPT-3')\n",
    "        return response\n",
    "\n",
    "    def summarize(self, task, prep_step=None, edit_task=None, n_choices=5):\n",
    "        \"\"\"\n",
    "        Generates summaries from the text using ChatGPT.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        task : str\n",
    "            The task to include in the ChatGPT prompt.\n",
    "        prep_step : str, optional\n",
    "            A preparatory step for the task, if applicable.\n",
    "        edit_task : str, optional\n",
    "            The final step for the task, if applicable.\n",
    "        n_choices : int, optional\n",
    "            Number of ChatGPT responses to generate. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        qna : dict\n",
    "            A dictionary representing the summarization task and the generated summaries.\n",
    "        \"\"\"\n",
    "        chatbot = Chaining(self.text, self.folder)\n",
    "        full_task = f'{prep_step} {task} {edit_task}'\n",
    "        prompt = chatbot.create_prompt(full_task, self.text)\n",
    "        firstline_pattern = r'\\s?(\\S*)(\\n*)(.+)'\n",
    "        title = re.match(firstline_pattern, self.text)[0]\n",
    "        self.qna = dict() \n",
    "        self.qna['date'] = datetime.now().strftime(\"%Y-%m-%d %H%M\")\n",
    "        self.qna['folder'] = self.folder\n",
    "        self.qna['article_title'] = title\n",
    "        self.qna['system_role'] = self.system_role\n",
    "        self.qna['model'] = self.model\n",
    "        self.qna[f'text'] = self.text\n",
    "        self.qna['prep step'] = prep_step\n",
    "        self.qna['summarization task'] = task\n",
    "        self.qna['edit task'] = edit_task\n",
    "        self.qna['full summarization task'] = full_task\n",
    "        self.summaries_dict = dict()\n",
    "        self.article_title = title\n",
    "        self.response_regex = r'response_(.*)'\n",
    "        self.simple_summary_dict = dict()\n",
    "        self.relevance_dict = dict()\n",
    "        self.n_previous_prompts = dict()\n",
    "\n",
    "        try:\n",
    "            response = chatbot.gpt(prompt, n_choices=n_choices, temperature=self.temperature)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**API request failed for `.summarize()`**')\n",
    "            return self.qna\n",
    "        try:\n",
    "            for index, choice in enumerate(response.choices):\n",
    "                self.summaries_dict[f'response_{\"{:02d}\".format(index+1)}'] = choice[\"message\"][\"content\"]\n",
    "            self.qna.setdefault('summary', [])\n",
    "            self.qna['summary'].extend([value for value in self.summaries_dict.values()])\n",
    "            # self.summaries_dict['prep_step'] = prep_step\n",
    "            # self.summaries_dict['task'] = task\n",
    "            # self.summaries_dict['edit_task'] = edit_task\n",
    "            # self.summaries_dict['prompt'] = full_task\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print('\\t**Error with response parsing**')\n",
    "\n",
    "\n",
    "    def simplify(self, simplify_task, audience, \n",
    "                    model=\"gpt-3.5-turbo\", temperature=0.0, n_choices=1, \n",
    "                    pause_per_request=0\n",
    "                    ):\n",
    "        simplify_iteration = len(self.simple_summary_dict) + 1 \n",
    "        self.n_previous_prompts['simply_summary'] = len(self.simple_summary_dict)\n",
    "        self.simple_summary_dict[simplify_iteration] = dict()\n",
    "        if simplify_iteration == None:\n",
    "            simplify_iteration = 1\n",
    "        full_simplify_task = f'{simplify_task} {audience}'\n",
    "        print('simplify_iteration: ', simplify_iteration)\n",
    "        print('Task:', full_simplify_task)\n",
    "        summaries_keys = [key for key in self.summaries_dict.keys() if re.match(self.response_regex, key)]\n",
    "        print('summaries_keys: \\n\\t', summaries_keys)\n",
    "        for key in summaries_keys:\n",
    "            new_key = re.sub(self.response_regex, rf'simple_summary\\1', key)\n",
    "            print(f'\\t\\t...Preparing to summarize {key}')\n",
    "            simplify_prompt = self.create_prompt(full_simplify_task, self.summaries_dict[key])\n",
    "            try:\n",
    "                response = self.gpt(simplify_prompt, n_choices=n_choices, temperature=temperature)\n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                filename = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "                print('\\t**API request failed for `.simplify()`**')\n",
    "                return self.qna\n",
    "            try:\n",
    "                self.simple_summary_dict[simplify_iteration][key] = dict()\n",
    "                for index, choice in enumerate(response.choices):\n",
    "                    self.simple_summary_dict[simplify_iteration][key][index] = {\n",
    "                        'simple summary choice': index+1, \n",
    "                        'simplify task': simplify_task,\n",
    "                        'audience': audience,\n",
    "                        'full simplify task': f'{simplify_task} {\"for\" if audience else \"\"} {audience}',\n",
    "                        'simple summary': choice[\"message\"][\"content\"],\n",
    "                        'original summary': self.summaries_dict[key]\n",
    "                    }\n",
    "                    print(f'\\t...Summary given')\n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                filename = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "                self.simple_summary_dict[simplify_iteration][new_key] = response\n",
    "                print(f'\\t...Error parsing response for summary request')\n",
    "            if pause_per_request > 0:\n",
    "                print(f'[.simplify()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                time.sleep(pause_per_request)\n",
    "        return self.simple_summary_dict\n",
    "    \n",
    "    def add_relevance(self, relevance_task, audience, \n",
    "                    model=\"gpt-3.5-turbo\", temperature=0.0, n_choices=1, summary_type='original',\n",
    "                    # relevance_iteration=None, \n",
    "                    pause_per_request=0\n",
    "                    ):\n",
    "        relevance_iteration = len(self.relevance_dict) + 1 \n",
    "        self.n_previous_prompts['relevance'] = len(self.relevance_dict)\n",
    "        self.relevance_dict[relevance_iteration] = dict()\n",
    "        if relevance_iteration == None:\n",
    "            relevance_iteration = 1\n",
    "        full_relevance_task = f'{relevance_task} {audience}'\n",
    "        print('relevance_iteration: ', relevance_iteration)\n",
    "        print('Task:', full_relevance_task)\n",
    "        if summary_type=='original':\n",
    "            summaries_keys = [key for key in self.summaries_dict.keys() if re.match(self.response_regex, key)]\n",
    "            summary_regex = self.response_regex\n",
    "        else:\n",
    "            self.simple_summary_response_regex = r'simple_summary_(.*)'\n",
    "            summaries_keys = [key for key in self.summaries_dict.keys() if re.match(self.simple_summary_response_regex, key)]\n",
    "            summary_regex = self.simple_summary_response_regex\n",
    "        print('summaries_keys: \\n\\t', summaries_keys)\n",
    "        input_summary_dict = self.summaries_dict if summary_type=='original' else self.simple_summary_dict\n",
    "        for key in summaries_keys:\n",
    "            new_key = re.sub(summary_regex, rf'relevance_\\1', key)\n",
    "            print(f'\\t\\t...Preparing to add relevance to {key}')\n",
    "            relevance_prompt = self.create_prompt(full_relevance_task, input_summary_dict[key])\n",
    "            try:\n",
    "                response = self.gpt(relevance_prompt, n_choices=n_choices, temperature=temperature)\n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                filename = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "                print('\\t**API request failed for `.add_relevance()`**')\n",
    "                return self.qna\n",
    "            try:\n",
    "                self.relevance_dict[relevance_iteration][key] = dict()\n",
    "                for index, choice in enumerate(response.choices):\n",
    "                    self.relevance_dict[relevance_iteration][key][index] = {\n",
    "                        'relevance choice': index+1, \n",
    "                        'relevance task': relevance_task,\n",
    "                        'audience': audience,\n",
    "                        'full relevance task': full_relevance_task,\n",
    "                        'relevance statement': choice[\"message\"][\"content\"],\n",
    "                        'preceding summary': input_summary_dict[key]\n",
    "                    }\n",
    "                    print(f'\\t...Relevance statement given')\n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                filename = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "                self.relevance_summary_dict[relevance_iteration][new_key] = response\n",
    "                print(f'\\t...Error parsing response for relevance request')\n",
    "            if pause_per_request > 0:\n",
    "                print(f'[.add_relevance()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                time.sleep(pause_per_request)\n",
    "        return self.relevance_dict\n",
    "    \n",
    "def batch_summarize_chain(text_dict, folder_path, prep_step, summarize_task, edit_task, chaining_bot_dict, iteration_id, \n",
    "    system_role=None, temperature=0.7, pause_per_request=0, n_choices=5,\n",
    "    save_outputs=False, csv_path=folder_path, pickle_path=folder_path, json_path=folder_path\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Summarize multiple texts using the same prompts.\n",
    "    Parameters:\n",
    "        - text_dict (dict) A dictionary containing the text data to be summarized. \n",
    "            The keys of the dictionary are the text IDs and the values are the full texts.\n",
    "        - prep_step, summarize_task, edit task (list)\n",
    "        - qna_dict: Dictionary to store the input and outputs.\n",
    "        - iteration_id (int, float, or string): Unique ID serving as the key for results in the qna_dict\n",
    "\n",
    "        iteration_id: int, float or string\n",
    "            A unique identifier for the current iteration.\n",
    "        temperature: float, optional (default=0.7)\n",
    "            The level of \"creativity\" to use when generating summaries. Higher temperatures will result in more diverse summaries, but may also result in lower quality summaries.\n",
    "        pause_per_request: int or float, optional (default=0)\n",
    "            The number of seconds to pause between requests to avoid exceeding API rate limits. Defaults to 0, which means no pause.\n",
    "        save_outputs: bool, optional (default=False)\n",
    "            Whether to save the outputs of the summarization process to disk.\n",
    "        filename: str, optional (default=None)\n",
    "            The name of the file to save the outputs to. If no filename is specified, a default filename will be used.\n",
    "        csv_path: str, optional \n",
    "            The path to the directory where CSV output files will be saved. Defaults to the 'output' folder in the project directory.\n",
    "        pickle_path: str, optional \n",
    "            The path to the directory where pickle output files will be saved. Defaults to the 'pickles' folder in the project directory.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        chaining_bot_dict: dict\n",
    "            A dictionary containing the Chaining instances. \n",
    "                The keys of the dictionary are the iteration IDs and the values are dictionaries whose\n",
    "                values are the Chaining instances.\n",
    "\n",
    "    \"\"\"\n",
    "    prompts_df = pd.DataFrame(product(prep_step, summarize_task, edit_task), \n",
    "        columns=['prep_step', 'summarize_task', 'edit_task'])\n",
    "\n",
    "    chaining_bot_dict[iteration_id] = dict()\n",
    "    for key in text_dict:\n",
    "        text = text_dict[key]\n",
    "        for index in prompts_df.index:\n",
    "            print(f'**Text #{key} prompt #{index} of {prompts_df.index.max()}**')\n",
    "            task = prompts_df.loc[index, 'summarize_task']\n",
    "            prep_step = prompts_df.loc[index, 'prep_step']\n",
    "            edit_task = prompts_df.loc[index, 'edit_task']\n",
    "            try:\n",
    "                print('Creating Chaining class instance')\n",
    "                chatbot = Chaining(\n",
    "                    text, folder_path=folder_path, temperature=temperature, system_role=system_role)\n",
    "                print('Chaining class instance created')\n",
    "                chatbot.summarize(\n",
    "                    task=task, prep_step=prep_step, edit_task=edit_task, n_choices=n_choices\n",
    "                    )\n",
    "                chaining_bot_dict[iteration_id][f'text{key}_prompt{\"{:02d}\".format(index)}'] = chatbot\n",
    "                print('\\t...Success!')\n",
    "                if pause_per_request > 0:\n",
    "                    print(f'[batch_summarize()] Sleeping {pause_per_request} sec to avoid exceeding API rate limit')\n",
    "                    time.sleep(pause_per_request) # Account for API rate limit of 3 API requests/limit \n",
    "            except Exception as error:\n",
    "                exc_type, exc_obj, tb = sys.exc_info()\n",
    "                f = tb.tb_frame\n",
    "                lineno = tb.tb_lineno\n",
    "                file = f.f_code.co_filename\n",
    "                print(\"An error occurred on line\", lineno, \"in\", file, \":\", error)\n",
    "                print('\\t...Error making chatbot request')\n",
    "                break\n",
    "    if save_outputs:\n",
    "        try:\n",
    "            save_instance_to_dict(\n",
    "                chaining_bot_dict[iteration_id], \n",
    "                description=f'batch_Chaining_attributes_initial',\n",
    "                ext=None, json_path=json_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[batch_summarize_chain()] Unable to save API response')\n",
    "\n",
    "    return chaining_bot_dict\n",
    "\n",
    "def create_qna_df(\n",
    "    qna_dict, chatbot_dict, iteration_id, chatbot_id=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create DataFrame from initial ChatGPT summaries.\n",
    "    \"\"\"\n",
    "    dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        dfs_list.append(pd.DataFrame(\n",
    "            chatbot_dict[chatbot_id][chatbot_key].qna, \n",
    "            index=[choice for choice in range(1, len(chatbot_dict[chatbot_id][chatbot_key].qna['summary'])+1)])\n",
    "            )\n",
    "    \n",
    "    qna_df = pd.concat(dfs_list).reset_index(names=['choice'])\n",
    "    columns = qna_df.columns.tolist()\n",
    "    columns.remove('choice')\n",
    "    columns.insert(3, 'choice') # Move 'choice' column\n",
    "\n",
    "    # qna_df['date'] = pd.Series('2023-06-12', index=qna_df.index)\n",
    "    # columns.insert(0, 'date')\n",
    "\n",
    "    qna_dict[iteration_id] = qna_df[columns]\n",
    "    print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    print(f'\\tOriginal summaries Dataframe columns: {qna_df.columns}')\n",
    "    return qna_dict\n",
    "\n",
    "def spreadsheet_columns(qna_dict, chatbot_dict, iteration_id, chatbot_id=None,\n",
    "    save=False, filename=None, path=folder_path\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Update column names to include corresponding column in a spreadsheet (e.g. A, B, C)\n",
    "    \"\"\"\n",
    "    qna_dict = create_qna_df(\n",
    "        qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id, \n",
    "        )\n",
    "    qna_dict[iteration_id]['date'] = qna_dict[iteration_id]['date'].str.replace(r'_\\d*', r'', regex=True)\n",
    "    spreadsheet_columns = [letter for letter in string.ascii_uppercase]+['A'+letter for letter in string.ascii_uppercase]\n",
    "    qna_dict[iteration_id].columns = [\n",
    "        f'{spreadsheet_columns[index]}: {column}' for index, column in enumerate(qna_dict[iteration_id].columns)\n",
    "        ]\n",
    "    str_columns = qna_dict[iteration_id].dtypes[qna_dict[iteration_id].dtypes == 'O'].index.tolist()\n",
    "    for column in str_columns:\n",
    "        qna_dict[iteration_id][column] = qna_dict[iteration_id][column].str.strip()\n",
    "    if save:\n",
    "        description = filename if filename else 'batch_Chaining_summaries_initial'\n",
    "        try:\n",
    "            save_csv(\n",
    "                qna_dict[iteration_id], filename=description, append_version=True,\n",
    "                path=path, index=False\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            file = f.f_code.co_filename\n",
    "            print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "            print('[spreadsheet_columns()] Unable to save original summaries DataFrame')\n",
    "    return qna_dict\n",
    "\n",
    "def prompt_chaining_dict(simplify_prompts, audience, simple_summaries_dict, chaining_bot_dict, iteration_id,\n",
    "    summary_iteration_id=None, n_choices=None, pause_per_request=0,\n",
    "    prompt_column='simplify', \n",
    "    # simplify_iteration=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Simplify or add context to a summary.\n",
    "    \"\"\"\n",
    "    summary_iteration_id = summary_iteration_id if summary_iteration_id else iteration_id\n",
    "    print('summary_iteration_id:', summary_iteration_id)\n",
    "    prompts_df = pd.DataFrame(product(simplify_prompts, audience), columns=[prompt_column, 'audience'])\n",
    "    if n_choices == None:\n",
    "        n_choices = 1 if prompt_column == 'simplify' else 5\n",
    "    print('n_choices:', n_choices)\n",
    "\n",
    "    simple_summaries_master_list = []\n",
    "    for text_prompt_key in chaining_bot_dict.keys():\n",
    "        print(f'**{text_prompt_key}')\n",
    "\n",
    "        for index in prompts_df.index:\n",
    "            prompt = prompts_df.loc[index, prompt_column]\n",
    "            audience = prompts_df.loc[index, 'audience']\n",
    "            if prompt_column == 'simplify':\n",
    "                summary_dict = chaining_bot_dict[text_prompt_key].simplify(\n",
    "                    prompt, audience, n_choices=n_choices, pause_per_request=pause_per_request, \n",
    "                    )\n",
    "            else: \n",
    "                summary_dict = chaining_bot_dict[text_prompt_key].add_relevance(\n",
    "                    prompt, audience, n_choices=n_choices, pause_per_request=pause_per_request, \n",
    "                    )\n",
    "            simple_summaries_master_list.append(summary_dict)\n",
    "  \n",
    "    simple_summaries_dict[iteration_id] = simple_summaries_master_list\n",
    "    return simple_summaries_dict\n",
    "\n",
    "def merge_all_chaining_results2(\n",
    "    chatbot_dict, qna_dict, iteration_id, \n",
    "    empty_columns=None, pivot=True, validate=None,\n",
    "    chatbot_id=None, save_df=False, save_chatbot=False, \n",
    "    csv_path=folder_path,\n",
    "    pickle_path=None,\n",
    "    json_path=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create a dataframe of original, 'simple', 'relevance' summaries from a Chaining object.\n",
    "    Merge it with the original summaries DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        - chain_results_dict (dict): dictionary of DataFrames.\n",
    "        - chatbot_dict (dict): dictionary of Chaining objects.\n",
    "        - iteration_id (int, float, or string): iteration_id (dict key) of the chatbot_dict to process.\n",
    "        - empty_columns (Bool, int, or dict): dictionary of empty columns to add to the DataFrame. \n",
    "            If True or 1, default dictionary is used.\n",
    "            If False or 0, no empty columns are added.\n",
    "        - pivot (Bool): whether to pivot the relevance summaries DataFrame. Default is True.\n",
    "        - validate (str): Argument to pass to pd.merge() to validate the merge.\n",
    "        - chatbot_id (int, float, or string): chatbot_id (dict key) of the chatbot_dict to process.\n",
    "        - save_df, save_chatbot (Bool): whether to save the DataFrame and chatbot_dict.\n",
    "        - csv_path, pickle_path, and json_path (raw string or string): Location to save the \n",
    "            outputs. Must provide csv_path to save; pickle_path and json_path are optional and \n",
    "            default to the same as csv_path if not provided.\n",
    "    \"\"\"\n",
    "    df_list_simple = []\n",
    "    df_list_relevance = []\n",
    "    qna_dfs_list = []\n",
    "    chatbot_id = iteration_id if chatbot_id == None else chatbot_id\n",
    "    for chatbot_key in chatbot_dict[chatbot_id].keys():\n",
    "        print(f'Processing {chatbot_key}...')\n",
    "        try: \n",
    "            n_previous_prompts_simple = chatbot_dict[chatbot_id][chatbot_key].previous_n_prompts['simple']\n",
    "            print(f'\\tNumber of previous relevance prompts: {n_previous_prompts_simple}', end='.')\n",
    "        except:\n",
    "            n_previous_prompts_simple = 0\n",
    "            print(f'\\tNo previous simple prompts for {chatbot_key}', end='.')\n",
    "        print('')\n",
    "        try: \n",
    "            n_previous_prompts_relevance = chatbot_dict[chatbot_id][chatbot_key].previous_n_prompts['relevance']\n",
    "            print(f'\\tNumber of previous relevance prompts: {n_previous_prompts_relevance}', end='.')\n",
    "        except:\n",
    "            n_previous_prompts_relevance = 0\n",
    "            print(f'\\tNo previous relevance prompts for {chatbot_key}', end='.')\n",
    "        print('')\n",
    "            \n",
    "        qna_dfs_list.append(pd.DataFrame(chatbot_dict[chatbot_id][chatbot_key].qna).reset_index(names=['choice']))\n",
    "\n",
    "        # create results dictionaries that only grabs the results of the new prompts instead of all\n",
    "        results_dict_simple = dict()\n",
    "        total_n_prompts_simple = len(chatbot_dict[chatbot_id][chatbot_key].simple_summary_dict)\n",
    "        for prompt_number in range(n_previous_prompts_simple+1, total_n_prompts_simple+1):\n",
    "            results_dict_simple[prompt_number] = chatbot_dict[chatbot_id][chatbot_key].simple_summary_dict[prompt_number]\n",
    "        chatbot_dict[chatbot_id][chatbot_key].simple_summary_dict\n",
    "\n",
    "        results_dict_relevance = dict()\n",
    "        total_n_prompts_relevance = len(chatbot_dict[chatbot_id][chatbot_key].relevance_dict)\n",
    "        for prompt_number_relevance in range(n_previous_prompts_relevance+1, total_n_prompts_relevance+1):\n",
    "            # print(f'\\tAppending results for prompt {prompt_number_relevance} of {total_n_prompts_relevance}')\n",
    "            results_dict_relevance[prompt_number_relevance] = chatbot_dict[chatbot_id][chatbot_key].relevance_dict[prompt_number_relevance]\n",
    "\n",
    "        for iteration_key_simple in results_dict_simple.keys():\n",
    "            response_keys_simple = sorted([text_prompt_key for text_prompt_key in results_dict_simple[iteration_key_simple].keys()])\n",
    "            # print(f'\\tAppending results for {iteration_key_simple}: ', end='')\n",
    "\n",
    "            for response_key_simple in response_keys_simple:\n",
    "                df_list_simple.append(pd.DataFrame(results_dict_simple[iteration_key_simple][response_key_simple]).transpose())\n",
    "        for iteration_key_relevance in results_dict_relevance.keys():\n",
    "            response_keys_relevance = sorted([text_prompt_key for text_prompt_key in results_dict_relevance[iteration_key_relevance].keys()])\n",
    "            for response_key_relevance in response_keys_relevance:\n",
    "                df_list_relevance.append(pd.DataFrame(results_dict_relevance[iteration_key_relevance][response_key_relevance]).transpose())\n",
    "    \n",
    "    simple_summary_df = pd.concat(df_list_simple)\n",
    "    relevance_df = pd.concat(df_list_relevance)\n",
    "    qna_df = create_qna_df(qna_dict, chatbot_dict, iteration_id, chatbot_id)[iteration_id]\n",
    "    # qna_df.rename(columns={'summary': 'original summary'}, inplace=True)\n",
    "    # print(f'Original summaries DataFrame shape: {qna_df.shape}')\n",
    "    # print(f'Original summaries Dataframe columns: {qna_df.columns}')\n",
    "    print('Simple summaries DataFrame shape:', simple_summary_df.shape)\n",
    "    print(f'\\tSimple summaries DataFrame columns: {[col for col in simple_summary_df.columns]}')\n",
    "    print('Relevance summaries DataFrame shape:', relevance_df.shape)\n",
    "    print(f'\\tRelevance summaries DataFrame columns: {[col for col in relevance_df.columns]}')\n",
    "\n",
    "    relevance_audience_list = sorted(relevance_df.audience.unique().tolist())\n",
    "    print(f'Unique relevance audience values: {relevance_audience_list}')\n",
    "    print(f'\\noriginal summaries df columns: {qna_df.columns}\\n')\n",
    "\n",
    "    new_results = qna_df.merge(\n",
    "        simple_summary_df, how='right',\n",
    "        right_on='original summary',\n",
    "        left_on='summary',\n",
    "        validate=validate\n",
    "        ).drop(columns='original summary')\n",
    "    if pivot == False:\n",
    "        spreadsheet_column_names = [\n",
    "            \"article_title\",\n",
    "            \"choice\",\n",
    "            \"system_role\",\n",
    "            \"model\",\n",
    "            \"text\",\n",
    "            \"prep step\",\n",
    "            \"summarization task\",\n",
    "            \"full summarization task\",\n",
    "            \"summary\",\n",
    "            \"simple summary choice\",\n",
    "            \"audience simplify\",\n",
    "            \"simplify task\",\n",
    "            \"full simplify task\",\n",
    "            \"simple summary\",\n",
    "            \"audience relevance\",\n",
    "            \"relevance task\",\n",
    "            \"full relevance task\",\n",
    "            \"relevance statement\"\n",
    "        ]  \n",
    "\n",
    "        validate=None\n",
    "        \n",
    "        print(f'DataFrame shape after merging with simple summaries: {new_results.shape}')\n",
    "        print(f'\\tColumns after merging with simple summaries: {[col for col in new_results.columns]}')\n",
    "        new_results= new_results.merge(\n",
    "            relevance_df, how='outer', suffixes=(' simplify', ' relevance'),\n",
    "            left_on='summary', right_on='preceding summary', validate=validate\n",
    "            ).drop(columns='preceding summary')\n",
    "    else:\n",
    "        spreadsheet_column_names = [\n",
    "            \"article_title\",\n",
    "            \"choice\",\n",
    "            \"system_role\",\n",
    "            \"model\",\n",
    "            \"text\",\n",
    "            \"prep step\",\n",
    "            \"summarization task\",\n",
    "            \"full summarization task\",\n",
    "            \"summary\",\n",
    "            \"simple summary choice\",\n",
    "            \"audience\",\n",
    "            \"simplify task\",\n",
    "            \"full simplify task\",\n",
    "            \"simple summary\",\n",
    "            \"relevance task\",\n",
    "            \"full relevance task\"\n",
    "        ] \n",
    "        relevance_pivot_df = relevance_df.pivot(\n",
    "            columns=['audience'],\n",
    "            values='relevance statement',\n",
    "            index=['preceding summary', 'relevance task',]\n",
    "        ).sort_index().reset_index()\n",
    "        new_results = new_results.merge(\n",
    "            relevance_pivot_df, how='outer', suffixes=(' simplify', ' relevance'),\n",
    "            left_on='summary', right_on='preceding summary',\n",
    "            validate='m:1' if validate else None\n",
    "        ).drop(columns='preceding summary')\n",
    "        new_results['full relevance task'] = new_results['relevance task'].apply(lambda x: f'{x} {relevance_audience_list[0]}')\n",
    "        new_results['add relevance task (seniors)'] = new_results[\"relevance task\"]\n",
    "        new_results['full add relevance task (seniors)'] =new_results['relevance task'].apply(lambda x: f'{x} {relevance_audience_list[1]}')\n",
    "        spreadsheet_column_names.append(relevance_audience_list[0])\n",
    "        spreadsheet_column_names.append('add relevance task (seniors)')\n",
    "        spreadsheet_column_names.append('full add relevance task (seniors)')\n",
    "        spreadsheet_column_names.append(relevance_audience_list[1])\n",
    "        \n",
    "    new_results = new_results[spreadsheet_column_names]\n",
    "    if empty_columns:\n",
    "        if pivot == False:\n",
    "            if (type(empty_columns) != dict):\n",
    "                empty_columns = {\n",
    "                    # \"choice numnber\": \"C\",\n",
    "                    \"original summary content rating\": \"K\",\n",
    "                    \"original summary language rating\": \"L\",\n",
    "                    \"top summary\": \"M\",\n",
    "                    \"simple summary content rating\": \"S\",\n",
    "                    \"simple summary language rating\": \"T\",\n",
    "                    \"top simple summary\": \"U\",\n",
    "                }\n",
    "        else:           \n",
    "            if (type(empty_columns) != dict):\n",
    "                empty_columns = {\n",
    "                    # \"choice numnber\": \"C\",\n",
    "                    \"original summary content rating\": \"K\",\n",
    "                    \"original summary language rating\": \"L\",\n",
    "                    \"top summary\": \"M\",\n",
    "                    \"simple summary content rating\": \"S\",\n",
    "                    \"simple summary language rating\": \"T\",\n",
    "                    'top simple summary': 'u',\n",
    "                    # 'full add relevance task': 'w',\n",
    "                    'added relevance content rating': 'y',\n",
    "                    'added relevance language rating': 'z',\n",
    "                    'top added relevance': 'aa',\n",
    "                }\n",
    "        print(f'Merged DataFrame shape: {new_results.shape}')\n",
    "        print('\\nColumns before adding empty columns:', [column for column in new_results.columns])\n",
    "        print('Inserting empty columns...', end='\\n\\t')\n",
    "        spreadsheet_column_names = [letter for letter in string.ascii_uppercase]+['A'+letter for letter in string.ascii_uppercase]\n",
    "        alphabet_dict = {char:idx for idx, char in enumerate(spreadsheet_column_names)}\n",
    "        for column_name, column_number in empty_columns.items():\n",
    "            empty_column_loc = alphabet_dict[empty_columns[column_name].upper()] -1\n",
    "            new_results.insert(loc=empty_column_loc, column=column_name, value='')\n",
    "            print(f'{empty_columns[column_name].upper()} ({empty_column_loc}): {column_name}', end=', ')\n",
    "        new_results.columns = [\n",
    "            f'{spreadsheet_column_names[index+1]}: {column}' for index, column in enumerate(new_results.columns)\n",
    "            ]\n",
    "\n",
    "    print(f'\\n** Merged dataframe shape:', new_results.shape)\n",
    "    print([column for column in new_results.columns])\n",
    "    qna_dict[iteration_id] = new_results\n",
    "    try:\n",
    "        original_summary_time = next(iter(chatbot_dict[chatbot_id].values())).date_created\n",
    "        description_tag = f'_{original_summary_time}_updated'\n",
    "    except:\n",
    "        description_tag=''\n",
    "    try:\n",
    "        original_summary_time = next(iter(chatbot_dict[chatbot_id].values())).date_created\n",
    "        description_tag = f'_{original_summary_time}_updated'\n",
    "        print(f'Original summary time: {original_summary_time}')\n",
    "    except:\n",
    "        description_tag=''\n",
    "    if save_df:\n",
    "        try:\n",
    "            save_output(\n",
    "                qna_dict[iteration_id], \n",
    "                description=f'batch_Chaining_summaries{description_tag}',\n",
    "                csv_path=csv_path, pickle_path=pickle_path)\n",
    "            print('')\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print(f'Unable to save DataFrame')\n",
    "    if save_chatbot:\n",
    "        json_path = csv_path if json_path is None else json_path\n",
    "        try:\n",
    "            print('Saving Chaining object (chatbot)...')\n",
    "            save_instance_to_dict(\n",
    "                chatbot_dict[chatbot_id], \n",
    "                description=f'batch_Chaining_attributes{description_tag}',\n",
    "                pickle_path=pickle_path, json_path=json_path\n",
    "                )\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            print(\"An error occurred on line\", lineno, \"in\", filename, \":\", error)\n",
    "            print(f'Unable to save chatbot')\n",
    "            \n",
    "    return qna_dict\n",
    "\n",
    "# Set parameters\n",
    "iteration_id = 2.10\n",
    "n_choices = 1\n",
    "pause_per_request=0\n",
    "# chatbot_id = iteration_id\n",
    "summary_iteration_id = 1.43\n",
    "save_outputs = False\n",
    "# save = True\n",
    "save = False\n",
    "empty_columns = False\n",
    "\n",
    "# # Create initial summaries\n",
    "# chaining_dict = batch_summarize_chain(\n",
    "#     text_dict, folder_path, prep_step, summarize_task, edit_task, chatbot_dict,\n",
    "#     system_role=system_role, \n",
    "#     n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "#     iteration_id=iteration_id, save_outputs=save_outputs\n",
    "#     )\n",
    "# qna_dict = spreadsheet_columns(\n",
    "#     qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id, save=save\n",
    "#     )\n",
    "\n",
    "# # Create simple summaries\n",
    "# audience = simplify_audience\n",
    "# simple_summaries = prompt_chaining_dict(user_simplify_task, simplify_audience, simple_summaries_dict, \n",
    "#     chatbot_dict[chatbot_id], iteration_id,\n",
    "#     n_choices=1, pause_per_request=pause_per_request, summary_iteration_id=summary_iteration_id\n",
    "#     )\n",
    "\n",
    "# # Add relevance\n",
    "# relevance = prompt_chaining_dict(user_relevance_task, relevance_audience, relevance_dict, \n",
    "#     chatbot_dict[summary_iteration_id], iteration_id, prompt_column='relevance', \n",
    "#     n_choices=1, pause_per_request=pause_per_request, summary_iteration_id=summary_iteration_id\n",
    "#     )\n",
    "\n",
    "# Merge the results\n",
    "# try:\n",
    "#     df_dict = merge_all_chaining_results2(\n",
    "#         chatbot_dict, qna_dict, iteration_id=iteration_id, pivot=True,\n",
    "#         empty_columns=empty_columns, chatbot_id=summary_iteration_id,\n",
    "#         save_df=save, save_chatbot=save, \n",
    "#             csv_path=folder_path,\n",
    "#     )\n",
    "#     print(f'\\nCompleted merge_all_chaining_results!:)')\n",
    "# except Exception as error:\n",
    "#     exc_type, exc_obj, tb = sys.exc_info()\n",
    "#     f = tb.tb_frame\n",
    "#     lineno = tb.tb_lineno\n",
    "#     file = f.f_code.co_filename\n",
    "#     print(f'An error occurred on line {lineno} in {file}: {error}')\n",
    "#     print('Unable to merge results')\n",
    "#     if save:\n",
    "#         save_instance_to_dict(chatbot_dict[chatbot_id], ext=None, json_path=folder_path)\n",
    "#         print(f'\\nCould not merge; saved Chaining instances as JSON.')\n",
    "qna_dict = merge_all_chaining_results2(\n",
    "    chatbot_dict, qna_dict, iteration_id=iteration_id, pivot=True,\n",
    "    empty_columns=empty_columns, chatbot_id=summary_iteration_id,\n",
    "    save_df=save, save_chatbot=save, \n",
    "        csv_path=folder_path,\n",
    ")\n",
    "print(f'\\nCompleted merge_all_chaining_results!:)')\n",
    "\n",
    "# df_dict[iteration_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
