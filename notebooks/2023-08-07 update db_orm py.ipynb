{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the option to wrap text within cells\n",
    "# pd.reset_option('all')\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_df_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from gpt_queue ORDER BY id DESC LIMIT 1\n",
      "Search term: (Sleep and obesity [ti]) AND (Curr Opin Clin Nutr Metab Care [ta])\n",
      "Number of abstract sections: 3\n",
      "Adding 1 rows to the database...\n",
      "\tSleep and obesity\n",
      "Error adding data to the database: (psycopg2.ProgrammingError) can't adapt type 'Series'\n",
      "[SQL: INSERT INTO sources (title, text, abstract, publication, authors, year, month, pub_volume, pub_issue, start_page, end_page, doi, section) VALUES (%(title)s, %(text)s, %(abstract)s, %(publication)s, %(authors)s, %(year)s, %(month)s, %(pub_volume)s, %(pub_issue)s, %(start_page)s, %(end_page)s, %(doi)s, %(section)s) RETURNING sources.id]\n",
      "[parameters: {'title': 'Sleep and obesity', 'text': 'Sleep and Obesity\\r\\n\\r\\nObstructive sleep apnea\\r\\n\\r\\nSleep loss occurs not only as a result of habitual behavior, but also in presence of patholog ... (4122 characters truncated) ...  self-reported and the analysis could not control the presence of a sleep disorder like OSA, which could in part account for the risk of weight gain.', 'abstract': 'PURPOSE OF REVIEW: This review summarizes the most recent evidence linking decreased sleep duration and poor sleep quality to obesity, focusing upon  ... (994 characters truncated) ... ite. Recent epidemiological and laboratory evidence confirm previous findings of an association between sleep loss and increased risk of obesity.<br>', 'publication': publication                       Curr Opin Clin Nutr Metab Care\n",
      "publication    Current opinion in clinical nutrition and meta...\n",
      "Name: 0, dtype: object, 'authors': 'Guglielmo Beccuti, Silvana Pannain', 'year': '2011', 'month': '', 'pub_volume': '14', 'pub_issue': '4', 'start_page': '402', 'end_page': '412', 'doi': '10.1097/MCO.0b013e3283479109', 'section': 'Sleep disturbance and obesity risk'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 302\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m qna_dict[iteration_id]\n\u001b[0;32m    301\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 302\u001b[0m     qna_dict \u001b[39m=\u001b[39m generate_summaries(n_choices, temperature, model, pause_per_request, folder_path, section, local\u001b[39m=\u001b[39;49mlocal, article_limit\u001b[39m=\u001b[39;49marticle_limit)\n",
      "Cell \u001b[1;32mIn[5], line 277\u001b[0m, in \u001b[0;36mgenerate_summaries\u001b[1;34m(n_choices, temperature, model, pause_per_request, folder_path, section, local, article_limit)\u001b[0m\n\u001b[0;32m    273\u001b[0m bulk_append(table\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msources\u001b[39m\u001b[39m'\u001b[39m, input_df\u001b[39m=\u001b[39mreferences_df_dict[iteration_id]) \u001b[39m# db_orm.py\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[39m# ##### \u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39m# Step 3: Get the new sources for summarization\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m sources_df \u001b[39m=\u001b[39m get_from_queue(input_df\u001b[39m=\u001b[39;49mtext_df, order_by\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m, order\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mASC\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    279\u001b[0m \u001b[39m# ##### \u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[39m# Step 4: Create summaries (functions contained in orm_summarize.py)\u001b[39;00m\n\u001b[0;32m    281\u001b[0m chatbot_dict \u001b[39m=\u001b[39m batch_summarize( \u001b[39m# orm_summarize.py\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     sources_df, folder_path, prep_step, summarize_task, edit_task,  \u001b[39m# parameter values found in prompts.py\u001b[39;00m\n\u001b[0;32m    283\u001b[0m     simplify_task, simplify_audience, format_task,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m     iteration_id\u001b[39m=\u001b[39miteration_id, save_outputs\u001b[39m=\u001b[39msave_outputs\n\u001b[0;32m    288\u001b[0m     )\n",
      "File \u001b[1;32m~\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\\db_session.py:57\u001b[0m, in \u001b[0;36mremote_sql_session.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m@wraps\u001b[39m(function)\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m with_remote_sql_session(function, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\\db_session.py:45\u001b[0m, in \u001b[0;36mwith_remote_sql_session\u001b[1;34m(function, *args, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m tunnel\u001b[39m.\u001b[39mstart()\n\u001b[0;32m     44\u001b[0m engine \u001b[39m=\u001b[39m get_engine_for_port(tunnel\u001b[39m.\u001b[39mlocal_bind_port)\n\u001b[1;32m---> 45\u001b[0m \u001b[39mreturn\u001b[39;00m with_sql_session(function, args, kwargs, engine\u001b[39m=\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\\db_session.py:27\u001b[0m, in \u001b[0;36mwith_sql_session\u001b[1;34m(function, args, kwargs, engine)\u001b[0m\n\u001b[0;32m     25\u001b[0m session \u001b[39m=\u001b[39m Session()\n\u001b[0;32m     26\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m function(session, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     28\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     session\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\\db_orm.py:94\u001b[0m, in \u001b[0;36mget_from_queue\u001b[1;34m(session, input_df, order_by, order)\u001b[0m\n\u001b[0;32m     91\u001b[0m     sources_series \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries({column\u001b[39m.\u001b[39mname: \u001b[39mgetattr\u001b[39m(result, column\u001b[39m.\u001b[39mname) \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m result\u001b[39m.\u001b[39m__table__\u001b[39m.\u001b[39mcolumns})\n\u001b[0;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m sources_series\n\u001b[1;32m---> 94\u001b[0m sources_df \u001b[39m=\u001b[39m input_df\u001b[39m.\u001b[39;49mapply(row_to_dict, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     95\u001b[0m ascending \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m order \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mASC\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     96\u001b[0m sources_df\u001b[39m.\u001b[39msort_values(order_by, ascending\u001b[39m=\u001b[39mascending, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\ginkgo\\Lib\\site-packages\\pandas\\core\\frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9557\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9559\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   9560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   9561\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9566\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   9567\u001b[0m )\n\u001b[1;32m-> 9568\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\ginkgo\\Lib\\site-packages\\pandas\\core\\apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 764\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\ginkgo\\Lib\\site-packages\\pandas\\core\\apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 891\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    893\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\ginkgo\\Lib\\site-packages\\pandas\\core\\apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    905\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    906\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 907\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[0;32m    908\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    909\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    910\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    911\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\\db_orm.py:86\u001b[0m, in \u001b[0;36mget_from_queue.<locals>.row_to_dict\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrow_to_dict\u001b[39m(row):\n\u001b[1;32m---> 86\u001b[0m     result \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mquery(Sources)\u001b[39m.\u001b[39;49mfilter_by(\n\u001b[0;32m     87\u001b[0m         title\u001b[39m=\u001b[39;49mrow[\u001b[39m'\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     88\u001b[0m         section\u001b[39m=\u001b[39;49mrow[\u001b[39m'\u001b[39;49m\u001b[39msection\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     89\u001b[0m     )\u001b[39m.\u001b[39;49mlimit(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mall()[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m     91\u001b[0m     sources_series \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries({column\u001b[39m.\u001b[39mname: \u001b[39mgetattr\u001b[39m(result, column\u001b[39m.\u001b[39mname) \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m result\u001b[39m.\u001b[39m__table__\u001b[39m.\u001b[39mcolumns})\n\u001b[0;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m sources_series\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP, Numeric, Boolean\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "## This script calls upon functions and magic functions in db_session.py\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class GPT_queue(Base):\n",
    "    __tablename__ = 'gpt_queue'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    body = mapped_column(Text)\n",
    "    section = mapped_column(String(100))\n",
    "    sent_to_sources = mapped_column(Boolean)\n",
    "    publication = mapped_column(String(100))\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    section = mapped_column(String(100))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    full_template = mapped_column(Text)\n",
    "    system_role = mapped_column(String(300))\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    rating_original_content = mapped_column(Integer) \n",
    "    simple_summary = mapped_column(Text)\n",
    "    rating_simple_content = mapped_column(Integer) \n",
    "    original_headline = mapped_column(String(255))\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    model = mapped_column(String(70))\n",
    "    temperature = mapped_column(Numeric)\n",
    "\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_from_queue(session, input_df, order_by='id', order='ASC'):\n",
    "    \"\"\"\n",
    "    Return the matching records from the sources table as a pandas dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - input_df: A pandas DataFrame with the article records from the gpt_queue table or equivalent. Columns include 'title' and 'section'.\n",
    "    - limit: The number of records to return.\n",
    "    \"\"\"\n",
    "    def row_to_dict(row):\n",
    "        result = session.query(Sources).filter_by(\n",
    "            title=row['title'],\n",
    "            section=row['section']\n",
    "        ).limit(1).all()[0]\n",
    "        \n",
    "        sources_series = pd.Series({column.name: getattr(result, column.name) for column in result.__table__.columns})\n",
    "        return sources_series\n",
    "\n",
    "    sources_df = input_df.apply(row_to_dict, axis=1)\n",
    "    ascending = True if order == 'ASC' else False\n",
    "    sources_df.sort_values(order_by, ascending=ascending, inplace=True)\n",
    "    return sources_df\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=None, order_by='id', order='ASC'):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if order_by:\n",
    "        query_statement += f' ORDER BY {order_by} {order}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries'):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    with session.no_autoflush:\n",
    "                        existing_record = session.query(Sources).filter_by(\n",
    "                            title=row['title'],\n",
    "                            doi=row['doi'],\n",
    "                            section=row['section']\n",
    "                        ).first()\n",
    "                        if not existing_record:\n",
    "                            data = Sources(\n",
    "                                title=row['title'],\n",
    "                                text=row['text'],\n",
    "                                abstract=row['abstract'],\n",
    "                                publication=row['publication'],\n",
    "                                authors=row['authors'],\n",
    "                                year=row['year'],\n",
    "                                month=row['month'],\n",
    "                                pub_volume=row['pub_volume'],\n",
    "                                pub_issue=row['pub_issue'],\n",
    "                                start_page=row['start_page'],\n",
    "                                end_page=row['end_page'],\n",
    "                                doi=row['doi'],\n",
    "                                section=row['section'] \n",
    "                            )\n",
    "                            session.add(data)\n",
    "                            print(f'\\t{row[\"title\"]}')\n",
    "                        else:\n",
    "                            print(f'\\t** Already exists in the database: {row[\"title\"]}.')\n",
    "                elif table == 'gpt_queue':\n",
    "                    data = GPT_queue(\n",
    "                        title=row['title'],\n",
    "                        body=row['body'],\n",
    "                        section=row['section'],\n",
    "                        sent_to_sources=row['sent_to_sources'],\n",
    "                        publication=row['publication']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                    print(f'\\t{row[\"title\"]}')\n",
    "                elif table == 'summaries':\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full_summarize_task'],\n",
    "                        system_role=row['system_role'],\n",
    "                    ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['summarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task'],\n",
    "                            system_role=row['system_role']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice'],\n",
    "                        model=row['model'],\n",
    "                        temperature=row['temperature']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                    print(f'\\tReference #{row[\"reference_id\"]}: {row[\"headline\"]}')\n",
    "                elif table == 'feed':\n",
    "                    source = session.query(Feed).filter_by(\n",
    "                        title=row['title'],\n",
    "                        journal=row['journal'],\n",
    "                        doi=row['doi']\n",
    "                    ).first()\n",
    "                    if source:\n",
    "                        print(f'\\tAlready exists in the database: {row[\"title\"]}.')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"New records added successfully (if applicable)!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "from functools import wraps\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\private\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from prompts import * # .py file stored in the path above\n",
    "from db_orm import * \n",
    "from sources import *\n",
    "from orm_summarize import *\n",
    "from article_processing import *\n",
    "\n",
    "#########\n",
    "#########\n",
    "# Prep: Set parameters\n",
    "folder_path = '../text/2023-05-03 5'\n",
    "section = 'Sleep disturbance and obesity risk'\n",
    "local = False\n",
    "n_choices = 1\n",
    "article_limit = 1\n",
    "temperature = 1\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "iteration_id = 1\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "# model = 'gpt-4'\n",
    "save_outputs=False\n",
    "\n",
    "def generate_summaries(n_choices, temperature, model, pause_per_request, folder_path, section, local, article_limit=article_limit):\n",
    "    ### Set up\n",
    "    qna_dict = dict()\n",
    "    chatbot_dict = dict()\n",
    "    references_df_dict = dict()\n",
    "\n",
    "    # set the option to wrap text within cells\n",
    "    pd.set_option('display.max_colwidth', 50)\n",
    "    pd.set_option('display.max_rows', 20)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "\n",
    "    ####### \n",
    "    # Step 1: Create sources table\n",
    "    if local:\n",
    "        text_df = parse_fulltext(folder_path, section).iloc[:article_limit if article_limit else len(text_df)]\n",
    "    else:\n",
    "        text_df = get_table(table='gpt_queue', limit=article_limit, order='DESC') # db_orm.py\n",
    "    references_df_dict[iteration_id] = create_sources_table(text_df) # sources.py\n",
    "    # return references_df_dict[iteration_id]\n",
    "\n",
    "    ###### \n",
    "    # Step 2:  Add rows from gpt_queue table to sources table \n",
    "    bulk_append(table='sources', input_df=references_df_dict[iteration_id]) # db_orm.py\n",
    "\n",
    "    # ##### \n",
    "    # Step 3: Get the new sources for summarization\n",
    "    sources_df = get_from_queue(input_df=text_df, order_by='id', order='ASC')\n",
    "\n",
    "    # ##### \n",
    "    # Step 4: Create summaries (functions contained in orm_summarize.py)\n",
    "    chatbot_dict = batch_summarize( # orm_summarize.py\n",
    "        sources_df, folder_path, prep_step, summarize_task, edit_task,  # parameter values found in prompts.py\n",
    "        simplify_task, simplify_audience, format_task,\n",
    "        chatbot_dict, temperature=temperature,\n",
    "        system_role=system_role, model=model, max_tokens=1000,\n",
    "        n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "        iteration_id=iteration_id, save_outputs=save_outputs\n",
    "        )\n",
    "    #########\n",
    "    # Step 5: Create summaries table\n",
    "    qna_dict = create_summaries_df(\n",
    "        qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "        )\n",
    "\n",
    "    ##########\n",
    "    # Step 5: Add results to summaries and prompts table \n",
    "    bulk_append(table='summaries', input_df=qna_dict[iteration_id]) # db_orm.py\n",
    "\n",
    "    return qna_dict[iteration_id]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    qna_dict = generate_summaries(n_choices, temperature, model, pause_per_request, folder_path, section, local=local, article_limit=article_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>sent_to_sources</th>\n",
       "      <th>section</th>\n",
       "      <th>publication</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publication</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>pub_volume</th>\n",
       "      <th>pub_issue</th>\n",
       "      <th>start_page</th>\n",
       "      <th>end_page</th>\n",
       "      <th>doi</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>Sleep and obesity</td>\n",
       "      <td>Sleep and Obesity\\r\\n\\r\\nObstructive sleep apn...</td>\n",
       "      <td>None</td>\n",
       "      <td>Sleep disturbance and obesity risk</td>\n",
       "      <td>Curr Opin Clin Nutr Metab Care</td>\n",
       "      <td>PURPOSE OF REVIEW: This review summarizes the ...</td>\n",
       "      <td>Current opinion in clinical nutrition and meta...</td>\n",
       "      <td>Guglielmo Beccuti, Silvana Pannain</td>\n",
       "      <td>2011</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>402</td>\n",
       "      <td>412</td>\n",
       "      <td>10.1097/MCO.0b013e3283479109</td>\n",
       "      <td>Sleep and Obesity\\r\\n\\r\\nObstructive sleep apn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              title                                               body  \\\n",
       "0  87  Sleep and obesity  Sleep and Obesity\\r\\n\\r\\nObstructive sleep apn...   \n",
       "\n",
       "  sent_to_sources                             section  \\\n",
       "0            None  Sleep disturbance and obesity risk   \n",
       "\n",
       "                      publication  \\\n",
       "0  Curr Opin Clin Nutr Metab Care   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  PURPOSE OF REVIEW: This review summarizes the ...   \n",
       "\n",
       "                                         publication  \\\n",
       "0  Current opinion in clinical nutrition and meta...   \n",
       "\n",
       "                              authors  year month pub_volume pub_issue  \\\n",
       "0  Guglielmo Beccuti, Silvana Pannain  2011               14         4   \n",
       "\n",
       "  start_page end_page                           doi  \\\n",
       "0        402      412  10.1097/MCO.0b013e3283479109   \n",
       "\n",
       "                                                text  \n",
       "0  Sleep and Obesity\\r\\n\\r\\nObstructive sleep apn...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_dict[iteration_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87]\n",
      "['Sleep and obesity']\n",
      "['Sleep and Obesity\\r\\n\\r\\nObstructive sleep apnea\\r\\n\\r\\nSleep loss occurs not only as a result of habitual behavior, but also in presence of pathological conditions associated with disturbed sleep, like obstructive sleep apnea (OSA). The increase in both the prevalence and the severity of obesity has translated into an increase in the prevalence of obesity-related comorbidities including OSA. The prevalence of OSA in the US adult population has been estimated to be 24% in men and 9% in women [49] but is increased in severe obesity by up to 93.6% among men and 73.5% among women [50].\\r\\n\\r\\nPoor sleep quality and excessive daytime sleepiness in the absence of obstructive sleep apnea\\r\\n\\r\\nSevere obesity appears to be associated with marked sleep disturbances, even in individuals who do not have OSA [72–75]. Such sleep disturbances may equally predispose severely obese individuals to accumulate a sleep debt and may contribute to the dysregulation of appetite, limit the drive for physical activity, and further compromise weight maintenance.\\r\\nCross-sectional studies examining self-reported sleep quality have generally found that worse sleep quality is associated with higher BMI [76–78], but the longitudinal studies have been scarce and the results inconsistent [79,80]. A more recent cross-sectional analysis in 400 women participants in the Sleep and Health in Women Study showed that not only sleep duration but also sleep quality, as determined by sleep efficiency and sleep architecture (specifically minutes of SWS, the ‘deep restorative sleep’) were inversely related to waist circumference, after adjusting for age, level of physical activity, smoking status, alcohol consumption and AHI [20•]. Such associations were stronger in young women (age <50 years), suggesting that in older age the relationship between sleep quality and obesity may be less robust. The relationship between less time spent in SWS and central obesity could be mediated by a decrease in growth hormone level, which is secreted during SWS. Growth hormone deficiency is associated with visceral obesity, which can be reversed by growth hormone replacement [81,82]. Furthermore, a reduced amount of SWS leads to elevated cortisol levels, which favor central obesity [83].\\r\\nIn the Cardiovascular Health Epidemiology Study, which focused exclusively on African–Americans, an impressive 50% or more of the participants surveyed reported suboptimal sleep duration and low-sleep quality as assessed by the Pittsburgh Sleep Quality scale [46]. In a multivariate analysis model, the effect of sleep duration on obesity risk was rather modest, with a significant association between lower sleep quality and increased BMI in women only, and this association was modulated by perceived stress level as measured by the Cohen scale.\\r\\nA sex difference in the association between poor-sleep quality and obesity risk has been confirmed in a longitudinal study of 7000 Finnish adults, aged 40–60 years. In women, sleep problems (difficulty initiating and maintaining sleep) predicted weight gain after 5–7 years. Moreover, there was a graded effect depending on the frequency of the sleep disturbances [47•].\\r\\nNordin and Kaplan [48] examined the effect of sleep discontinuity on the development of obesity over a 30-year period in approximately 7000 middle-age adults. Sleep quality was self-reported and assessed by the question ‘how often do you have any trouble getting to sleep or staying asleep’? On the basis of the answers compared from the first and last observation, the participants were divided into four categories of sleep continuity, consistently good continuity, consistent discontinuity, impaired continuity (worsened over time), and improved continuity. The main outcome was the risk of transition to and from obesity. Consistent sleep discontinuity was associated with 70% risk of conversion to obesity after adjusting for confounding variables related to demographics, pain, lifestyle, and health including sleep duration. Both consistent sleep discontinuity and impaired sleep continuity reduced the chance for transitioning from obesity, thus increasing the risk of staying obese. The major limitation of the study was that sleep quality and anthropometrics were self-reported and the analysis could not control the presence of a sleep disorder like OSA, which could in part account for the risk of weight gain.']\n",
      "[None]\n",
      "['Sleep disturbance and obesity risk']\n",
      "[['Curr Opin Clin Nutr Metab Care'\n",
      "  'Current opinion in clinical nutrition and metabolic care']]\n",
      "['PURPOSE OF REVIEW: This review summarizes the most recent evidence linking decreased sleep duration and poor sleep quality to obesity, focusing upon studies in adults.<br>RECENT FINDINGS: Published and unpublished health examination surveys and epidemiological studies suggest that the worldwide prevalence of obesity has doubled since 1980. In 2008, 1 in 10 adults was obese, with women more likely to be obese than men. This obesity epidemic has been paralleled by a trend of reduced sleep duration. Poor sleep quality, which leads to overall sleep loss has also become a frequent complaint. Growing evidence from both laboratory and epidemiological studies points to short sleep duration and poor sleep quality as new risk factors for the development of obesity.<br>SUMMARY: Sleep is an important modulator of neuroendocrine function and glucose metabolism and sleep loss has been shown to result in metabolic and endocrine alterations, including decreased glucose tolerance, decreased insulin sensitivity, increased evening concentrations of cortisol, increased levels of ghrelin, decreased levels of leptin, and increased hunger and appetite. Recent epidemiological and laboratory evidence confirm previous findings of an association between sleep loss and increased risk of obesity.<br>']\n",
      "[['Curr Opin Clin Nutr Metab Care'\n",
      "  'Current opinion in clinical nutrition and metabolic care']]\n",
      "['Guglielmo Beccuti, Silvana Pannain']\n",
      "['2011']\n",
      "['']\n",
      "['14']\n",
      "['4']\n",
      "['402']\n",
      "['412']\n",
      "['10.1097/MCO.0b013e3283479109']\n",
      "['Sleep and Obesity\\r\\n\\r\\nObstructive sleep apnea\\r\\n\\r\\nSleep loss occurs not only as a result of habitual behavior, but also in presence of pathological conditions associated with disturbed sleep, like obstructive sleep apnea (OSA). The increase in both the prevalence and the severity of obesity has translated into an increase in the prevalence of obesity-related comorbidities including OSA. The prevalence of OSA in the US adult population has been estimated to be 24% in men and 9% in women [49] but is increased in severe obesity by up to 93.6% among men and 73.5% among women [50].\\r\\n\\r\\nPoor sleep quality and excessive daytime sleepiness in the absence of obstructive sleep apnea\\r\\n\\r\\nSevere obesity appears to be associated with marked sleep disturbances, even in individuals who do not have OSA [72–75]. Such sleep disturbances may equally predispose severely obese individuals to accumulate a sleep debt and may contribute to the dysregulation of appetite, limit the drive for physical activity, and further compromise weight maintenance.\\r\\nCross-sectional studies examining self-reported sleep quality have generally found that worse sleep quality is associated with higher BMI [76–78], but the longitudinal studies have been scarce and the results inconsistent [79,80]. A more recent cross-sectional analysis in 400 women participants in the Sleep and Health in Women Study showed that not only sleep duration but also sleep quality, as determined by sleep efficiency and sleep architecture (specifically minutes of SWS, the ‘deep restorative sleep’) were inversely related to waist circumference, after adjusting for age, level of physical activity, smoking status, alcohol consumption and AHI [20•]. Such associations were stronger in young women (age <50 years), suggesting that in older age the relationship between sleep quality and obesity may be less robust. The relationship between less time spent in SWS and central obesity could be mediated by a decrease in growth hormone level, which is secreted during SWS. Growth hormone deficiency is associated with visceral obesity, which can be reversed by growth hormone replacement [81,82]. Furthermore, a reduced amount of SWS leads to elevated cortisol levels, which favor central obesity [83].\\r\\nIn the Cardiovascular Health Epidemiology Study, which focused exclusively on African–Americans, an impressive 50% or more of the participants surveyed reported suboptimal sleep duration and low-sleep quality as assessed by the Pittsburgh Sleep Quality scale [46]. In a multivariate analysis model, the effect of sleep duration on obesity risk was rather modest, with a significant association between lower sleep quality and increased BMI in women only, and this association was modulated by perceived stress level as measured by the Cohen scale.\\r\\nA sex difference in the association between poor-sleep quality and obesity risk has been confirmed in a longitudinal study of 7000 Finnish adults, aged 40–60 years. In women, sleep problems (difficulty initiating and maintaining sleep) predicted weight gain after 5–7 years. Moreover, there was a graded effect depending on the frequency of the sleep disturbances [47•].\\r\\nNordin and Kaplan [48] examined the effect of sleep discontinuity on the development of obesity over a 30-year period in approximately 7000 middle-age adults. Sleep quality was self-reported and assessed by the question ‘how often do you have any trouble getting to sleep or staying asleep’? On the basis of the answers compared from the first and last observation, the participants were divided into four categories of sleep continuity, consistently good continuity, consistent discontinuity, impaired continuity (worsened over time), and improved continuity. The main outcome was the risk of transition to and from obesity. Consistent sleep discontinuity was associated with 70% risk of conversion to obesity after adjusting for confounding variables related to demographics, pain, lifestyle, and health including sleep duration. Both consistent sleep discontinuity and impaired sleep continuity reduced the chance for transitioning from obesity, thus increasing the risk of staying obese. The major limitation of the study was that sleep quality and anthropometrics were self-reported and the analysis could not control the presence of a sleep disorder like OSA, which could in part account for the risk of weight gain.']\n"
     ]
    }
   ],
   "source": [
    "for column in qna_dict.columns:\n",
    "    print(qna_dict[column].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from gpt_queue ORDER BY id DESC LIMIT 1\n",
      "Search term: (Sleep and obesity [ti]) AND (Curr Opin Clin Nutr Metab Care [ta])\n",
      "Number of abstract sections: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['PURPOSE OF REVIEW: This review summarizes the most recent evidence linking decreased sleep duration and poor sleep quality to obesity, focusing upon studies in adults.<br>RECENT FINDINGS: Published and unpublished health examination surveys and epidemiological studies suggest that the worldwide prevalence of obesity has doubled since 1980. In 2008, 1 in 10 adults was obese, with women more likely to be obese than men. This obesity epidemic has been paralleled by a trend of reduced sleep duration. Poor sleep quality, which leads to overall sleep loss has also become a frequent complaint. Growing evidence from both laboratory and epidemiological studies points to short sleep duration and poor sleep quality as new risk factors for the development of obesity.<br>SUMMARY: Sleep is an important modulator of neuroendocrine function and glucose metabolism and sleep loss has been shown to result in metabolic and endocrine alterations, including decreased glucose tolerance, decreased insulin sensitivity, increased evening concentrations of cortisol, increased levels of ghrelin, decreased levels of leptin, and increased hunger and appetite. Recent epidemiological and laboratory evidence confirm previous findings of an association between sleep loss and increased risk of obesity.<br>'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP, Numeric, Boolean\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "## This script calls upon functions and magic functions in db_session.py\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class GPT_queue(Base):\n",
    "    __tablename__ = 'gpt_queue'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    body = mapped_column(Text)\n",
    "    section = mapped_column(String(100))\n",
    "    sent_to_sources = mapped_column(Boolean)\n",
    "    publication = mapped_column(String(100))\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    section = mapped_column(String(100))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    full_template = mapped_column(Text)\n",
    "    system_role = mapped_column(String(300))\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    rating_original_content = mapped_column(Integer) \n",
    "    simple_summary = mapped_column(Text)\n",
    "    rating_simple_content = mapped_column(Integer) \n",
    "    original_headline = mapped_column(String(255))\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    model = mapped_column(String(70))\n",
    "    temperature = mapped_column(Numeric)\n",
    "\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_from_queue(session, input_df, order_by='id', order='ASC'):\n",
    "    \"\"\"\n",
    "    Return the matching records from the sources table as a pandas dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - input_df: A pandas DataFrame with the article records from the gpt_queue table or equivalent. Columns include 'title' and 'section'.\n",
    "    - limit: The number of records to return.\n",
    "    \"\"\"\n",
    "    def row_to_dict(row):\n",
    "        result = session.query(Sources).filter_by(\n",
    "            title=row['title'],\n",
    "            section=row['section']\n",
    "        ).limit(1).all()[0]\n",
    "        \n",
    "        sources_series = pd.Series({column.name: getattr(result, column.name) for column in result.__table__.columns})\n",
    "        return sources_series\n",
    "\n",
    "    sources_df = input_df.apply(row_to_dict, axis=1)\n",
    "    ascending = True if order == 'ASC' else False\n",
    "    sources_df.sort_values(order_by, ascending=ascending, inplace=True)\n",
    "    return sources_df\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=None, order_by='id', order='ASC'):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if order_by:\n",
    "        query_statement += f' ORDER BY {order_by} {order}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries'):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    with session.no_autoflush:\n",
    "                        existing_record = session.query(Sources).filter_by(\n",
    "                            title=row['title'],\n",
    "                            doi=row['doi'],\n",
    "                            section=row['section']\n",
    "                        ).first()\n",
    "                        if not existing_record:\n",
    "                            data = Sources(\n",
    "                                title=row['title'],\n",
    "                                text=row['text'],\n",
    "                                abstract=row['abstract'],\n",
    "                                publication=row['journal'],\n",
    "                                authors=row['authors'],\n",
    "                                year=row['year'],\n",
    "                                month=row['month'],\n",
    "                                pub_volume=row['pub_volume'],\n",
    "                                pub_issue=row['pub_issue'],\n",
    "                                start_page=row['start_page'],\n",
    "                                end_page=row['end_page'],\n",
    "                                doi=row['doi'],\n",
    "                                section=row['section'] \n",
    "                            )\n",
    "                            session.add(data)\n",
    "                            print(f'\\t{row[\"title\"]}')\n",
    "                        else:\n",
    "                            print(f'\\t** Already exists in the database: {row[\"title\"]}.')\n",
    "                elif table == 'gpt_queue':\n",
    "                    data = GPT_queue(\n",
    "                        title=row['title'],\n",
    "                        body=row['body'],\n",
    "                        section=row['section'],\n",
    "                        sent_to_sources=row['sent_to_sources'],\n",
    "                        publication=row['journal']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                    print(f'\\t{row[\"title\"]}')\n",
    "                elif table == 'summaries':\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full_summarize_task'],\n",
    "                        system_role=row['system_role'],\n",
    "                    ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['summarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task'],\n",
    "                            system_role=row['system_role']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice'],\n",
    "                        model=row['model'],\n",
    "                        temperature=row['temperature']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                    print(f'\\tReference #{row[\"reference_id\"]}: {row[\"headline\"]}')\n",
    "                elif table == 'feed':\n",
    "                    source = session.query(Feed).filter_by(\n",
    "                        title=row['title'],\n",
    "                        journal=row['journal'],\n",
    "                        doi=row['doi']\n",
    "                    ).first()\n",
    "                    if source:\n",
    "                        print(f'\\tAlready exists in the database: {row[\"title\"]}.')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"New records added successfully (if applicable)!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "from functools import wraps\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\private\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from prompts import * # .py file stored in the path above\n",
    "from db_orm import * \n",
    "from sources import *\n",
    "from orm_summarize import *\n",
    "from article_processing import *\n",
    "\n",
    "#########\n",
    "#########\n",
    "# Prep: Set parameters\n",
    "folder_path = '../text/2023-05-03 5'\n",
    "section = 'Sleep disturbance and obesity risk'\n",
    "local = False\n",
    "n_choices = 1\n",
    "article_limit = 1\n",
    "temperature = 1\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "iteration_id = 1\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "# model = 'gpt-4'\n",
    "save_outputs=False\n",
    "\n",
    "def generate_summaries(n_choices, temperature, model, pause_per_request, folder_path, section, local, article_limit=article_limit):\n",
    "    ### Set up\n",
    "    qna_dict = dict()\n",
    "    chatbot_dict = dict()\n",
    "    references_df_dict = dict()\n",
    "\n",
    "    # set the option to wrap text within cells\n",
    "    pd.set_option('display.max_colwidth', 50)\n",
    "    pd.set_option('display.max_rows', 20)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "\n",
    "    ####### \n",
    "    # Step 1: Create sources table\n",
    "    if local:\n",
    "        text_df = parse_fulltext(folder_path, section).iloc[:article_limit if article_limit else len(text_df)]\n",
    "    else:\n",
    "        text_df = get_table(table='gpt_queue', limit=article_limit, order='DESC') # db_orm.py\n",
    "    references_df_dict[iteration_id] = create_sources_table(text_df) # sources.py\n",
    "    # return references_df_dict[iteration_id]\n",
    "\n",
    "    ###### \n",
    "    # Step 2:  Add rows from gpt_queue table to sources table \n",
    "    bulk_append(table='sources', input_df=references_df_dict[iteration_id]) # db_orm.py\n",
    "\n",
    "    # ##### \n",
    "    # Step 3: Get the new sources for summarization\n",
    "    sources_df = get_from_queue(input_df=text_df, order_by='id', order='ASC')\n",
    "\n",
    "    # ##### \n",
    "    # Step 4: Create summaries (functions contained in orm_summarize.py)\n",
    "    chatbot_dict = batch_summarize( # orm_summarize.py\n",
    "        sources_df, folder_path, prep_step, summarize_task, edit_task,  # parameter values found in prompts.py\n",
    "        simplify_task, simplify_audience, format_task,\n",
    "        chatbot_dict, temperature=temperature,\n",
    "        system_role=system_role, model=model, max_tokens=1000,\n",
    "        n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "        iteration_id=iteration_id, save_outputs=save_outputs\n",
    "        )\n",
    "    #########\n",
    "    # Step 5: Create summaries table\n",
    "    qna_dict = create_summaries_df(\n",
    "        qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "        )\n",
    "\n",
    "    ##########\n",
    "    # Step 5: Add results to summaries and prompts table \n",
    "    bulk_append(table='summaries', input_df=qna_dict[iteration_id]) # db_orm.py\n",
    "\n",
    "    return qna_dict[iteration_id]\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import requests\n",
    "from article_processing import create_text_dict_from_folder\n",
    "from orm_summarize import *\n",
    "api_key = os.getenv('api_ncbi') # Pubmed API key\n",
    "\n",
    "### These scripts populate data in the sources table with data from the Pubmed API.\n",
    "\n",
    "def search_article(title, publication, api_key, verbose=False):\n",
    "    \"\"\"\n",
    "    Search for article title in PubMed database.\n",
    "\n",
    "    Parameters:\n",
    "    - title (str): article title\n",
    "    - api_key (str): NCBI API key\n",
    "\n",
    "    Returns:\n",
    "    response (str): Article metadata from PubMed database if present. Otherwise, returns list of PMIDs.\n",
    "    \"\"\"\n",
    "    base_url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
    "    title_without_not = re.sub(r'not', '', title)\n",
    "    if api_key:\n",
    "        base_url += f'&api_key={api_key}'\n",
    "    if publication:\n",
    "        search_term = f'({title_without_not} [ti]) AND ({publication} [ta])'\n",
    "    else:\n",
    "        search_term = f'{title_without_not} [ti]'\n",
    "    params = {\n",
    "        'db': 'pubmed',\n",
    "        'term': search_term,\n",
    "        'retmax': 5,\n",
    "        'retmode': 'json'\n",
    "    }\n",
    "    print(f'Search term: {search_term}')\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    cleaned_title = re.sub(r'</?[ib]>', '', title) # remove bold and italic html tags\n",
    "    cleaned_title = re.sub(r'[^a-zA-Z0-9 ]', '', cleaned_title).lower().strip()\n",
    "    cleaned_title = re.sub(r\"\\u2010\", '', cleaned_title)\n",
    "    # print(f'Data keys: {data.keys()}')\n",
    "    try:\n",
    "        id_list = data['esearchresult']['idlist']\n",
    "        if id_list:\n",
    "            for index in range(len(id_list)):\n",
    "                result = retrieve_citation(id_list[index], api_key).decode('utf-8')\n",
    "                cleaned_result = re.sub(r'[^a-zA-Z0-9 <>/]', '', result).lower().strip() \n",
    "                result = retrieve_citation(id_list[index], api_key).decode('utf-8')\n",
    "                result_title_match = re.search(r'<articletitle>(.*?)</articletitle>', cleaned_result)\n",
    "                if result_title_match:\n",
    "                    result_title = result_title_match.group(1)\n",
    "                    cleaned_result_title = re.sub(r'</?[ib]>', '', result_title)\n",
    "                    cleaned_result_title = re.sub(r'/(?![^<>]*>)', '', cleaned_result_title) # Remove any / that is not within html tag\n",
    "                    cleaned_result_title = re.sub(r'[^a-zA-Z0-9 <>/]', '', cleaned_result_title).lower().strip()\n",
    "                else:\n",
    "                    cleaned_result_title = cleaned_result\n",
    "                if cleaned_title == cleaned_result_title:\n",
    "                    if verbose:\n",
    "                        print(f'Match found for {title}: PMID = {article_id}.')\n",
    "                        return result\n",
    "                else:\n",
    "                    continue\n",
    "            if cleaned_title != cleaned_result_title:\n",
    "                print(f'Warning: Article title not found in PMIDs.')\n",
    "                print(f'Check these PMIDs: {id_list}')\n",
    "                print(f'\\tInput title: {title.lower().strip()}')\n",
    "                print(f'\\tResult title: {result_title if result_title else cleaned_result}')\n",
    "                print(f'\\tCleaned input title: {cleaned_title}')\n",
    "                print(f'\\tCleaned result title: {cleaned_result_title}\\n')\n",
    "            return result     \n",
    "    except Exception as error: \n",
    "        exc_type, exc_obj, tb = sys.exc_info()\n",
    "        file = tb.tb_frame\n",
    "        lineno = tb.tb_lineno\n",
    "        filename = file.f_code.co_filename\n",
    "        print(f'\\tAn error occurred on line {lineno} in {filename}: {error}')    \n",
    "        print('Article not found.')\n",
    "        print(f'\\tInput title: {title.lower().strip()}')\n",
    "        return ''.join([id for id in id_list]) \n",
    "    \n",
    "def retrieve_citation(article_id, api_key):\n",
    "    \"\"\"\n",
    "    Retrieve article metadata from PubMed database.\n",
    "    \"\"\"\n",
    "    base_url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "    if api_key:\n",
    "        base_url += f'&api_key={api_key}'\n",
    "    params = {\n",
    "        'db': 'pubmed',\n",
    "        'id': article_id\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    return response.content\n",
    "\n",
    "def extract_pubmed_details(record_string):\n",
    "    \"\"\"\n",
    "    Helper function called by `pubmed_details_by_title` to parse article metadata from PubMed database.\n",
    "    \"\"\"\n",
    "    authors = re.findall(r'<Author ValidYN=\"Y\".*?><LastName>(.*?)</LastName><ForeName>(.*?)</ForeName>', record_string)\n",
    "    formatted_authors = ', '.join(['{} {}'.format(author[1], author[0]) for author in authors])\n",
    "\n",
    "    # Extract publication year\n",
    "    publication_year = re.search(r'<PubDate><Year>(\\d{4})</Year>', record_string)\n",
    "    publication_year = publication_year.group(1) if publication_year else ''\n",
    "    publication_month = re.search(r'<PubDate>.*?<Month>(Aug)</Month>.*?</PubDate>', record_string)\n",
    "    publication_month = publication_month.group(1) if publication_month else ''\n",
    "\n",
    "    # Extract article title\n",
    "    article_title = re.search(r'<ArticleTitle>(.*?)</ArticleTitle>', record_string)\n",
    "    article_title = article_title.group(1) if article_title else ''\n",
    "\n",
    "    # Extract journal title\n",
    "    journal_title = re.search(r'<Title>(.*?)</Title>', record_string)\n",
    "    journal_title = journal_title.group(1) if journal_title else ''\n",
    "\n",
    "    # Extract journal volume\n",
    "    journal_volume = re.search(r'<Volume>(.*?)</Volume>', record_string)\n",
    "    journal_volume = journal_volume.group(1) if journal_volume else ''\n",
    "\n",
    "    # Extract journal issue\n",
    "    journal_issue = re.search(r'<Issue>(.*?)</Issue>', record_string)\n",
    "    journal_issue = journal_issue.group(1) if journal_issue else ''\n",
    "\n",
    "    # Extract start page\n",
    "    start_page = re.search(r'<StartPage>(.*?)</StartPage>', record_string)\n",
    "    start_page = start_page.group(1) if start_page else ''\n",
    "\n",
    "    # Extract end page\n",
    "    end_page = re.search(r'<EndPage>(.*?)</EndPage>', record_string)\n",
    "    end_page = end_page.group(1) if end_page else ''\n",
    "\n",
    "    # Extract ELocationID\n",
    "    doi = re.search(r'<ELocationID.*?EIdType=\"doi\".*?>(.*?)</ELocationID>', record_string)\n",
    "    doi = doi.group(1) if doi else ''\n",
    "\n",
    "    abstract_matches = re.findall(r'(<AbstractText.*?>.*?</AbstractText>)', record_string)\n",
    "    print(f'Number of abstract sections: {len(abstract_matches)}')\n",
    "    if len(abstract_matches) > 1:\n",
    "        cleaned_abstract_sections = []\n",
    "        for match in abstract_matches:\n",
    "            # clean_match = re.sub(r'<AbstractText(.*?>.*)</AbstractText>', r'\\1', match)\n",
    "            clean_match = re.sub(r'<AbstractText.*?((?:Label=\".*\")?.*?>.*)</AbstractText>', r'\\1', match)\n",
    "            clean_match = re.sub(r'(?: Label=\"(.*?)\")?.*?>(.*)', r'\\1: \\2', clean_match)\n",
    "            cleaned_abstract_sections.append(clean_match)\n",
    "            \n",
    "        abstract = ''.join([f'{group}<br>' for group in cleaned_abstract_sections])\n",
    "    else:\n",
    "        abstract = re.sub(r'<AbstractText.*?>(.*?)</AbstractText>', r'\\1', abstract_matches[0])  if abstract_matches else ''\n",
    "\n",
    "    # abstract = 'abstract'\n",
    "\n",
    "    return {\n",
    "        'pubmed_title': article_title,\n",
    "        'abstract': abstract,\n",
    "        'journal': journal_title,\n",
    "        'authors': formatted_authors,\n",
    "        'year': publication_year,\n",
    "        'month': publication_month,\n",
    "        'pub_volume': journal_volume,\n",
    "        'pub_issue': journal_issue,\n",
    "        'start_page': start_page,\n",
    "        'end_page': end_page,\n",
    "        'doi': doi,\n",
    "    }\n",
    "\n",
    "\n",
    "def pubmed_details_by_title(title, publication, api_key):\n",
    "    \"\"\"\n",
    "    Search for article title in PubMed database and return article details.\n",
    "\n",
    "    Parameters:\n",
    "    - title (str): article title\n",
    "    - api_key (str): NCBI API key\n",
    "\n",
    "    Returns:\n",
    "    article_details (dict): Article metadata from PubMed database if present. Otherwise, returns list of PMIDs.\n",
    "    \"\"\"\n",
    "    record_string = search_article(title, publication, api_key)\n",
    "    # return record_string\n",
    "    if record_string:\n",
    "        article_details = extract_pubmed_details(record_string)\n",
    "        return article_details\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def add_pubmed_details(text_df, api_key):\n",
    "    \"\"\"\n",
    "    Add the article metadata to a DataFrame containing article title and text.\n",
    "\n",
    "    Parameters:\n",
    "    - text_df (pd.DataFrame): DataFrame containing article title and text.\n",
    "    - api_key (str): NCBI API key\n",
    "\n",
    "    Returns:\n",
    "    DataFrame with added PubMed details for each article.\n",
    "    \"\"\"\n",
    "    article_details_list = []\n",
    "    for index in text_df.index:\n",
    "        article = text_df.loc[index, 'title']\n",
    "        text = str(text_df.loc[index, 'body'])\n",
    "        publication = text_df.loc[index, 'publication']\n",
    "        article_details = pubmed_details_by_title(article, publication, api_key)\n",
    "        if article_details:\n",
    "            article_details['text'] = text\n",
    "            article_details_list.append(article_details)\n",
    "        else:\n",
    "            article_details_list.append({\n",
    "                'pubmed_title': article,\n",
    "                'abstract': '',\n",
    "                'journal': publication,\n",
    "                'authors': '',\n",
    "                'year': '',\n",
    "                'month': '',\n",
    "                'pub_volume': '',\n",
    "                'pub_issue': '',\n",
    "                'start_page': '',\n",
    "                'end_page': '',\n",
    "                'doi': '',\n",
    "                'text': text,\n",
    "                'section': section\n",
    "            })\n",
    "    article_details_df = pd.DataFrame(article_details_list)\n",
    "    return pd.concat([text_df.reset_index(drop=True), article_details_df], axis=1)\n",
    "\n",
    "def compare_columns(df, col1='title', col2='pubmed_title'):\n",
    "    \"\"\"\n",
    "    Compare two columns in a DataFrame. Drop the second column if the two columns are identical.\n",
    "    Otherwise, return the dataframe with new column with the comparison results, \n",
    "    where `True` indicates a mismatch.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the two columns to be compared.\n",
    "    - col1 (str): Name of the first column to be compared.\n",
    "    - col2 (str): Name of the second column to be compared.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame with added column containing the comparison results.\n",
    "    \"\"\"\n",
    "    # Remove punctuation and special characters\n",
    "    remove_punct = lambda text: re.sub(f'[{string.punctuation}]', '', text)\n",
    "    col1 = df[col1].apply(remove_punct)\n",
    "    col2 = df[col2].apply(remove_punct)\n",
    "\n",
    "    # Convert to lowercase and remove white spaces\n",
    "    clean_text = lambda text: text.lower().strip()\n",
    "    col1 = col1.apply(clean_text)\n",
    "    col2 = col2.apply(clean_text)\n",
    "\n",
    "    # Perform the comparison\n",
    "    comparison = col1 != col2\n",
    "    if sum(comparison) == 0:\n",
    "        df = df.drop(columns=['pubmed_title'])\n",
    "    else:\n",
    "        df['flag_title'] = comparison\n",
    "        flagged_indices = df[df['flag_title'] == True].index\n",
    "        for index in flagged_indices:\n",
    "            print(f'Flagged: ')\n",
    "            print(f'\\tArticle title: {df.loc[index, \"title\"]}')\n",
    "            print(f'\\tPubMed title: {df.loc[index, \"pubmed_title\"]}')\n",
    "            print()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_sources_table(text_df, col1='title', col2='pubmed_title'):\n",
    "    references_df = add_pubmed_details(text_df, api_key)\n",
    "\n",
    "    references_df = compare_columns(references_df, col1=col1, col2=col2)\n",
    "    return references_df\n",
    "\n",
    "\n",
    "iteration_id = 2\n",
    "text_df = get_table(table='gpt_queue', limit=article_limit, order='DESC') # db_orm.py\n",
    "\n",
    "references_df_dict[iteration_id] = create_sources_table(text_df) # sources.py\n",
    "references_df_dict[iteration_id]['abstract'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>sent_to_sources</th>\n",
       "      <th>section</th>\n",
       "      <th>publication</th>\n",
       "      <th>abstract</th>\n",
       "      <th>journal</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>pub_volume</th>\n",
       "      <th>pub_issue</th>\n",
       "      <th>start_page</th>\n",
       "      <th>end_page</th>\n",
       "      <th>doi</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>Sleep and obesity</td>\n",
       "      <td>Sleep and Obesity\\r\\n\\r\\nObstructive sleep apn...</td>\n",
       "      <td>None</td>\n",
       "      <td>Sleep disturbance and obesity risk</td>\n",
       "      <td>Curr Opin Clin Nutr Metab Care</td>\n",
       "      <td>PURPOSE OF REVIEW: This review summarizes the ...</td>\n",
       "      <td>Current opinion in clinical nutrition and meta...</td>\n",
       "      <td>Guglielmo Beccuti, Silvana Pannain</td>\n",
       "      <td>2011</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>402</td>\n",
       "      <td>412</td>\n",
       "      <td>10.1097/MCO.0b013e3283479109</td>\n",
       "      <td>Sleep and Obesity\\r\\n\\r\\nObstructive sleep apn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              title                                               body  \\\n",
       "0  87  Sleep and obesity  Sleep and Obesity\\r\\n\\r\\nObstructive sleep apn...   \n",
       "\n",
       "  sent_to_sources                             section  \\\n",
       "0            None  Sleep disturbance and obesity risk   \n",
       "\n",
       "                      publication  \\\n",
       "0  Curr Opin Clin Nutr Metab Care   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  PURPOSE OF REVIEW: This review summarizes the ...   \n",
       "\n",
       "                                             journal  \\\n",
       "0  Current opinion in clinical nutrition and meta...   \n",
       "\n",
       "                              authors  year month pub_volume pub_issue  \\\n",
       "0  Guglielmo Beccuti, Silvana Pannain  2011               14         4   \n",
       "\n",
       "  start_page end_page                           doi  \\\n",
       "0        402      412  10.1097/MCO.0b013e3283479109   \n",
       "\n",
       "                                                text  \n",
       "0  Sleep and Obesity\\r\\n\\r\\nObstructive sleep apn...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references_df_dict[iteration_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from gpt_queue ORDER BY id DESC LIMIT 1\n",
      "Search term: (Sleep and obesity [ti]) AND (Curr Opin Clin Nutr Metab Care [ta])\n",
      "Number of abstract sections: 3\n",
      "Adding 1 rows to the database...\n",
      "\t** Already exists in the database: Sleep and obesity.\n",
      "New records added successfully (if applicable)!\n",
      "**Text #202 prompt #1 of 1**\n",
      "Creating Chaining class instance\n",
      "***OpenAI model: gpt-3.5-turbo-16k-0613\n",
      "Chaining class instance created\n",
      "\tDone creating prompt\n",
      "\tSending request to gpt-3.5-turbo-16k-0613\n",
      "\t\tRequesting 1 choices using gpt-3.5-turbo-16k-0613\n",
      "\tDone sending request to GPT-3\n",
      "\t...Completed\n",
      "Processing 202_prompt00...\n",
      "Original summaries DataFrame shape: (1, 19)\n",
      "\tOriginal summaries Dataframe columns: Index(['choice', 'timestamp', 'reference_id', 'article_title', 'text',\n",
      "       'system_role', 'model', 'temperature', 'prep_step', 'summarize_task',\n",
      "       'edit_task', 'simplify_task', 'simplify_audience', 'format_task',\n",
      "       'full_summarize_task', 'folder', 'summary', 'headline',\n",
      "       'simple_summary'],\n",
      "      dtype='object')\n",
      "Adding 1 rows to the database...\n",
      "\tReference #202: None\n",
      "New records added successfully (if applicable)!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import requests\n",
    "from article_processing import create_text_dict_from_folder\n",
    "from orm_summarize import *\n",
    "api_key = os.getenv('api_ncbi') # Pubmed API key\n",
    "\n",
    "### These scripts populate data in the sources table with data from the Pubmed API.\n",
    "\n",
    "def search_article(title, publication, api_key, verbose=False):\n",
    "    \"\"\"\n",
    "    Search for article title in PubMed database.\n",
    "\n",
    "    Parameters:\n",
    "    - title (str): article title\n",
    "    - api_key (str): NCBI API key\n",
    "\n",
    "    Returns:\n",
    "    response (str): Article metadata from PubMed database if present. Otherwise, returns list of PMIDs.\n",
    "    \"\"\"\n",
    "    base_url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
    "    title_without_not = re.sub(r'not', '', title)\n",
    "    if api_key:\n",
    "        base_url += f'&api_key={api_key}'\n",
    "    if publication:\n",
    "        search_term = f'({title_without_not} [ti]) AND ({publication} [ta])'\n",
    "    else:\n",
    "        search_term = f'{title_without_not} [ti]'\n",
    "    params = {\n",
    "        'db': 'pubmed',\n",
    "        'term': search_term,\n",
    "        'retmax': 5,\n",
    "        'retmode': 'json'\n",
    "    }\n",
    "    print(f'Search term: {search_term}')\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    cleaned_title = re.sub(r'</?[ib]>', '', title) # remove bold and italic html tags\n",
    "    cleaned_title = re.sub(r'[^a-zA-Z0-9 ]', '', cleaned_title).lower().strip()\n",
    "    cleaned_title = re.sub(r\"\\u2010\", '', cleaned_title)\n",
    "    # print(f'Data keys: {data.keys()}')\n",
    "    try:\n",
    "        id_list = data['esearchresult']['idlist']\n",
    "        if id_list:\n",
    "            for index in range(len(id_list)):\n",
    "                result = retrieve_citation(id_list[index], api_key).decode('utf-8')\n",
    "                cleaned_result = re.sub(r'[^a-zA-Z0-9 <>/]', '', result).lower().strip() \n",
    "                result = retrieve_citation(id_list[index], api_key).decode('utf-8')\n",
    "                result_title_match = re.search(r'<articletitle>(.*?)</articletitle>', cleaned_result)\n",
    "                if result_title_match:\n",
    "                    result_title = result_title_match.group(1)\n",
    "                    cleaned_result_title = re.sub(r'</?[ib]>', '', result_title)\n",
    "                    cleaned_result_title = re.sub(r'/(?![^<>]*>)', '', cleaned_result_title) # Remove any / that is not within html tag\n",
    "                    cleaned_result_title = re.sub(r'[^a-zA-Z0-9 <>/]', '', cleaned_result_title).lower().strip()\n",
    "                else:\n",
    "                    cleaned_result_title = cleaned_result\n",
    "                if cleaned_title == cleaned_result_title:\n",
    "                    if verbose:\n",
    "                        print(f'Match found for {title}: PMID = {article_id}.')\n",
    "                        return result\n",
    "                else:\n",
    "                    continue\n",
    "            if cleaned_title != cleaned_result_title:\n",
    "                print(f'Warning: Article title not found in PMIDs.')\n",
    "                print(f'Check these PMIDs: {id_list}')\n",
    "                print(f'\\tInput title: {title.lower().strip()}')\n",
    "                print(f'\\tResult title: {result_title if result_title else cleaned_result}')\n",
    "                print(f'\\tCleaned input title: {cleaned_title}')\n",
    "                print(f'\\tCleaned result title: {cleaned_result_title}\\n')\n",
    "            return result     \n",
    "    except Exception as error: \n",
    "        exc_type, exc_obj, tb = sys.exc_info()\n",
    "        file = tb.tb_frame\n",
    "        lineno = tb.tb_lineno\n",
    "        filename = file.f_code.co_filename\n",
    "        print(f'\\tAn error occurred on line {lineno} in {filename}: {error}')    \n",
    "        print('Article not found.')\n",
    "        print(f'\\tInput title: {title.lower().strip()}')\n",
    "        return ''.join([id for id in id_list]) \n",
    "    \n",
    "def retrieve_citation(article_id, api_key):\n",
    "    \"\"\"\n",
    "    Retrieve article metadata from PubMed database.\n",
    "    \"\"\"\n",
    "    base_url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "    if api_key:\n",
    "        base_url += f'&api_key={api_key}'\n",
    "    params = {\n",
    "        'db': 'pubmed',\n",
    "        'id': article_id\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    return response.content\n",
    "\n",
    "def extract_pubmed_details(record_string):\n",
    "    \"\"\"\n",
    "    Helper function called by `pubmed_details_by_title` to parse article metadata from PubMed database.\n",
    "    \"\"\"\n",
    "    authors = re.findall(r'<Author ValidYN=\"Y\".*?><LastName>(.*?)</LastName><ForeName>(.*?)</ForeName>', record_string)\n",
    "    formatted_authors = ', '.join(['{} {}'.format(author[1], author[0]) for author in authors])\n",
    "\n",
    "    # Extract publication year\n",
    "    publication_year = re.search(r'<PubDate><Year>(\\d{4})</Year>', record_string)\n",
    "    publication_year = publication_year.group(1) if publication_year else ''\n",
    "    publication_month = re.search(r'<PubDate>.*?<Month>(Aug)</Month>.*?</PubDate>', record_string)\n",
    "    publication_month = publication_month.group(1) if publication_month else ''\n",
    "\n",
    "    # Extract article title\n",
    "    article_title = re.search(r'<ArticleTitle>(.*?)</ArticleTitle>', record_string)\n",
    "    article_title = article_title.group(1) if article_title else ''\n",
    "\n",
    "    # Extract journal title\n",
    "    journal_title = re.search(r'<Title>(.*?)</Title>', record_string)\n",
    "    journal_title = journal_title.group(1) if journal_title else ''\n",
    "\n",
    "    # Extract journal volume\n",
    "    journal_volume = re.search(r'<Volume>(.*?)</Volume>', record_string)\n",
    "    journal_volume = journal_volume.group(1) if journal_volume else ''\n",
    "\n",
    "    # Extract journal issue\n",
    "    journal_issue = re.search(r'<Issue>(.*?)</Issue>', record_string)\n",
    "    journal_issue = journal_issue.group(1) if journal_issue else ''\n",
    "\n",
    "    # Extract start page\n",
    "    start_page = re.search(r'<StartPage>(.*?)</StartPage>', record_string)\n",
    "    start_page = start_page.group(1) if start_page else ''\n",
    "\n",
    "    # Extract end page\n",
    "    end_page = re.search(r'<EndPage>(.*?)</EndPage>', record_string)\n",
    "    end_page = end_page.group(1) if end_page else ''\n",
    "\n",
    "    # Extract ELocationID\n",
    "    doi = re.search(r'<ELocationID.*?EIdType=\"doi\".*?>(.*?)</ELocationID>', record_string)\n",
    "    doi = doi.group(1) if doi else ''\n",
    "\n",
    "    abstract_matches = re.findall(r'(<AbstractText.*?>.*?</AbstractText>)', record_string)\n",
    "    print(f'Number of abstract sections: {len(abstract_matches)}')\n",
    "    if len(abstract_matches) > 1:\n",
    "        cleaned_abstract_sections = []\n",
    "        for match in abstract_matches:\n",
    "            # clean_match = re.sub(r'<AbstractText(.*?>.*)</AbstractText>', r'\\1', match)\n",
    "            clean_match = re.sub(r'<AbstractText.*?((?:Label=\".*\")?.*?>.*)</AbstractText>', r'\\1', match)\n",
    "            clean_match = re.sub(r'(?: Label=\"(.*?)\")?.*?>(.*)', r'\\1: \\2', clean_match)\n",
    "            cleaned_abstract_sections.append(clean_match)\n",
    "            \n",
    "        abstract = ''.join([f'{group}<br>' for group in cleaned_abstract_sections])\n",
    "    else:\n",
    "        abstract = re.sub(r'<AbstractText.*?>(.*?)</AbstractText>', r'\\1', abstract_matches[0])  if abstract_matches else ''\n",
    "\n",
    "    # abstract = 'abstract'\n",
    "\n",
    "    return {\n",
    "        'pubmed_title': article_title,\n",
    "        'abstract': abstract,\n",
    "        'journal': journal_title,\n",
    "        'authors': formatted_authors,\n",
    "        'year': publication_year,\n",
    "        'month': publication_month,\n",
    "        'pub_volume': journal_volume,\n",
    "        'pub_issue': journal_issue,\n",
    "        'start_page': start_page,\n",
    "        'end_page': end_page,\n",
    "        'doi': doi,\n",
    "    }\n",
    "\n",
    "\n",
    "def pubmed_details_by_title(title, publication, api_key):\n",
    "    \"\"\"\n",
    "    Search for article title in PubMed database and return article details.\n",
    "\n",
    "    Parameters:\n",
    "    - title (str): article title\n",
    "    - api_key (str): NCBI API key\n",
    "\n",
    "    Returns:\n",
    "    article_details (dict): Article metadata from PubMed database if present. Otherwise, returns list of PMIDs.\n",
    "    \"\"\"\n",
    "    record_string = search_article(title, publication, api_key)\n",
    "    # return record_string\n",
    "    if record_string:\n",
    "        article_details = extract_pubmed_details(record_string)\n",
    "        return article_details\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def add_pubmed_details(text_df, api_key):\n",
    "    \"\"\"\n",
    "    Add the article metadata to a DataFrame containing article title and text.\n",
    "\n",
    "    Parameters:\n",
    "    - text_df (pd.DataFrame): DataFrame containing article title and text.\n",
    "    - api_key (str): NCBI API key\n",
    "\n",
    "    Returns:\n",
    "    DataFrame with added PubMed details for each article.\n",
    "    \"\"\"\n",
    "    article_details_list = []\n",
    "    for index in text_df.index:\n",
    "        article = text_df.loc[index, 'title']\n",
    "        text = str(text_df.loc[index, 'body'])\n",
    "        publication = text_df.loc[index, 'publication']\n",
    "        article_details = pubmed_details_by_title(article, publication, api_key)\n",
    "        if article_details:\n",
    "            article_details['text'] = text\n",
    "            article_details_list.append(article_details)\n",
    "        else:\n",
    "            article_details_list.append({\n",
    "                'pubmed_title': article,\n",
    "                'abstract': '',\n",
    "                'journal': publication,\n",
    "                'authors': '',\n",
    "                'year': '',\n",
    "                'month': '',\n",
    "                'pub_volume': '',\n",
    "                'pub_issue': '',\n",
    "                'start_page': '',\n",
    "                'end_page': '',\n",
    "                'doi': '',\n",
    "                'text': text,\n",
    "                'section': section\n",
    "            })\n",
    "    article_details_df = pd.DataFrame(article_details_list)\n",
    "    return pd.concat([text_df.reset_index(drop=True), article_details_df], axis=1)\n",
    "\n",
    "def compare_columns(df, col1='title', col2='pubmed_title'):\n",
    "    \"\"\"\n",
    "    Compare two columns in a DataFrame. Drop the second column if the two columns are identical.\n",
    "    Otherwise, return the dataframe with new column with the comparison results, \n",
    "    where `True` indicates a mismatch.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the two columns to be compared.\n",
    "    - col1 (str): Name of the first column to be compared.\n",
    "    - col2 (str): Name of the second column to be compared.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame with added column containing the comparison results.\n",
    "    \"\"\"\n",
    "    # Remove punctuation and special characters\n",
    "    remove_punct = lambda text: re.sub(f'[{string.punctuation}]', '', text)\n",
    "    col1 = df[col1].apply(remove_punct)\n",
    "    col2 = df[col2].apply(remove_punct)\n",
    "\n",
    "    # Convert to lowercase and remove white spaces\n",
    "    clean_text = lambda text: text.lower().strip()\n",
    "    col1 = col1.apply(clean_text)\n",
    "    col2 = col2.apply(clean_text)\n",
    "\n",
    "    # Perform the comparison\n",
    "    comparison = col1 != col2\n",
    "    if sum(comparison) == 0:\n",
    "        df = df.drop(columns=['pubmed_title'])\n",
    "    else:\n",
    "        df['flag_title'] = comparison\n",
    "        flagged_indices = df[df['flag_title'] == True].index\n",
    "        for index in flagged_indices:\n",
    "            print(f'Flagged: ')\n",
    "            print(f'\\tArticle title: {df.loc[index, \"title\"]}')\n",
    "            print(f'\\tPubMed title: {df.loc[index, \"pubmed_title\"]}')\n",
    "            print()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_sources_table(text_df, col1='title', col2='pubmed_title'):\n",
    "    references_df = add_pubmed_details(text_df, api_key)\n",
    "\n",
    "    references_df = compare_columns(references_df, col1=col1, col2=col2)\n",
    "    return references_df\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP, Numeric, Boolean\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "## This script calls upon functions and magic functions in db_session.py\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class GPT_queue(Base):\n",
    "    __tablename__ = 'gpt_queue'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    body = mapped_column(Text)\n",
    "    section = mapped_column(String(100))\n",
    "    sent_to_sources = mapped_column(Boolean)\n",
    "    publication = mapped_column(String(100))\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    section = mapped_column(String(100))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    full_template = mapped_column(Text)\n",
    "    system_role = mapped_column(String(300))\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    rating_original_content = mapped_column(Integer) \n",
    "    simple_summary = mapped_column(Text)\n",
    "    rating_simple_content = mapped_column(Integer) \n",
    "    original_headline = mapped_column(String(255))\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    model = mapped_column(String(70))\n",
    "    temperature = mapped_column(Numeric)\n",
    "\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_from_queue(session, input_df, order_by='id', order='ASC'):\n",
    "    \"\"\"\n",
    "    Return the matching records from the sources table as a pandas dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - input_df: A pandas DataFrame with the article records from the gpt_queue table or equivalent. Columns include 'title' and 'section'.\n",
    "    - limit: The number of records to return.\n",
    "    \"\"\"\n",
    "    def row_to_dict(row):\n",
    "        result = session.query(Sources).filter_by(\n",
    "            title=row['title'],\n",
    "            section=row['section']\n",
    "        ).limit(1).all()[0]\n",
    "        \n",
    "        sources_series = pd.Series({column.name: getattr(result, column.name) for column in result.__table__.columns})\n",
    "        return sources_series\n",
    "\n",
    "    sources_df = input_df.apply(row_to_dict, axis=1)\n",
    "    ascending = True if order == 'ASC' else False\n",
    "    sources_df.sort_values(order_by, ascending=ascending, inplace=True)\n",
    "    return sources_df\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=None, order_by='id', order='ASC'):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if order_by:\n",
    "        query_statement += f' ORDER BY {order_by} {order}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries'):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    with session.no_autoflush:\n",
    "                        existing_record = session.query(Sources).filter_by(\n",
    "                            title=row['title'],\n",
    "                            doi=row['doi'],\n",
    "                            section=row['section']\n",
    "                        ).first()\n",
    "                        if not existing_record:\n",
    "                            data = Sources(\n",
    "                                title=row['title'],\n",
    "                                text=row['text'],\n",
    "                                abstract=row['abstract'],\n",
    "                                publication=row['journal'],\n",
    "                                authors=row['authors'],\n",
    "                                year=row['year'],\n",
    "                                month=row['month'],\n",
    "                                pub_volume=row['pub_volume'],\n",
    "                                pub_issue=row['pub_issue'],\n",
    "                                start_page=row['start_page'],\n",
    "                                end_page=row['end_page'],\n",
    "                                doi=row['doi'],\n",
    "                                section=row['section'] \n",
    "                            )\n",
    "                            session.add(data)\n",
    "                            print(f'\\t{row[\"title\"]}')\n",
    "                        else:\n",
    "                            print(f'\\t** Already exists in the database: {row[\"title\"]}.')\n",
    "                elif table == 'gpt_queue':\n",
    "                    data = GPT_queue(\n",
    "                        title=row['title'],\n",
    "                        body=row['body'],\n",
    "                        section=row['section'],\n",
    "                        sent_to_sources=row['sent_to_sources'],\n",
    "                        publication=row['journal']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                    print(f'\\t{row[\"title\"]}')\n",
    "                elif table == 'summaries':\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full_summarize_task'],\n",
    "                        system_role=row['system_role'],\n",
    "                    ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['summarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task'],\n",
    "                            system_role=row['system_role']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice'],\n",
    "                        model=row['model'],\n",
    "                        temperature=row['temperature']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                    print(f'\\tReference #{row[\"reference_id\"]}: {row[\"headline\"]}')\n",
    "                elif table == 'feed':\n",
    "                    source = session.query(Feed).filter_by(\n",
    "                        title=row['title'],\n",
    "                        journal=row['journal'],\n",
    "                        doi=row['doi']\n",
    "                    ).first()\n",
    "                    if source:\n",
    "                        print(f'\\tAlready exists in the database: {row[\"title\"]}.')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"New records added successfully (if applicable)!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "from functools import wraps\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\private\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from prompts import * # .py file stored in the path above\n",
    "from db_orm import * \n",
    "from sources import *\n",
    "from orm_summarize import *\n",
    "from article_processing import *\n",
    "\n",
    "#########\n",
    "#########\n",
    "# Prep: Set parameters\n",
    "folder_path = '../text/2023-05-03 5'\n",
    "section = 'Sleep disturbance and obesity risk'\n",
    "local = False\n",
    "n_choices = 1\n",
    "article_limit = 1\n",
    "temperature = 1\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "iteration_id = 2.1\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "# model = 'gpt-4'\n",
    "save_outputs=False\n",
    "\n",
    "def generate_summaries(n_choices, temperature, model, pause_per_request, folder_path, section, local, article_limit=article_limit):\n",
    "    ### Set up\n",
    "    qna_dict = dict()\n",
    "    chatbot_dict = dict()\n",
    "    references_df_dict = dict()\n",
    "\n",
    "    # set the option to wrap text within cells\n",
    "    pd.set_option('display.max_colwidth', 50)\n",
    "    pd.set_option('display.max_rows', 20)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "\n",
    "    ####### \n",
    "    # Step 1: Create sources table\n",
    "    if local:\n",
    "        text_df = parse_fulltext(folder_path, section).iloc[:article_limit if article_limit else len(text_df)]\n",
    "    else:\n",
    "        text_df = get_table(table='gpt_queue', limit=article_limit, order='DESC') # db_orm.py\n",
    "    references_df_dict[iteration_id] = create_sources_table(text_df) # sources.py\n",
    "    # return references_df_dict[iteration_id]\n",
    "\n",
    "    ###### \n",
    "    # Step 2:  Add rows from gpt_queue table to sources table \n",
    "    bulk_append(table='sources', input_df=references_df_dict[iteration_id]) # db_orm.py\n",
    "\n",
    "    # ##### \n",
    "    # Step 3: Get the new sources for summarization\n",
    "    sources_df = get_from_queue(input_df=text_df, order_by='id', order='ASC')\n",
    "\n",
    "    # ##### \n",
    "    # Step 4: Create summaries (functions contained in orm_summarize.py)\n",
    "    chatbot_dict = batch_summarize( # orm_summarize.py\n",
    "        sources_df, folder_path, prep_step, summarize_task, edit_task,  # parameter values found in prompts.py\n",
    "        simplify_task, simplify_audience, format_task,\n",
    "        chatbot_dict, temperature=temperature,\n",
    "        system_role=system_role, model=model, max_tokens=1000,\n",
    "        n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "        iteration_id=iteration_id, save_outputs=save_outputs\n",
    "        )\n",
    "    #########\n",
    "    # Step 5: Create summaries table\n",
    "    qna_dict = create_summaries_df(\n",
    "        qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "        )\n",
    "\n",
    "    ##########\n",
    "    # Step 5: Add results to summaries and prompts table \n",
    "    bulk_append(table='summaries', input_df=qna_dict[iteration_id]) # db_orm.py\n",
    "\n",
    "    return qna_dict[iteration_id]\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    qna_dict = generate_summaries(n_choices, temperature, model, pause_per_request, folder_path, section, local=local, article_limit=article_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>reference_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>choice</th>\n",
       "      <th>text</th>\n",
       "      <th>system_role</th>\n",
       "      <th>model</th>\n",
       "      <th>temperature</th>\n",
       "      <th>prep_step</th>\n",
       "      <th>summarize_task</th>\n",
       "      <th>edit_task</th>\n",
       "      <th>simplify_task</th>\n",
       "      <th>simplify_audience</th>\n",
       "      <th>format_task</th>\n",
       "      <th>full_summarize_task</th>\n",
       "      <th>folder</th>\n",
       "      <th>summary</th>\n",
       "      <th>headline</th>\n",
       "      <th>simple_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-07 11:38:17.841662-07:00</td>\n",
       "      <td>202</td>\n",
       "      <td>Sleep and obesity</td>\n",
       "      <td>1</td>\n",
       "      <td>Sleep and Obesity\\r\\n\\r\\nObstructive sleep apn...</td>\n",
       "      <td>You are an expert at science communication.</td>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>1</td>\n",
       "      <td>Describe the interesting points to your cowork...</td>\n",
       "      <td>Take the key points and numerical descriptors to</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4. Return your final response in a JSON format...</td>\n",
       "      <td>Take the key points and numerical descriptors ...</td>\n",
       "      <td>text/2023-05-03 5</td>\n",
       "      <td>Sleep loss and poor sleep quality, even in the...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          timestamp  reference_id      article_title  choice  \\\n",
       "0  2023-08-07 11:38:17.841662-07:00           202  Sleep and obesity       1   \n",
       "\n",
       "                                                text  \\\n",
       "0  Sleep and Obesity\\r\\n\\r\\nObstructive sleep apn...   \n",
       "\n",
       "                                   system_role                   model  \\\n",
       "0  You are an expert at science communication.  gpt-3.5-turbo-16k-0613   \n",
       "\n",
       "   temperature                                          prep_step  \\\n",
       "0            1  Describe the interesting points to your cowork...   \n",
       "\n",
       "                                     summarize_task edit_task simplify_task  \\\n",
       "0  Take the key points and numerical descriptors to                           \n",
       "\n",
       "  simplify_audience                                        format_task  \\\n",
       "0                    4. Return your final response in a JSON format...   \n",
       "\n",
       "                                 full_summarize_task             folder  \\\n",
       "0  Take the key points and numerical descriptors ...  text/2023-05-03 5   \n",
       "\n",
       "                                             summary headline simple_summary  \n",
       "0  Sleep loss and poor sleep quality, even in the...     None           None  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: SELECT * from gpt_queue ORDER BY id DESC LIMIT 1\n",
      "Search term: (Sleep and obesity [ti]) AND (Curr Opin Clin Nutr Metab Care [ta])\n",
      "Number of abstract sections: 3\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import requests\n",
    "from article_processing import create_text_dict_from_folder\n",
    "from orm_summarize import *\n",
    "api_key = os.getenv('api_ncbi') # Pubmed API key\n",
    "\n",
    "### These scripts populate data in the sources table with data from the Pubmed API.\n",
    "\n",
    "def search_article(title, publication, api_key, verbose=False):\n",
    "    \"\"\"\n",
    "    Search for article title in PubMed database.\n",
    "\n",
    "    Parameters:\n",
    "    - title (str): article title\n",
    "    - api_key (str): NCBI API key\n",
    "\n",
    "    Returns:\n",
    "    response (str): Article metadata from PubMed database if present. Otherwise, returns list of PMIDs.\n",
    "    \"\"\"\n",
    "    base_url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
    "    title_without_not = re.sub(r'not', '', title)\n",
    "    if api_key:\n",
    "        base_url += f'&api_key={api_key}'\n",
    "    if publication:\n",
    "        search_term = f'({title_without_not} [ti]) AND ({publication} [ta])'\n",
    "    else:\n",
    "        search_term = f'{title_without_not} [ti]'\n",
    "    params = {\n",
    "        'db': 'pubmed',\n",
    "        'term': search_term,\n",
    "        'retmax': 5,\n",
    "        'retmode': 'json'\n",
    "    }\n",
    "    print(f'Search term: {search_term}')\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    cleaned_title = re.sub(r'</?[ib]>', '', title) # remove bold and italic html tags\n",
    "    cleaned_title = re.sub(r'[^a-zA-Z0-9 ]', '', cleaned_title).lower().strip()\n",
    "    cleaned_title = re.sub(r\"\\u2010\", '', cleaned_title)\n",
    "    try:\n",
    "        id_list = data['esearchresult']['idlist']\n",
    "        if id_list:\n",
    "            for index in range(len(id_list)):\n",
    "                result = retrieve_citation(id_list[index], api_key).decode('utf-8')\n",
    "                cleaned_result = re.sub(r'[^a-zA-Z0-9 <>/]', '', result).lower().strip() \n",
    "                result = retrieve_citation(id_list[index], api_key).decode('utf-8')\n",
    "                result_title_match = re.search(r'<articletitle>(.*?)</articletitle>', cleaned_result)\n",
    "                if result_title_match:\n",
    "                    result_title = result_title_match.group(1)\n",
    "                    cleaned_result_title = re.sub(r'</?[ib]>', '', result_title)\n",
    "                    cleaned_result_title = re.sub(r'/(?![^<>]*>)', '', cleaned_result_title) # Remove any / that is not within html tag\n",
    "                    cleaned_result_title = re.sub(r'[^a-zA-Z0-9 <>/]', '', cleaned_result_title).lower().strip()\n",
    "                else:\n",
    "                    cleaned_result_title = cleaned_result\n",
    "                if cleaned_title == cleaned_result_title:\n",
    "                    if verbose:\n",
    "                        print(f'Match found for {title}: PMID = {article_id}.')\n",
    "                        return result\n",
    "                else:\n",
    "                    continue\n",
    "            if cleaned_title != cleaned_result_title:\n",
    "                print(f'Warning: Article title not found in PMIDs.')\n",
    "                print(f'Check these PMIDs: {id_list}')\n",
    "                print(f'\\tInput title: {title.lower().strip()}')\n",
    "                print(f'\\tResult title: {result_title if result_title else cleaned_result}')\n",
    "                print(f'\\tCleaned input title: {cleaned_title}')\n",
    "                print(f'\\tCleaned result title: {cleaned_result_title}\\n')\n",
    "            return result     \n",
    "    except Exception as error: \n",
    "        print(f'Response: \\n{data}')\n",
    "        exc_type, exc_obj, tb = sys.exc_info()\n",
    "        file = tb.tb_frame\n",
    "        lineno = tb.tb_lineno\n",
    "        filename = file.f_code.co_filename\n",
    "        print(f'\\tAn error occurred on line {lineno} in {filename}: {error}')    \n",
    "        print('Article not found.')\n",
    "        print(f'\\tInput title: {title.lower().strip()}')\n",
    "        return ''.join([id for id in id_list]) \n",
    "    \n",
    "def retrieve_citation(article_id, api_key):\n",
    "    \"\"\"\n",
    "    Retrieve article metadata from PubMed database.\n",
    "    \"\"\"\n",
    "    base_url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "    if api_key:\n",
    "        base_url += f'&api_key={api_key}'\n",
    "    params = {\n",
    "        'db': 'pubmed',\n",
    "        'id': article_id\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    return response.content\n",
    "\n",
    "def extract_pubmed_details(record_string):\n",
    "    \"\"\"\n",
    "    Helper function called by `pubmed_details_by_title` to parse article metadata from PubMed database.\n",
    "    \"\"\"\n",
    "    authors = re.findall(r'<Author ValidYN=\"Y\".*?><LastName>(.*?)</LastName><ForeName>(.*?)</ForeName>', record_string)\n",
    "    formatted_authors = ', '.join(['{} {}'.format(author[1], author[0]) for author in authors])\n",
    "\n",
    "    # Extract publication year\n",
    "    publication_year = re.search(r'<PubDate><Year>(\\d{4})</Year>', record_string)\n",
    "    publication_year = publication_year.group(1) if publication_year else ''\n",
    "    publication_month = re.search(r'<PubDate>.*?<Month>(Aug)</Month>.*?</PubDate>', record_string)\n",
    "    publication_month = publication_month.group(1) if publication_month else ''\n",
    "\n",
    "    # Extract article title\n",
    "    article_title = re.search(r'<ArticleTitle>(.*?)</ArticleTitle>', record_string)\n",
    "    article_title = article_title.group(1) if article_title else ''\n",
    "\n",
    "    # Extract journal title\n",
    "    journal_title = re.search(r'<Title>(.*?)</Title>', record_string)\n",
    "    journal_title = journal_title.group(1) if journal_title else ''\n",
    "\n",
    "    # Extract journal volume\n",
    "    journal_volume = re.search(r'<Volume>(.*?)</Volume>', record_string)\n",
    "    journal_volume = journal_volume.group(1) if journal_volume else ''\n",
    "\n",
    "    # Extract journal issue\n",
    "    journal_issue = re.search(r'<Issue>(.*?)</Issue>', record_string)\n",
    "    journal_issue = journal_issue.group(1) if journal_issue else ''\n",
    "\n",
    "    # Extract start page\n",
    "    start_page = re.search(r'<StartPage>(.*?)</StartPage>', record_string)\n",
    "    start_page = start_page.group(1) if start_page else ''\n",
    "\n",
    "    # Extract end page\n",
    "    end_page = re.search(r'<EndPage>(.*?)</EndPage>', record_string)\n",
    "    end_page = end_page.group(1) if end_page else ''\n",
    "\n",
    "    # Extract ELocationID\n",
    "    doi = re.search(r'<ELocationID.*?EIdType=\"doi\".*?>(.*?)</ELocationID>', record_string)\n",
    "    doi = doi.group(1) if doi else ''\n",
    "\n",
    "    abstract_matches = re.findall(r'(<AbstractText.*?>.*?</AbstractText>)', record_string)\n",
    "    print(f'Number of abstract sections: {len(abstract_matches)}')\n",
    "    if len(abstract_matches) > 1:\n",
    "        cleaned_abstract_sections = []\n",
    "        for match in abstract_matches:\n",
    "            # clean_match = re.sub(r'<AbstractText(.*?>.*)</AbstractText>', r'\\1', match)\n",
    "            clean_match = re.sub(r'<AbstractText.*?((?:Label=\".*\")?.*?>.*)</AbstractText>', r'\\1', match)\n",
    "            clean_match = re.sub(r'(?: Label=\"(.*?)\")?.*?>(.*)', r'\\1: \\2', clean_match)\n",
    "            cleaned_abstract_sections.append(clean_match)\n",
    "            \n",
    "        abstract = ''.join([f'{group}<br>' for group in cleaned_abstract_sections])\n",
    "    else:\n",
    "        abstract = re.sub(r'<AbstractText.*?>(.*?)</AbstractText>', r'\\1', abstract_matches[0])  if abstract_matches else ''\n",
    "\n",
    "    return {\n",
    "        'pubmed_title': article_title,\n",
    "        'abstract': abstract,\n",
    "        'journal': journal_title,\n",
    "        'authors': formatted_authors,\n",
    "        'year': publication_year,\n",
    "        'month': publication_month,\n",
    "        'pub_volume': journal_volume,\n",
    "        'pub_issue': journal_issue,\n",
    "        'start_page': start_page,\n",
    "        'end_page': end_page,\n",
    "        'doi': doi,\n",
    "    }\n",
    "\n",
    "\n",
    "def pubmed_details_by_title(title, publication, api_key):\n",
    "    \"\"\"\n",
    "    Search for article title in PubMed database and return article details.\n",
    "\n",
    "    Parameters:\n",
    "    - title (str): article title\n",
    "    - api_key (str): NCBI API key\n",
    "\n",
    "    Returns:\n",
    "    article_details (dict): Article metadata from PubMed database if present. Otherwise, returns list of PMIDs.\n",
    "    \"\"\"\n",
    "    record_string = search_article(title, publication, api_key)\n",
    "    # return record_string\n",
    "    if record_string:\n",
    "        article_details = extract_pubmed_details(record_string)\n",
    "        return article_details\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def add_pubmed_details(text_df, api_key):\n",
    "    \"\"\"\n",
    "    Add the article metadata to a DataFrame containing article title and text.\n",
    "\n",
    "    Parameters:\n",
    "    - text_df (pd.DataFrame): DataFrame containing article title and text.\n",
    "    - api_key (str): NCBI API key\n",
    "\n",
    "    Returns:\n",
    "    DataFrame with added PubMed details for each article.\n",
    "    \"\"\"\n",
    "    article_details_list = []\n",
    "    for index in text_df.index:\n",
    "        article = text_df.loc[index, 'title']\n",
    "        text = str(text_df.loc[index, 'body'])\n",
    "        publication = text_df.loc[index, 'publication']\n",
    "        article_details = pubmed_details_by_title(article, publication, api_key)\n",
    "        if article_details:\n",
    "            article_details['text'] = text\n",
    "            article_details_list.append(article_details)\n",
    "        else:\n",
    "            article_details_list.append({\n",
    "                'pubmed_title': article,\n",
    "                'abstract': '',\n",
    "                'journal': publication,\n",
    "                'authors': '',\n",
    "                'year': '',\n",
    "                'month': '',\n",
    "                'pub_volume': '',\n",
    "                'pub_issue': '',\n",
    "                'start_page': '',\n",
    "                'end_page': '',\n",
    "                'doi': '',\n",
    "                'text': text,\n",
    "                'section': section\n",
    "            })\n",
    "    article_details_df = pd.DataFrame(article_details_list)\n",
    "    return pd.concat([text_df.reset_index(drop=True), article_details_df], axis=1)\n",
    "\n",
    "def compare_columns(df, col1='title', col2='pubmed_title'):\n",
    "    \"\"\"\n",
    "    Compare two columns in a DataFrame. Drop the second column if the two columns are identical.\n",
    "    Otherwise, return the dataframe with new column with the comparison results, \n",
    "    where `True` indicates a mismatch.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the two columns to be compared.\n",
    "    - col1 (str): Name of the first column to be compared.\n",
    "    - col2 (str): Name of the second column to be compared.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame with added column containing the comparison results.\n",
    "    \"\"\"\n",
    "    # Remove punctuation and special characters\n",
    "    remove_punct = lambda text: re.sub(f'[{string.punctuation}]', '', text)\n",
    "    col1 = df[col1].apply(remove_punct)\n",
    "    col2 = df[col2].apply(remove_punct)\n",
    "\n",
    "    # Convert to lowercase and remove white spaces\n",
    "    clean_text = lambda text: text.lower().strip()\n",
    "    col1 = col1.apply(clean_text)\n",
    "    col2 = col2.apply(clean_text)\n",
    "\n",
    "    # Perform the comparison\n",
    "    comparison = col1 != col2\n",
    "    if sum(comparison) == 0:\n",
    "        df = df.drop(columns=['pubmed_title'])\n",
    "    else:\n",
    "        df['flag_title'] = comparison\n",
    "        flagged_indices = df[df['flag_title'] == True].index\n",
    "        for index in flagged_indices:\n",
    "            print(f'Flagged: ')\n",
    "            print(f'\\tArticle title: {df.loc[index, \"title\"]}')\n",
    "            print(f'\\tPubMed title: {df.loc[index, \"pubmed_title\"]}')\n",
    "            print()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_sources_table(text_df, col1='title', col2='pubmed_title'):\n",
    "    references_df = add_pubmed_details(text_df, api_key)\n",
    "\n",
    "    references_df = compare_columns(references_df, col1=col1, col2=col2)\n",
    "    return references_df\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from db_session import *\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String, Text, TIMESTAMP, Numeric, Boolean\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import uuid\n",
    "import pandas as pd\n",
    "# from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import mapped_column\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "## This script calls upon functions and magic functions in db_session.py\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class GPT_queue(Base):\n",
    "    __tablename__ = 'gpt_queue'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    body = mapped_column(Text)\n",
    "    section = mapped_column(String(100))\n",
    "    sent_to_sources = mapped_column(Boolean)\n",
    "    publication = mapped_column(String(100))\n",
    "\n",
    "class Sources(Base):\n",
    "    __tablename__ = 'sources'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    title = mapped_column(String(255))\n",
    "    text = mapped_column(Text)\n",
    "    abstract = mapped_column(Text)\n",
    "    publication = mapped_column(String(100))\n",
    "    authors = mapped_column(String(300))\n",
    "    year = mapped_column(Integer)\n",
    "    month = mapped_column(String(10))\n",
    "    pub_volume = mapped_column(String(10))\n",
    "    pub_issue = mapped_column(String(10))\n",
    "    start_page = mapped_column(String(10))\n",
    "    end_page = mapped_column(String(10))\n",
    "    doi = mapped_column(String(50))\n",
    "    section = mapped_column(String(100))\n",
    "    summaries = relationship('Summaries', back_populates='sources')\n",
    "\n",
    "class Prompts(Base):\n",
    "    __tablename__ = 'prompts'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    full_template = mapped_column(Text)\n",
    "    system_role = mapped_column(String(300))\n",
    "    prep_steps = mapped_column(Text)\n",
    "    task = mapped_column(Text)\n",
    "    edit_steps = mapped_column(Text)\n",
    "    simplify_steps = mapped_column(Text)\n",
    "    audience = mapped_column(String(200))\n",
    "    format_steps = mapped_column(Text)\n",
    "\n",
    "    summaries = relationship('Summaries', back_populates='prompts')\n",
    "    \n",
    "class Summaries(Base):\n",
    "    __tablename__ = 'summaries'\n",
    "    id = mapped_column(Integer, primary_key=True)\n",
    "    timestamp = mapped_column(TIMESTAMP(timezone=True))\n",
    "    original_summary = mapped_column(Text)\n",
    "    rating_original_content = mapped_column(Integer) \n",
    "    simple_summary = mapped_column(Text)\n",
    "    rating_simple_content = mapped_column(Integer) \n",
    "    original_headline = mapped_column(String(255))\n",
    "    prompt_id = mapped_column(Integer, ForeignKey('prompts.id'), autoincrement=False)\n",
    "    reference_id = mapped_column(Integer, ForeignKey('sources.id'), autoincrement=False)\n",
    "    choice = mapped_column(Integer)\n",
    "    model = mapped_column(String(70))\n",
    "    temperature = mapped_column(Numeric)\n",
    "\n",
    "    prompts = relationship('Prompts', back_populates='summaries')\n",
    "    sources = relationship('Sources', back_populates='summaries')\n",
    "\n",
    "@remote_sql_session\n",
    "def get_from_queue(session, input_df, order_by='id', order='ASC'):\n",
    "    \"\"\"\n",
    "    Return the matching records from the sources table as a pandas dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - input_df: A pandas DataFrame with the article records from the gpt_queue table or equivalent. Columns include 'title' and 'section'.\n",
    "    - limit: The number of records to return.\n",
    "    \"\"\"\n",
    "    def row_to_dict(row):\n",
    "        result = session.query(Sources).filter_by(\n",
    "            title=row['title'],\n",
    "            section=row['section']\n",
    "        ).limit(1).all()[0]\n",
    "        \n",
    "        sources_series = pd.Series({column.name: getattr(result, column.name) for column in result.__table__.columns})\n",
    "        return sources_series\n",
    "\n",
    "    sources_df = input_df.apply(row_to_dict, axis=1)\n",
    "    ascending = True if order == 'ASC' else False\n",
    "    sources_df.sort_values(order_by, ascending=ascending, inplace=True)\n",
    "    return sources_df\n",
    "\n",
    "@remote_sql_session\n",
    "def get_table(session, query='SELECT *', table='publications', limit=None, order_by='id', order='ASC'):\n",
    "    \"\"\"\n",
    "    Return a database table as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    query_statement = f'{query} from {table}'\n",
    "    if order_by:\n",
    "        query_statement += f' ORDER BY {order_by} {order}'\n",
    "    if limit:\n",
    "        query_statement += f' LIMIT {limit}'\n",
    "    print(f'Query: {query_statement}')\n",
    "    q = session.execute(text(query_statement))\n",
    "    df = pd.DataFrame(q.fetchall())\n",
    "    return df\n",
    "\n",
    "\n",
    "def bulk_append(input_df, table='summaries'):\n",
    "    \"\"\"\n",
    "    Add articles to the `sources` table in the database from a dataframe containing article text and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - references_df: pandas dataframe containing article text and metadata.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    @remote_sql_session\n",
    "    def insert_rows(session):\n",
    "        try:\n",
    "            print(f'Adding {len(input_df)} rows to the database...')\n",
    "            def insert_row(row):\n",
    "                if table == 'sources':\n",
    "                    with session.no_autoflush:\n",
    "                        existing_record = session.query(Sources).filter_by(\n",
    "                            title=row['title'],\n",
    "                            doi=row['doi'],\n",
    "                            section=row['section']\n",
    "                        ).first()\n",
    "                        if not existing_record:\n",
    "                            data = Sources(\n",
    "                                title=row['title'],\n",
    "                                text=row['text'],\n",
    "                                abstract=row['abstract'],\n",
    "                                publication=row['journal'],\n",
    "                                authors=row['authors'],\n",
    "                                year=row['year'],\n",
    "                                month=row['month'],\n",
    "                                pub_volume=row['pub_volume'],\n",
    "                                pub_issue=row['pub_issue'],\n",
    "                                start_page=row['start_page'],\n",
    "                                end_page=row['end_page'],\n",
    "                                doi=row['doi'],\n",
    "                                section=row['section'] \n",
    "                            )\n",
    "                            session.add(data)\n",
    "                            print(f'\\t{row[\"title\"]}')\n",
    "                        else:\n",
    "                            print(f'\\t** Already exists in the database: {row[\"title\"]}.')\n",
    "                elif table == 'gpt_queue':\n",
    "                    data = GPT_queue(\n",
    "                        title=row['title'],\n",
    "                        body=row['body'],\n",
    "                        section=row['section'],\n",
    "                        sent_to_sources=row['sent_to_sources'],\n",
    "                        publication=row['publication']\n",
    "                    )\n",
    "                    session.add(data)\n",
    "                    print(f'\\t{row[\"title\"]}')\n",
    "                elif table == 'summaries':\n",
    "                    prompt = session.query(Prompts).filter_by(\n",
    "                        full_template=row['full_summarize_task'],\n",
    "                        system_role=row['system_role'],\n",
    "                    ).first()\n",
    "                    if prompt:\n",
    "                        prompt_id = prompt.id\n",
    "                    else:\n",
    "                        prompt = Prompts(\n",
    "                            full_template=row['full_summarize_task'],\n",
    "                            prep_steps=row['prep_step'],\n",
    "                            task=row['summarize_task'],\n",
    "                            edit_steps=row['edit_task'],\n",
    "                            audience=row['simplify_audience'],\n",
    "                            simplify_steps=row['simplify_task'],\n",
    "                            format_steps=row['format_task'],\n",
    "                            system_role=row['system_role']\n",
    "                        )\n",
    "                        session.add(prompt)\n",
    "                        session.flush()\n",
    "                        prompt_id = prompt.id\n",
    "\n",
    "                    summary = Summaries(\n",
    "                        timestamp=row['timestamp'],\n",
    "                        original_summary=row['summary'],\n",
    "                        simple_summary=row['simple_summary'],\n",
    "                        original_headline=row['headline'],\n",
    "                        prompt_id=prompt_id,\n",
    "                        reference_id=row['reference_id'],\n",
    "                        choice=row['choice'],\n",
    "                        model=row['model'],\n",
    "                        temperature=row['temperature']\n",
    "                    )\n",
    "                    session.add(summary)\n",
    "                    print(f'\\tReference #{row[\"reference_id\"]}: {row[\"headline\"]}')\n",
    "                elif table == 'feed':\n",
    "                    source = session.query(Feed).filter_by(\n",
    "                        title=row['title'],\n",
    "                        journal=row['journal'],\n",
    "                        doi=row['doi']\n",
    "                    ).first()\n",
    "                    if source:\n",
    "                        print(f'\\tAlready exists in the database: {row[\"title\"]}.')\n",
    "\n",
    "            input_df.apply(insert_row, axis=1)\n",
    "\n",
    "            session.commit()\n",
    "            print(\"New records added successfully (if applicable)!\")\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error adding data to the database: {str(e)}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    return insert_rows()\n",
    "\n",
    "from functools import wraps\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\private\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from prompts import * # .py file stored in the path above\n",
    "from db_orm import * \n",
    "from orm_summarize import *\n",
    "from article_processing import *\n",
    "# from sources import *\n",
    "api_key = os.getenv('api_ncbi') # Pubmed API key\n",
    "\n",
    "#########\n",
    "#########\n",
    "# Prep: Set parameters\n",
    "folder_path = '../text/2023-05-03 5'\n",
    "section = 'Sleep disturbance and obesity risk'\n",
    "local = False\n",
    "n_choices = 1\n",
    "article_limit = 1\n",
    "temperature = 1\n",
    "pause_per_request=0\n",
    "# summary_iteration_id = iteration_id\n",
    "iteration_id = 2.2\n",
    "chatbot_id = iteration_id\n",
    "model = 'gpt-3.5-turbo-16k-0613'\n",
    "# model = 'gpt-4'\n",
    "save_outputs=False\n",
    "\n",
    "def generate_summaries(n_choices, temperature, model, pause_per_request, folder_path, section, local, article_limit=article_limit):\n",
    "    ### Set up\n",
    "    qna_dict = dict()\n",
    "    chatbot_dict = dict()\n",
    "    references_df_dict = dict()\n",
    "\n",
    "    # set the option to wrap text within cells\n",
    "    pd.set_option('display.max_colwidth', 50)\n",
    "    pd.set_option('display.max_rows', 20)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "\n",
    "    ####### \n",
    "    # Step 1: Create sources table\n",
    "    if local:\n",
    "        text_df = parse_fulltext(folder_path, section).iloc[:article_limit if article_limit else len(text_df)]\n",
    "    else:\n",
    "        text_df = get_table(table='gpt_queue', limit=article_limit, order='DESC') # db_orm.py\n",
    "    references_df_dict[iteration_id] = create_sources_table(text_df) # sources.py\n",
    "    # return references_df_dict[iteration_id]\n",
    "\n",
    "    ###### \n",
    "    # Step 2:  Add rows from gpt_queue table to sources table \n",
    "    bulk_append(table='sources', input_df=references_df_dict[iteration_id]) # db_orm.py\n",
    "\n",
    "    # ##### \n",
    "    # Step 3: Get the new sources for summarization\n",
    "    sources_df = get_from_queue(input_df=text_df, order_by='id', order='ASC')\n",
    "\n",
    "    # ##### \n",
    "    # Step 4: Create summaries (functions contained in orm_summarize.py)\n",
    "    chatbot_dict = batch_summarize( # orm_summarize.py\n",
    "        sources_df, folder_path, prep_step, summarize_task, edit_task,  # parameter values found in prompts.py\n",
    "        simplify_task, simplify_audience, format_task,\n",
    "        chatbot_dict, temperature=temperature,\n",
    "        system_role=system_role, model=model, max_tokens=1000,\n",
    "        n_choices=n_choices, pause_per_request=pause_per_request,\n",
    "        iteration_id=iteration_id, save_outputs=save_outputs\n",
    "        )\n",
    "    #########\n",
    "    # Step 5: Create summaries table\n",
    "    qna_dict = create_summaries_df(\n",
    "        qna_dict, chatbot_dict, iteration_id, chatbot_id=chatbot_id\n",
    "        )\n",
    "\n",
    "    ##########\n",
    "    # Step 5: Add results to summaries and prompts table \n",
    "    bulk_append(table='summaries', input_df=qna_dict[iteration_id]) # db_orm.py\n",
    "\n",
    "    return qna_dict[iteration_id]\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    qna_dict = generate_summaries(n_choices, temperature, model, pause_per_request, folder_path, section, local=local, article_limit=article_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>sent_to_sources</th>\n",
       "      <th>section</th>\n",
       "      <th>publication</th>\n",
       "      <th>abstract</th>\n",
       "      <th>journal</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>pub_volume</th>\n",
       "      <th>pub_issue</th>\n",
       "      <th>start_page</th>\n",
       "      <th>end_page</th>\n",
       "      <th>doi</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>Sleep and obesity</td>\n",
       "      <td>Sleep and Obesity\\r\\n\\r\\nObstructive sleep apn...</td>\n",
       "      <td>None</td>\n",
       "      <td>Sleep disturbance and obesity risk</td>\n",
       "      <td>Curr Opin Clin Nutr Metab Care</td>\n",
       "      <td>PURPOSE OF REVIEW: This review summarizes the ...</td>\n",
       "      <td>Current opinion in clinical nutrition and meta...</td>\n",
       "      <td>Guglielmo Beccuti, Silvana Pannain</td>\n",
       "      <td>2011</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>402</td>\n",
       "      <td>412</td>\n",
       "      <td>10.1097/MCO.0b013e3283479109</td>\n",
       "      <td>Sleep and Obesity\\r\\n\\r\\nObstructive sleep apn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              title                                               body  \\\n",
       "0  87  Sleep and obesity  Sleep and Obesity\\r\\n\\r\\nObstructive sleep apn...   \n",
       "\n",
       "  sent_to_sources                             section  \\\n",
       "0            None  Sleep disturbance and obesity risk   \n",
       "\n",
       "                      publication  \\\n",
       "0  Curr Opin Clin Nutr Metab Care   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  PURPOSE OF REVIEW: This review summarizes the ...   \n",
       "\n",
       "                                             journal  \\\n",
       "0  Current opinion in clinical nutrition and meta...   \n",
       "\n",
       "                              authors  year month pub_volume pub_issue  \\\n",
       "0  Guglielmo Beccuti, Silvana Pannain  2011               14         4   \n",
       "\n",
       "  start_page end_page                           doi  \\\n",
       "0        402      412  10.1097/MCO.0b013e3283479109   \n",
       "\n",
       "                                                text  \n",
       "0  Sleep and Obesity\\r\\n\\r\\nObstructive sleep apn...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " qna_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Current opinion in clinical nutrition and metabolic care'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_dict['journal'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
