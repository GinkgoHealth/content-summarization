{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\Ginkgo coding\\content-summarization\\src\")\n",
    "from file_functions import *\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from crochet import setup, wait_for\n",
    "import re\n",
    "from IPython import display\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "setup()\n",
    "\n",
    "def trim_text(text, regex=None):\n",
    "    if regex==None:\n",
    "        regex = '.*<h2>Abstract</h2>.*(?:Introduction.*)?(<h2.*?>Introduction</h2>.*References)<.*' \n",
    "    try:\n",
    "        processed = re.search(regex, text, re.DOTALL).group(1)\n",
    "        html_display = display.HTML(processed)\n",
    "    except: \n",
    "        print('Unable to parse article text')\n",
    "        processed = '<Error parsing article text>' \n",
    "        html_display = processed\n",
    "    return processed, html_display\n",
    "\n",
    "def text_dict_from_web(article_dict, header=2, to_display=0,\n",
    "        regex_str='.*<h\\d>Abstract</h\\d>.*(?:Introduction.*)?(<h\\d.*?>Introduction</h\\d>.*References)<.*'\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Create a text dictionary from a dictionary containing web-scraped articles.\n",
    "\n",
    "    Parameters:\n",
    "        article_dict (dict): Values of each dictionary item are a dictionary representing the data from a \n",
    "            single article: 'url', 'text', and 'title'.\n",
    "\n",
    "    Returns:\n",
    "        text_dict: Dictionary where each item is a string of the text of an article, starting with the title.\n",
    "    \"\"\"\n",
    "    journal = next(iter(article_dict.values()))['journal']\n",
    "    print(f'Parsing {len(article_dict)} articles from {journal}')\n",
    "    regex_str = regex_str.replace('\\d', f'{header}')\n",
    "    regex = rf'{regex_str}'\n",
    "    print(f'Regex pattern: {regex}')\n",
    "    text_dict = dict()\n",
    "    display_dict = dict()\n",
    "    if type(to_display) != list:\n",
    "        to_display = [to_display] \n",
    "    for article_key in article_dict:\n",
    "        trimmed_text, display = trim_text(article_dict[article_key]['text'], regex)\n",
    "        text_dict[article_key] = f\"{article_dict[article_key]['title']}\\n\\n{trimmed_text}\"\n",
    "        if article_key in to_display:\n",
    "            display_dict[article_key] = display\n",
    "    print(f'text_dict keys: {[key for key in text_dict.keys()]}')\n",
    "    return text_dict, display_dict\n",
    "\n",
    "class crawler_RSS1(scrapy.Spider):\n",
    "    name = \"crawler_RSS1\"\n",
    "    \n",
    "    def __init__(self, n_articles):\n",
    "        self.n_articles = n_articles\n",
    "    \n",
    "    def start_requests(self):\n",
    "        journals = {\n",
    "            'PLOS One': 'https://journals.plos.org/plosone/feed/atom',\n",
    "            'BMJ Open': 'https://bmjopen.bmj.com/rss/current.xml',\n",
    "            'Journal of Medical Internet Research': 'https://www.jmir.org/feed/atom',\n",
    "            'PLOS Medicine': 'https://journals.plos.org/plosmedicine/feed/atom'\n",
    "\n",
    "            # 'Annual Review of Medicine': 'https://www.annualreviews.org/action/showFeed?ui=45mu4&mi=3fndc3&ai=sm&jc=med&type=etoc&feed=atom' # response code 403\n",
    "            }\n",
    "        for index, journal in enumerate(journals):\n",
    "            # article_dict[index] = dict()\n",
    "            yield scrapy.Request(\n",
    "                url=journals[journal], callback=self.parse_front, \n",
    "                cb_kwargs={'journal': journal, 'journal_index': index, 'article_dict': article_dict}\n",
    "                )\n",
    "    \n",
    "    def parse_front(self, response, journal, journal_index, article_dict):\n",
    "        response.selector.remove_namespaces() # This is needed for any Atom feeds\n",
    "        # print('Initiation')\n",
    "        try:\n",
    "            if self.n_articles != 1:\n",
    "                article_title = response.xpath('//entry/title/text()').getall()\n",
    "                article_url = response.css('entry > link[rel=\"alternate\"]::attr(href)').getall()\n",
    "                if article_url == []:\n",
    "                    print(f'\\tExtracting using method 2 for {journal}')\n",
    "                    article_title = response.xpath('//item/title/text()').getall()\n",
    "                    article_url = response.css('item > link::text').getall()\n",
    "            else:\n",
    "                article_title = [response.xpath('//entry/title/text()').get()]\n",
    "                article_url = [response.css('entry > link[rel=\"alternate\"]::attr(href)').get()]\n",
    "                if article_url[0] is None:\n",
    "                    print(f'\\tExtracting using method 2 for {journal}')\n",
    "                    article_title = [response.xpath('//item/title/text()').get()]\n",
    "                    article_url = [response.css('item > link::text').get()]\n",
    "        except:\n",
    "            print('fail')\n",
    "        print(f'Found {len(article_title)} articles and {len(article_url)} URLs for {journal}')\n",
    "\n",
    "        # This is required for BMJ Open, which for some reason repeats each article title.\n",
    "        if len(article_title) == len(article_url) * 2:\n",
    "            unique_article_title = []\n",
    "            [unique_article_title.append(article) for article in article_title if article not in unique_article_title]\n",
    "            article_title = unique_article_title\n",
    "            print(f'\\tCorrected number of article titles: {len(article_title)}')\n",
    "        if type(n_articles) == int:\n",
    "            article_url = article_url[:n_articles]\n",
    "\n",
    "        for index, url in enumerate(article_url):\n",
    "            # print(url)\n",
    "            key = round(journal_index + index/100, 2)\n",
    "            article_dict[key] = {\n",
    "                'journal': journal,\n",
    "                'title': article_title[index],\n",
    "                'url': url\n",
    "            }\n",
    "            yield response.follow(\n",
    "                url=url, callback=self.parse_pages, \n",
    "                cb_kwargs={'key': key, 'article_dict': article_dict})\n",
    "                \n",
    "    \n",
    "    def parse_pages(self, response, key, article_dict):\n",
    "        # print(f'Journal #{key}')\n",
    "        text = response.xpath('//h2|//p|//h3|//h4').extract()\n",
    "        article_dict[key]['text'] = ''.join(['\\n'+line for line in text])\n",
    "        if key - int(key) == 0:\n",
    "            print(f'\\t{article_dict[key][\"journal\"]}')\n",
    "            print(f'\\t\\tArticle attributes: {[key for key in article_dict[key].keys()]}')\n",
    "        \n",
    "@wait_for(40)\n",
    "def run_RSS_spider(n_articles='all'):\n",
    "    \"\"\"\n",
    "    Scrape articles from RSS feeds. Must instantiate a blank dictionary as `article_dict` before running the script.\n",
    "    Parameters:\n",
    "        - n_articles (int): Number of articles to scrape from each journal. \n",
    "            If 'all' or other non-integer value, scrape all articles. Default is 'all'.\n",
    "\n",
    "    How to call the function:\n",
    "    ```\n",
    "    article_dict = dict()\n",
    "    run_RSS_spider(n_articles)\n",
    "\n",
    "    ```\n",
    "    \"\"\"\n",
    "    crawler = CrawlerRunner()\n",
    "    d = crawler.crawl(crawler_RSS1, n_articles)\n",
    "    return d\n",
    "\n",
    "def article_titles(article_dict):\n",
    "    \"\"\"\n",
    "    Print the titles of the articles in a dictionary of articles.\n",
    "    \"\"\"\n",
    "    for article in sorted(article_dict):\n",
    "        print(f\"{article}: {article_dict[article]['title']}\")\n",
    "        print(f\"\\t{article_dict[article]['journal']} {article_dict[article]['url']}\\n\")\n",
    "\n",
    "def save_article_dict(article_dict, path, description='scraped_articles_dict', append_version=True,\n",
    "    save_pickle=True, save_json=False, to_csv=False):\n",
    "    \"\"\"\n",
    "    Save a dictionary of articles to a file. Default behaviour is to save as a pickle only.\n",
    "    Parameters:\n",
    "        - article_dict (dict): Dictionary of articles.\n",
    "        - path (str): Path to save the file.\n",
    "        - description (str): Description of the file for the filename.\n",
    "        - append_version (bool): If True, append the date to the filename.\n",
    "        - save_pickle (bool): If True, save the dictionary as a pickle file.\n",
    "        - save_json (bool): If True, save the dictionary as a JSON file.\n",
    "        - to_csv (bool): If True, convert the dictionary to a DataFrame to save as a CSV file.\n",
    "    \"\"\"\n",
    "    if save_pickle == True:\n",
    "        savepickle(article_dict, filename=f'{description}_', path=path, append_version=append_version)\n",
    "    if save_json == True:\n",
    "        save_to_json(article_dict, description=description, path=path, append_version=append_version)\n",
    "    if to_csv == True:\n",
    "        save_csv(pd.DataFrame(article_dict).transpose(), path=path, filename=f'{description}_',\n",
    "            index=False, append_version=append_version)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iteration_id = 1\n",
    "    main_dict = dict()\n",
    "    article_dict = dict()\n",
    "    n_articles = 2\n",
    "    run_RSS_spider(n_articles)\n",
    "\n",
    "    main_dict[iteration_id] = article_dict\n",
    "    # sorted(article_dict.keys())\n",
    "\n",
    "    # save_article_dict(article_dict, path='../web_articles/2023-06-21', to_csv=True, save_json=True)\n",
    "    article_titles(article_dict)\n",
    "\n",
    "    # time.sleep(10)\n",
    "    # text_dict, display_dict = text_dict_from_web(article_dict, to_display=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
