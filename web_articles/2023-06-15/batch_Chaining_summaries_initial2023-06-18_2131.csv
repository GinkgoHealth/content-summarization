A: date,B: folder,C: article_title,D: choice,E: system_role,F: model,G: text,H: prep step,I: summarization task,J: edit task,K: full summarization task,L: summary
2023-06-18 2131,web_articles/2023-06-15,Conducting a systematic review and evaluation of commercially available mobile applications (apps) on a health-related topic: the TECH approach and a step-by-step methodological guide,1,"You are a science writer texting friends, family, and colleagues about the science research you have read.",gpt-3.5-turbo-16k-0613,"Conducting a systematic review and evaluation of commercially available mobile applications (apps) on a health-related topic: the TECH approach and a step-by-step methodological guide

<h2 class="""">Introduction</h2><p id=""p-11"">With the rise in the use of smartphones and other mobile technologies, there has been an increase in the availability of health applications (mHealth apps) designed to be used by individuals for various health issues. Health apps can also support health and care professionals in their daily clinical practice by providing decision support, access to clinical guidelines and education and training.<a id=""xref-ref-1-1"" class=""xref-bibr"" href=""#ref-1"">1</a> In 2018, over 325 000 health apps were developed,<a id=""xref-ref-2-1"" class=""xref-bibr"" href=""#ref-2"">2</a> covering many health conditions and targeted behaviours. For example, mHealth apps can help to support self-management of conditions like diabetes,<a id=""xref-ref-3-1"" class=""xref-bibr"" href=""#ref-3"">3</a> facilitate remote monitoring of patients with chronic conditions<a id=""xref-ref-4-1"" class=""xref-bibr"" href=""#ref-4"">4</a> or support patients with general behaviour change such as increasing/monitoring physical activity<a id=""xref-ref-5-1"" class=""xref-bibr"" href=""#ref-5"">5</a> or dietary change.<a id=""xref-ref-6-1"" class=""xref-bibr"" href=""#ref-6"">6</a> Some health apps also support public health initiatives, such as promoting healthy lifestyles and encouraging the uptake of screening and vaccination programmes.<a id=""xref-ref-7-1"" class=""xref-bibr"" href=""#ref-7"">7 8</a> The World Health Organization (WHO) have released mHealth apps to educate people about road safety, sun protection measures and COVID-19.<a id=""xref-ref-9-1"" class=""xref-bibr"" href=""#ref-9"">9</a> However, concerns have been raised about the quality of advice and support such health apps provide.<a id=""xref-ref-10-1"" class=""xref-bibr"" href=""#ref-10"">10</a> Bates <em>et al</em> also highlight concerns with the accuracy of apps designed to support medical diagnosis and potential gaps in the quality of apps regarding safety and privacy.<a id=""xref-ref-2-2"" class=""xref-bibr"" href=""#ref-2"">2</a></p><p id=""p-12"">There have been attempts to provide frameworks for evaluating the quality of mHealth apps. A systematic review identified 45 frameworks for evaluating mHealth apps, which varied according to the target users (eg, developers, patients), specific conditions (eg, diabetes, mental health, cancer, pain) and various elements of evaluation that are identified in the core domains for Health Technology Assessment framework (such as safety and effectiveness).<a id=""xref-ref-11-1"" class=""xref-bibr"" href=""#ref-11"">11</a> Other frameworks have promoted a more holistic approach by encompassing privacy and security, the evidence base, ease of use and data integration<a id=""xref-ref-12-1"" class=""xref-bibr"" href=""#ref-12"">12</a> or ethical principles related to using health apps in health psychology.<a id=""xref-ref-13-1"" class=""xref-bibr"" href=""#ref-13"">13</a> Reviews have also identified existing methods for assessing mHealth app quality,<a id=""xref-ref-14-1"" class=""xref-bibr"" href=""#ref-14"">14</a> as well as guidelines for reporting evaluations of specific types of technology, such as sensors<a id=""xref-ref-15-1"" class=""xref-bibr"" href=""#ref-15"">15</a> and mHealth interventions, more broadly.<a id=""xref-ref-16-1"" class=""xref-bibr"" href=""#ref-16"">16</a> This includes the development of the Consolidated Standards of Reporting Trials of Electronic and Mobile HEalth Applications and onLine TeleHealth (CONSORT-EHEALTH) checklist, an extension of the original CONSORT checklist for reporting randomised controlled trials (RCTs).<a id=""xref-ref-17-1"" class=""xref-bibr"" href=""#ref-17"">17</a> This focuses on improving the reporting of evidence from research into the use and effectiveness of mHealth applications in research studies.</p><p id=""p-13"">The existing initiatives reflect a focus on the quality of research activity, whereby mHealth interventions are evaluated in research studies, and the results of those studies are reported.<a id=""xref-ref-18-1"" class=""xref-bibr"" href=""#ref-18"">18</a> The process of systematically reviewing published studies then provides an overview of the evidence base for the use and effectiveness of different types of mHealth interventions for different patient populations and various purposes. However, guidance is missing on how to systematically review commercially developed mHealth apps (ie, the software products available to download from app stores), which are often not derived from research or subject to evaluation in research studies.</p><p id=""p-14"">The process of searching, screening, extracting and analysing data, and critically appraising mHealth apps available via commercial platforms can differ from traditional approaches to reviewing published research studies of health apps. A key difference between a systematic literature review and a systematic health app review is the items evaluated. In a systematic review, reviewers attempt to identify evidence from research studies from peer-reviewed journals or grey literature to evaluate the effectiveness (RCT evidence) or other characteristics that influence the effectiveness, uptake and engagement with digital health interventions. As we have outlined, several existing systematic reviews of mHealth applications do this. In a systematic health app review, we focus on providing a transparent and replicable evaluation of the functionality, quality and purpose of mHealth apps for particular user groups or health conditions. A systematic health app review is informed by the principles and process of more traditional systematic reviews, in terms of approaches to searching, use of inclusion/exclusion criteria and explicit assessment measures of quality. However, how these are operationalised is methodologically different and is the focus of this paper. By building on our research team’s experiences of conducting and publishing various reviews of commercially available mHealth apps, we provide an overview of the methodological considerations, aiming to systematise the process and support high-quality reviews of mHealth apps. In doing so, we outline the 7-step process for conducting systematic health app reviews.</p></div><div class=""section methods"" id=""sec-6""><h2 class="""">Methods</h2><p id=""p-15"">In this paper, we use examples from our previous work as case studies, supported by work from other authors to develop a new framework for conducting a review of commercially available health apps. We combine our experience (see <a id=""xref-table-wrap-1-1"" class=""xref-table"" href=""#T1"">table 1</a>) with the results of a hand search of the top medical informatics journals (ie, The Lancet Digital Health, npj Digital Medicine, Journal of Biomedical Informatics and the Journal of the American Medical Informatics Association) over the last five years (2018–2022) to identify other reviews of commercial health apps to contribute to the discussion of this methodological approach. Based on this, we propose methods for writing the research question and aim, determining the eligibility criteria and carrying out the review and highlight and discuss the methodological issues raised at each stage.</p><div id=""T1"" class=""table pos-float""><div class=""table-inline table-callout-links""><div class=""callout""><span>View this table:</span><ul class=""callout-links""><li class=""view-inline first""><a href=""##"" class=""table-expand-inline"" data-table-url=""/highwire/markup/289350/expansion?postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed&amp;table-expand-inline=1"" data-icon-position="""" data-hide-link-title=""0"">View inline</a></li>
<li class=""view-popup last""><a href=""/highwire/markup/289350/expansion?width=1000&amp;height=500&amp;iframe=true&amp;postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed"" class=""colorbox colorbox-load table-expand-popup"" rel=""gallery-fragment-tables"" data-icon-position="""" data-hide-link-title=""0"">View popup</a></li>
</ul></div></div><div class=""table-caption""><span class=""table-label"">Table 1</span> <p id=""p-16"" class=""first-child"">Summary of our app reviews, which are used as cases to inform the methods for conducting systematic app reviews</p><div class=""sb-div caption-clear""></div></div></div><p id=""p-17"">The reviews we draw on cover a range of apps and provide examples of a number of the decisions and challenges in conducting such reviews. Two of our reviews informed wider research studies; a review of apps used to support hand hygiene to provide the focus for a subsequent research evaluation,<a id=""xref-ref-19-2"" class=""xref-bibr"" href=""#ref-19"">19</a> and a review of patient-facing genetics apps to inform the design and development of a genetic counselling app.<a id=""xref-ref-20-2"" class=""xref-bibr"" href=""#ref-20"">20</a> Our other app reviews have provided evidence alongside a more traditional systematic review of the published research literature or as an independent review to guide clinicians/patients about mHealth apps they could use to support patient care.</p><p id=""p-18"">As the app review process follows much of the same steps as conducting systematic literature reviews, we also drew on some existing guidance to formulate the seven steps. This included the work by Khan <em>et al</em><a id=""xref-ref-21-1"" class=""xref-bibr"" href=""#ref-21"">21</a> who name five steps for conducting systematic literature reviews: (1) framing the question, (2) identifying relevant publications, (3) assessing the quality of studies, (4) summarising the evidence and (5) interpreting the findings. Xiao and Watson<a id=""xref-ref-22-1"" class=""xref-bibr"" href=""#ref-22"">22</a> name similar steps to conducting reviews but added steps for developing and validating the review protocol, screening for inclusion, extracting data and reporting the findings.</p></div><div class=""section results"" id=""sec-7""><h2 class="""">Results</h2><p id=""p-19"">Through discussion within the research team and drawing on our experiences of conducting app reviews and through cross-checking with app reviews by other author teams, we have outlined seven steps to support rigour in conducting reviews of health apps available on the app market. The steps are: (1) writing a research question or aim; (2) conducting scoping searches and developing the protocol; (3) determining the eligibility criteria using the TECH framework; (4) conducting the final search and screening of health apps; (5) data extraction; (6) quality, functionality and other assessments and (7) analysis and synthesis of findings. Each step is discussed in turn.</p><div id=""sec-8"" class=""subsection""><h3>Step 1: writing a research question (or aims)</h3><p id=""p-20"">The focus of an app review will influence the development of the research questions or aims and underpinning approach to evaluating health apps. If the purpose is to produce a standalone review to support future research and innovation in a specific health domain, understanding existing gaps can help formulate a more general research question. However, if the review is the starting point of a programme of research that aims to design, develop and evaluate a new health app with a population of patients, carers, health professionals or the public then the research questions may be more focused to examine aspects of apps such as their quality, functionality or availability. Formulating an answerable review question is essential for systematic literature reviews. While formulating a review question helps guide all stages of a systematic literature review (eg, searching, screening, extracting and synthesising), not every question format applies to systematic app reviews. For example, the PICO format is appropriate for systematic literature reviews looking at the effectiveness of interventions in a target population. However, in systematic health app reviews, reviewers can only access the results of effectiveness or evaluation studies (if conducted) if they are published. A bespoke alternative is required, just as with systematic qualitative reviews where SPIDER is used.</p><p id=""p-21"">Therefore, we propose the acronym ‘TECH’, which represents (1) Target user, (2) Evaluation focus, (3) Connectedness and (4) Health domain, as a mechanism to develop a focused research question to guide a health app review. TECH was designed through discussion by the research team and by mapping key concepts against existing frameworks (eg, SPIDER and PICO). <a id=""xref-fig-1-1"" class=""xref-fig"" href=""#F1"">Figure 1</a> presents the acronym, questions which researchers may consider (and the similarity to other acronyms) and a worked example for one of our reviews which aimed to identify commercially available atrial fibrillation self-management apps, analyse and synthesise characteristics, functions, privacy/security, incorporated behaviour change techniques and quality and usability.<a id=""xref-ref-23-2"" class=""xref-bibr"" href=""#ref-23"">23</a> TECH is designed to capture the nuances of health domains, supporting the development of clear, answerable research questions. It is important to note that connectedness refers to connecting with other devices or applications and existing human-driven digital or other services—such as a health app for booking appointments with therapists.</p><div id=""F1"" class=""fig pos-float  odd""><div class=""highwire-figure""><div class=""fig-inline-img-wrapper""><div class=""fig-inline-img""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.large.jpg?width=800&amp;height=600&amp;carousel=1"" title=""TECH framework with a worked example."" class=""highwire-fragment fragment-images colorbox-load"" rel=""gallery-fragment-images-142261359"" data-figure-caption='&lt;div class=""highwire-markup""&gt;TECH framework with a worked example.&lt;/div&gt;' data-icon-position="""" data-hide-link-title=""0""><span class=""hw-responsive-img""><img class=""highwire-fragment fragment-image lazyload"" alt=""Figure 1"" src=""data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"" data-src=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.medium.gif"" width=""440"" height=""271""><noscript><img class=""highwire-fragment fragment-image"" alt=""Figure 1"" src=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.medium.gif"" width=""440"" height=""271""></noscript></span></a></div></div><ul class=""highwire-figure-links inline""><li class=""download-fig first""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.large.jpg?download=true"" class=""highwire-figure-link highwire-figure-link-download"" title=""Download Figure 1"" data-icon-position="""" data-hide-link-title=""0"">Download figure</a></li>
<li class=""new-tab""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.large.jpg"" class=""highwire-figure-link highwire-figure-link-newtab"" target=""_blank"" data-icon-position="""" data-hide-link-title=""0"">Open in new tab</a></li>
<li class=""download-ppt last""><a href=""/highwire/powerpoint/289385"" class=""highwire-figure-link highwire-figure-link-ppt"" data-icon-position="""" data-hide-link-title=""0"">Download powerpoint</a></li>
</ul></div><div class=""fig-caption"" xmlns:xhtml=""http://www.w3.org/1999/xhtml""><span class=""fig-label"">Figure 1</span> <p id=""p-22"" class=""first-child"">TECH framework with a worked example.</p><div class=""sb-div caption-clear""></div></div></div></div><div id=""sec-9"" class=""subsection""><h3>Step 2: conducting scoping searches and developing the protocol</h3><p id=""p-23"">A preliminary (scoping) search of the health app market via the Apple, Google and Microsoft app stores is an essential first step to help determine whether the number of commercial health apps available is feasible to review. It is worth noting that the language used in descriptions of commercial mHealth apps can vary widely and differ from the scientific language used in published research studies. Hence, a broad search using a range of terminology should be employed initially to avoid missing relevant health apps. We recommend that researchers use basic keywords focused on the health domain/topic (see <a id=""xref-fig-1-2"" class=""xref-fig"" href=""#F1"">figure 1</a>) as the search function within app stores is limited. For example, for our hand hygiene app review we only used two keywords: hand hygiene and hand washing.<a id=""xref-ref-19-3"" class=""xref-bibr"" href=""#ref-19"">19</a> In our cancer app review,<a id=""xref-ref-24-2"" class=""xref-bibr"" href=""#ref-24"">24</a> we used more keywords, but all were related to the health domain and only one focused on the target user (patients): cancer, cancer patient, cancer treatment, cancer management and cancer side effects.</p><p id=""p-24"">If too few health apps are returned, this might allow for broadening the scope of the topic or adding more keywords, while too many apps will likely require the scope and language used to be narrowed. This means that the research question and the eligibility criteria may need to be refined iteratively, with multiple scoping searches performed until a reasonable number of apps are identified. The number of potential apps that may be included in the review can be counted by reading the app’s name and description and judging its relevance to the topic.</p><p id=""p-25"">To give an indication of how many apps is reasonable to review, we previously identified 236,<a id=""xref-ref-25-2"" class=""xref-bibr"" href=""#ref-25"">25</a> 405,<a id=""xref-ref-24-3"" class=""xref-bibr"" href=""#ref-24"">24</a> 555,<a id=""xref-ref-23-3"" class=""xref-bibr"" href=""#ref-23"">23</a> 668,<a id=""xref-ref-19-4"" class=""xref-bibr"" href=""#ref-19"">19</a> 754<a id=""xref-ref-20-3"" class=""xref-bibr"" href=""#ref-20"">20</a> and 3938<a id=""xref-ref-26-2"" class=""xref-bibr"" href=""#ref-26"">26</a> health apps from initial searches, before screening or deduplication took place. One of our reviews identified 7561 apps before screening<a id=""xref-ref-27-2"" class=""xref-bibr"" href=""#ref-27"">27</a> due to the topic (exercise), for which many apps exist. Following the initial screening of app titles and app store descriptions, this number was significantly reduced, and only 13 were included in the review. However, each research team should decide what number is appropriate by considering resources (eg, time, budget, and the number of reviewers available) and the topic of interest.</p><p id=""p-26"">Scoping searches can also help to identify important considerations for refining the inclusion and exclusion criteria. For example, in our review of hand hygiene apps,<a id=""xref-ref-19-5"" class=""xref-bibr"" href=""#ref-19"">19</a> we initially intended to target healthcare providers as the population of interest. However, the scoping search highlighted that few apps specified their intended users. We, therefore, removed healthcare providers as the intended audience from the inclusion criteria and replaced this with adults more generally.</p><p id=""p-27"">It is important to note that the scoping search should be conducted when the team is almost ready to begin the final searches, as health apps can disappear and emerge quickly from app stores. This means that the numbers determined from the scoping searches will likely differ from the number of apps identified in the final search. Longer periods between the scoping and final search will result in more substantial differences in the number of apps available.</p><p id=""p-28"">In addition to scoping searches in the app stores, we recommend conducting initial searches in databases (eg, MEDLINE and SCOPUS) and protocol registration databases to identify whether similar app reviews have been published or are underway. We also strongly recommend that researchers prospectively develop a protocol to guide their methods. Unlike systematic reviews which should usually be registered on PROSPERO (<a href=""https://www.crd.york.ac.uk/prospero/"">https://www.crd.york.ac.uk/prospero/</a>) or OSF (<a href=""https://osf.io/"">https://osf.io/</a>), there have not been any formal requirements to publish protocols of systematic health app reviews. However, we recommend that future protocols for commercial health app checks be published (in advance of the searches) on OSF to reduce the likelihood that the review is unnecessarily duplicated and ensure greater research transparency. This is becoming accepted practice for other reviews (eg, scoping reviews) for which PROSPERO registration is currently not possible.<a id=""xref-ref-28-1"" class=""xref-bibr"" href=""#ref-28"">28</a></p></div><div id=""sec-10"" class=""subsection""><h3>Step 3: determining the eligibility criteria using the TECH framework</h3><p id=""p-29"">Inclusion and exclusion criteria should be carefully defined using the information obtained in the scoping searches. Frameworks such as PICO or SPIDER may help develop eligibility criteria for a review so that characteristics such as the population/sample, type of intervention or phenomenon of interest and outcomes related to mHealth apps are considered.<a id=""xref-ref-29-1"" class=""xref-bibr"" href=""#ref-29"">29</a> However, we propose using a more focused framework for health app reviews to support the nuances of undertaking this type of systematic review, as the aim is not to examine the effectiveness of apps or to synthesise the findings of qualitative studies on health apps. As described above, the ‘TECH’ acronym considers the Target user, Evaluation focus, Connectedness and Health domain (see <a id=""xref-fig-1-3"" class=""xref-fig"" href=""#F1"">figure 1</a> for the worked example). Well-thought-out eligibility criteria using the TECH framework may support the development of an appropriate search strategy considering four aspects of commercial health apps, which can lead to a systematic and unbiased selection of appropriate apps. Additionally, reviewers should consider searching the literature to identify published and relevant app reviews to refine the eligibility criteria further.</p><p id=""p-30"">A health app may be characterised by its intended use and target audience. Therefore, we suggest separating the eligibility criteria into two components: app characteristics (evaluation focus, connectedness, health domain) and target audience characteristics (eg, age, gender, race/ethnicity, geographic location). App characteristics are likely to include the type of health intervention or health prevention method, such as self-management, that can be captured under the ‘Evaluation focus’, along with the target disease, problem or focus of the health app, which falls under the ‘Health domain’. It may be helpful to state whether the health condition is specific or if it covers a broad category of health-related issues. Some health apps can link to other software applications, connect to hardware devices (eg, wearable technologies) or rely on additional external devices (eg, virtual reality headsets or smartwatches) to function correctly, which should be captured under the ‘Connectedness’ criteria. The target audience characteristics will likely include the population type, including patients, healthcare professionals, healthcare students, carers, the public, or particular organisations. Audience characteristics may also include age ranges or if the app is aimed at children or adults; whether the app is aimed at individuals or groups; whether it is assumed that users will pay to access the app (or content within it) and whether the app is available in certain languages or locations.</p></div><div id=""sec-11"" class=""subsection""><h3>Step 4: conducting the final search and screening of health apps</h3><p id=""p-31"">Once the scoping searches are complete and the search terms have been collated, the final search can be run on the main app stores (ie, Apple, Google Play and Microsoft) to identify potentially relevant health apps. Several third-party app stores or repositories are also available for Android-based apps (eg, Amazon Appstore, F-Droid and Samsung Galaxy Apps), some of which are open source making the download process easier. The volume of app stores that can be searched will depend on the time and resources available to the review team. Other approaches include using a proprietary software database which enables searching for mobile health apps across iOS and Google Play app stores<a id=""xref-ref-30-1"" class=""xref-bibr"" href=""#ref-30"">30</a> or publicly available online rating frameworks for health apps that use expert reviewers<a id=""xref-ref-31-1"" class=""xref-bibr"" href=""#ref-31"">31</a> such as the Organisation for the Review and Care of Health Apps (ORCHA, <a href=""https://orchahealth.com/"">https://orchahealth.com/</a>), the PsyberGuide by One Mind (<a href=""https://onemindpsyberguide.org/"">https://onemindpsyberguide.org/</a>) and MindTools. This approach could be combined with independent searching and evaluating of commercial mHealth apps to ensure an exhaustive assessment is conducted.</p><p id=""p-32"">In contrast to established bibliographical research databases such as MEDLINE or PubMed, which enable complex searches (eg, use of Boolean operators and filtering options), the search function within app stores is limited. Basic filters may exclude apps that cost or only include child-friendly apps. Some stores (eg, the Google Play store) also enable for users to identify family-friendly apps and distinguish the type of app being searched (ie, phone, tablet, TV, Chromebook, watch or car). The Apple app store also has basic filters for the price (any or free), category (including health and fitness) and sorting (relevance, popularity, ratings or release date). Other factors app stores use may also affect search results (eg, app rating or use of adverts). To overcome this, searching across multiple app stores is advisable.</p><p id=""p-33"">The search on an app store results in a list of available apps and their descriptions. Unlike research literature databases, there are no options to export the results in a useful format (eg, RIS) for uploading to specialised screening and data management software (eg, Covidence<a id=""xref-ref-32-1"" class=""xref-bibr"" href=""#ref-32"">32</a> or Rayyan<a id=""xref-ref-33-1"" class=""xref-bibr"" href=""#ref-33"">33</a>). Hence, the research team must review each app on the results page of each app store to determine if it meets the inclusion criteria for the review. Like systematic literature reviews, using two or more researchers to reach a consensus on eligibility from the review enhances rigour and study quality. Screening should ideally be conducted on the same day to avoid differences in search results on the different app stores, which can vary day-to-day and across countries. A workaround can be to log the app name, version number and link to the webpage on the app store hosting each app to capture the search results and ensure these are used consistently by the review team.</p><p id=""p-34"">The second screening stage involves downloading all apps deemed to have met the inclusion criteria, requiring at least one Android and one iPhone smartphone between the review team. Strategies for this can include having separate researchers download apps using different devices or sharing one device between reviewers. Some health apps also require user accounts to be set up and verified before allowing access to their full functionality, which may be required to assess eligibility. In one of our reviews, we approached app developers for full app access, finding that they were more than happy to allow this.<a id=""xref-ref-27-3"" class=""xref-bibr"" href=""#ref-27"">27</a> Additional researchers can be consulted to resolve differences during the second screening phase. Finally, modifying a Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) flow diagram<a id=""xref-ref-34-1"" class=""xref-bibr"" href=""#ref-34"">34</a> can provide a transparent overview of the search and screening process. This also requires clearly stating the number of duplicates across the searches in addition to how many apps were excluded at each screening stage, with the reasons outlined at the second stage.</p></div><div id=""sec-12"" class=""subsection""><h3>Step 5: data extraction</h3><p id=""p-35"">Like systematic literature reviews, the data extraction process in app reviews requires identifying relevant information from the eligible apps. Data are extracted into a pre-defined data extraction (coding) sheet by using the app. The length of use to extract the information depends on the types of apps, number of data extraction items and the focus of the review. For example, some apps will take longer to review as they may require more comprehensive information to be extracted, users to register personal profiles or send push notifications at specific times of the day (eg, behaviour change apps).</p><p id=""p-36"">A range of data extraction items may be used. Henson <em>et al</em><a id=""xref-ref-12-2"" class=""xref-bibr"" href=""#ref-12"">12</a> developed a five-level framework for evaluating health apps, concluding that background information, privacy and security, evidence, ease of use and data integration are vital components to consider. Across our previous work, we have categorised the items as descriptive information, technical information and content (see <a id=""xref-table-wrap-2-1"" class=""xref-table"" href=""#T2"">table 2</a>). We have included additional items regarding gamification principles and tactics used in an app review by Rajani <em>et al</em><a id=""xref-ref-35-1"" class=""xref-bibr"" href=""#ref-35"">35</a> and levels of personalisation, security and privacy, which Parmar <em>et al</em><a id=""xref-ref-30-2"" class=""xref-bibr"" href=""#ref-30"">30</a> proposed. Other approaches used in one of our reviews<a id=""xref-ref-23-4"" class=""xref-bibr"" href=""#ref-23"">23</a> include extracting information on the Online Trust Alliance Best Practices Privacy Recommendations.<a id=""xref-ref-36-1"" class=""xref-bibr"" href=""#ref-36"">36</a> This considers four elements: (1) basic notice/disclosure, (2) key compliance policies, (3) protected privacy and protected sharing criteria and (4) miscellaneous privacy elements. Lastly, our osteoporosis app review considered ratings by the ORCHA (<a href=""https://orchahealth.com/"">https://orchahealth.com/</a>). ORCHA objectively reviews health apps, giving scores for three domains (data privacy, professional assurance and usability/accessibility) and an overall score (%). Scores below 65% imply the presence of some issues, and below 45% indicate considerable issues.</p><div id=""T2"" class=""table pos-float""><div class=""table-inline table-callout-links""><div class=""callout""><span>View this table:</span><ul class=""callout-links""><li class=""view-inline first""><a href=""##"" class=""table-expand-inline"" data-table-url=""/highwire/markup/289338/expansion?postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed&amp;table-expand-inline=1"" data-icon-position="""" data-hide-link-title=""0"">View inline</a></li>
<li class=""view-popup last""><a href=""/highwire/markup/289338/expansion?width=1000&amp;height=500&amp;iframe=true&amp;postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed"" class=""colorbox colorbox-load table-expand-popup"" rel=""gallery-fragment-tables"" data-icon-position="""" data-hide-link-title=""0"">View popup</a></li>
</ul></div></div><div class=""table-caption""><span class=""table-label"">Table 2</span> <p id=""p-37"" class=""first-child"">Example of data extraction items for a commercial health app review</p><div class=""sb-div caption-clear""></div></div></div><p id=""p-59"">We encourage researchers to develop their own data extraction items relevant to their topics of interest. For example, in our hand hygiene app review and like other app reviews,<a id=""xref-ref-37-1"" class=""xref-bibr"" href=""#ref-37"">37 38</a> we developed criteria to assess the comprehensiveness of the content by identifying themes across reputable sources and guidelines on hand hygiene. Other examples include extracting information about health app security and privacy, including HIPPA or COPPA compliance, whether a medical disclaimer was provided, encrypted data disclaimer, and user verification strategies during login.<a id=""xref-ref-30-3"" class=""xref-bibr"" href=""#ref-30"">30</a></p><p id=""p-60"">We also note that sometimes the information sought is not readily available or transparently reported within apps. In this case, researchers should note where information is missing, using acronyms like N/R (not reported) or N/A (not available). This can also be an interesting finding and an opportunity for apps to be improved. For example, excluding information about data sharing may be concerning for health apps that collect and record personal medical information.</p><p id=""p-61"">Readability metrics can also help determine how appropriate the language used in each app is. Researchers can determine readability by copying a paragraph into a Microsoft Word document and using two Flesch-Kincaid metrics that are built into the word processing software.<a id=""xref-ref-39-1"" class=""xref-bibr"" href=""#ref-39"">39 40</a> The Flesch-Kincaid Reading Ease score ranges from 0 to 100, with higher scores indicating that the material is easier to read.<a id=""xref-ref-39-2"" class=""xref-bibr"" href=""#ref-39"">39</a> The Flesch-Kincaid Grade Level gives a score that refers to the equivalent grade level of education in the USA.<a id=""xref-ref-40-1"" class=""xref-bibr"" href=""#ref-40"">40</a> For example, a score of 12 indicates that a twelfth grader (aged 17 or 18) in the USA should be able to understand the content.</p></div><div id=""sec-13"" class=""subsection""><h3>Step 6: quality, functionality, and other assessments</h3><div id=""sec-14"" class=""subsection""><h4>Quality</h4><p id=""p-62"">Evaluating the quality of apps requires a different approach to using critical appraisal tools or risk of bias measures commonly used in systematic literature reviews. Quality can be assessed using the Mobile App Rating Scale (MARS), which comprises 19 items across four objective scales (engagement, functionality, aesthetics and information quality) and an additional 4-item subjective quality scale.<a id=""xref-ref-41-1"" class=""xref-bibr"" href=""#ref-41"">41</a> Each item is rated on a 5-point Likert scale: (1) inadequate, (2) poor, (3) acceptable, (4) good and (5) excellent. MARS has been translated into several languages, including French, Spanish, German and Italian<a id=""xref-ref-42-1"" class=""xref-bibr"" href=""#ref-42"">42–45</a> and is suitable for assessing mobile apps for health conditions due to its reliability, validity and objectivity.<a id=""xref-ref-46-1"" class=""xref-bibr"" href=""#ref-46"">46</a> In all but two of the app reviews we have conducted to date, we have excluded the subjective quality scale to ensure that assessments are as objective as possible. Nevertheless, Stoyanov <em>et al</em><a id=""xref-ref-41-2"" class=""xref-bibr"" href=""#ref-41"">41</a> reported that the objective measures correlated well with the subjective measures. A step-by-step training video on the use of the MARS is available on YouTube.<a id=""xref-ref-47-1"" class=""xref-bibr"" href=""#ref-47"">47</a></p><p id=""p-63"">The MARS question ‘Has the app been trialled/tested?’ can be answered by searching for literature on evaluation (eg, usability, satisfaction or effectiveness) and using more traditional methods, such as risk of bias, to evaluate the quality of the evidence.<a id=""xref-ref-48-1"" class=""xref-bibr"" href=""#ref-48"">48</a> However, some review teams may wish to take this a step further if the review aims to recommend evidence-based apps to their target population. For example, in our review on strength and balance exercises for older adults,<a id=""xref-ref-27-4"" class=""xref-bibr"" href=""#ref-27"">27</a> we also visited the app/developer websites and contacted the developers directly for information on any evaluations that had taken place concerning the effectiveness of the apps in preventing falls. Due to the absence of evaluations, we compared the interventions promoted by the app with those used in known ‘gold standard’ strength and balance programmes to determine if they had an evidence base.</p><p id=""p-64"">Researchers may also build on the MARS items to assess the quality of each app in more detail. In one of our reviews, we added predetermined criteria to further evaluate the current state of development of apps for pain assessment and to provide future directions to developers.<a id=""xref-ref-26-3"" class=""xref-bibr"" href=""#ref-26"">26</a> For example, for the item about customisation, we looked at whether the app provides a setting tool allowing users to change the interface to suit them best. For the interactivity item, we also extracted data on the manikin regarding dimension (2-dimensional or 3-dimensional), orientation (left/right) and gender (male, female or neutral). It is important to note that directly amending the MARS may impact the validity of the tool. However, additional relevant items can further explore the dimensions.</p></div><div id=""sec-15"" class=""subsection""><h4>Functionality</h4><p id=""p-65"">We recommend using the IMS Institute for Healthcare Informatics functionality score to assess the functionality of apps.<a id=""xref-ref-49-1"" class=""xref-bibr"" href=""#ref-49"">49</a> This records the availability of 11 different functions within an app, rated 1 if they are present and 0 if otherwise (see <a id=""xref-table-wrap-3-1"" class=""xref-table"" href=""#T3"">table 3</a>). It complements the MARS functionality score, which measures the quality of performance, ease of use, navigation and design of an app using rating scales. The IMS functionality score is calculated from seven main criteria (inform, instruct, record, display, guide, remind or alert and communicate) and four subcategories under the ‘record’ item (collect, share, evaluate, intervene). An overall functionality score, between 0 and 11 for the full scale, is calculated by summing the scores across the individual items. The IMS scale may be tailored to ensure relevance for a specific review; for instance, in our review of hand hygiene apps, we omitted the ‘evaluate data’ criteria because it was irrelevant to the topic.<a id=""xref-ref-19-6"" class=""xref-bibr"" href=""#ref-19"">19</a></p><div id=""T3"" class=""table pos-float""><div class=""table-inline table-callout-links""><div class=""callout""><span>View this table:</span><ul class=""callout-links""><li class=""view-inline first""><a href=""##"" class=""table-expand-inline"" data-table-url=""/highwire/markup/289355/expansion?postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed&amp;table-expand-inline=1"" data-icon-position="""" data-hide-link-title=""0"">View inline</a></li>
<li class=""view-popup last""><a href=""/highwire/markup/289355/expansion?width=1000&amp;height=500&amp;iframe=true&amp;postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed"" class=""colorbox colorbox-load table-expand-popup"" rel=""gallery-fragment-tables"" data-icon-position="""" data-hide-link-title=""0"">View popup</a></li>
</ul></div></div><div class=""table-caption""><span class=""table-label"">Table 3</span> <p id=""p-66"" class=""first-child"">IMS Institute for Healthcare Informatics functionality score items and descriptions</p><div class=""sb-div caption-clear""></div></div></div><p id=""p-67"">Written results from the IMS scale are generally supported with visual representations, such as radar graphs/charts, which map variables onto axes protruding from a central point (see <a id=""xref-fig-2-1"" class=""xref-fig"" href=""#F2"">figure 2</a>). Each axis can represent a different item of the IMS, with values plotted onto each axis. These types of data visualisations could be helpful for clinicians, patients, developers and other stakeholders so they can quickly see which health apps are ranked high or low across a range of evaluation metrics to inform decision-making about which, if any, to use.</p><div id=""F2"" class=""fig pos-float  odd""><div class=""highwire-figure""><div class=""fig-inline-img-wrapper""><div class=""fig-inline-img""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.large.jpg?width=800&amp;height=600&amp;carousel=1"" title=""Simulated radar graph mapping the 11 IMS criteria."" class=""highwire-fragment fragment-images colorbox-load"" rel=""gallery-fragment-images-142261359"" data-figure-caption='&lt;div class=""highwire-markup""&gt;Simulated radar graph mapping the 11 IMS criteria.&lt;/div&gt;' data-icon-position="""" data-hide-link-title=""0""><span class=""hw-responsive-img""><img class=""highwire-fragment fragment-image lazyload"" alt=""Figure 2"" src=""data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"" data-src=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.medium.gif"" width=""440"" height=""266""><noscript><img class=""highwire-fragment fragment-image"" alt=""Figure 2"" src=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.medium.gif"" width=""440"" height=""266""></noscript></span></a></div></div><ul class=""highwire-figure-links inline""><li class=""download-fig first""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.large.jpg?download=true"" class=""highwire-figure-link highwire-figure-link-download"" title=""Download Figure 2"" data-icon-position="""" data-hide-link-title=""0"">Download figure</a></li>
<li class=""new-tab""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.large.jpg"" class=""highwire-figure-link highwire-figure-link-newtab"" target=""_blank"" data-icon-position="""" data-hide-link-title=""0"">Open in new tab</a></li>
<li class=""download-ppt last""><a href=""/highwire/powerpoint/289366"" class=""highwire-figure-link highwire-figure-link-ppt"" data-icon-position="""" data-hide-link-title=""0"">Download powerpoint</a></li>
</ul></div><div class=""fig-caption""><span class=""fig-label"">Figure 2</span> <p id=""p-68"" class=""first-child"">Simulated radar graph mapping the 11 IMS criteria.</p><div class=""sb-div caption-clear""></div></div></div></div><div id=""sec-16"" class=""subsection""><h4>Other assessments</h4><p id=""p-69"">Other ways to evaluate health apps include examining the user reviews of each app on the app stores. Plante <em>et al</em><a id=""xref-ref-50-1"" class=""xref-bibr"" href=""#ref-50"">50</a> took this approach when reviewing blood pressure measuring smartphone apps by downloading the ratings and reviews from the iTunes store and developing a series of narrative themes linked to the high and low-rated apps. Some themes associated with high user ratings included accuracy, login functionality, convenience and successful measurement. In contrast, lower-rated apps were associated with inaccuracy, inability to produce a successful reading and refunds requested by users. A qualitative approach to understanding the experiences of a range of users can help inform the final evaluation of a range of apps.</p><p id=""p-70"">Other rating tools include THESIS, developed by evaluating over 200 mHealth apps with a panel of experts.<a id=""xref-ref-51-1"" class=""xref-bibr"" href=""#ref-51"">51</a> THESIS encompasses six domains: (1) transparency, (2) health content, (3) technical content, (4) security/privacy, (5) usability and (6) subjective rating, which considers other factors like software stability, interoperability, bandwidth and application size. Additionally, health apps have been evaluated for their security features (eg, app signing security, encryption schemes, malware presence, permissions and secure communication adoption) along with users' subjective perceptions of app security.<a id=""xref-ref-52-1"" class=""xref-bibr"" href=""#ref-52"">52</a> However, this approach requires technical expertise to undertake static and dynamic analysis techniques that may be outside some research teams' scope.</p><p id=""p-71"">Other reviews have also evaluated criteria such as ethical values and medical claims. This includes beneficence, non-maleficence, autonomy, justice and legal obligation in COVID-19 mobile phone apps following the Systems Wide Analysis of mobile health-related Technologies provided in the NHS Digital Assessment Questionnaire<a id=""xref-ref-53-1"" class=""xref-bibr"" href=""#ref-53"">53</a> and the medical claims of mental health apps such as scientific language, technical expertise and lived experience perspectives.<a id=""xref-ref-10-2"" class=""xref-bibr"" href=""#ref-10"">10</a></p></div></div><div id=""sec-17"" class=""subsection""><h3>Step 7: analysis and synthesis of findings</h3><p id=""p-72"">Data synthesis may be performed by generating descriptive statistics (sums, averages, standard deviations and percentages) on relevant items or combining these with forms of qualitative synthesis. Previously, we identified the highest-scoring apps regarding functionality and quality, presenting these with a written description of their main features. Inter-rater reliability can be calculated for the binary IMS Institute for Healthcare Informatics functionality scores using Cohen’s Kappa statistic<a id=""xref-ref-54-1"" class=""xref-bibr"" href=""#ref-54"">54</a> and an intraclass correlation coefficient (ICC) can be used to calculate inter-rater reliability for the ordinal MARS scores.<a id=""xref-ref-55-1"" class=""xref-bibr"" href=""#ref-55"">55</a> The ICC is the most commonly used statistic for assessing inter-rater reliability for ordinal variables. We have typically used an absolute agreement 2-way mixed-effects, average-measures model,<a id=""xref-ref-56-1"" class=""xref-bibr"" href=""#ref-56"">56</a> which assumes that the raters are fixed and that systematic differences between raters are relevant.</p></div><div id=""sec-18"" class=""subsection""><h3>Patient and public involvement and engagement</h3><p id=""p-73"">None of the commercial health app reviews generated by our research team actively included patients or the public due to pragmatic reasons, including time, resources and funding constraints. Additionally, we did not identify any health app review recently published in the top medical informatics journals that took this approach. However, patient and public involvement and engagement (PPIE) is viewed favourably by many funders, researchers and policymakers as it can add value to health research and facilitates its dissemination and impact. As outlined above, all stages of a commercial app review could benefit from the perspectives of patients, carers and members of the public towards the health apps being evaluated. As with traditional systematic reviews, they could assist with reviewing the protocol, searching, screening, extracting and analysing data<a id=""xref-ref-57-1"" class=""xref-bibr"" href=""#ref-57"">57</a> and co-production by undertaking quality, usability or other assessments and participating in various dissemination activities. PPIE could provide another valuable dimension to the process and enrich the results of a commercial health app review.</p></div><div id=""sec-19"" class=""subsection""><h3>Key differences between systematic literature reviews and systematic health app reviews</h3><p id=""p-74"">Here, we summarise the main differences between a traditional approach to systematic reviewing literature versus undertaking a commercial health app review, as outlined in this methodological discussion (see <a id=""xref-table-wrap-4-1"" class=""xref-table"" href=""#T4"">table 4</a>).</p><div id=""T4"" class=""table pos-float""><div class=""table-inline table-callout-links""><div class=""callout""><span>View this table:</span><ul class=""callout-links""><li class=""view-inline first""><a href=""##"" class=""table-expand-inline"" data-table-url=""/highwire/markup/289372/expansion?postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed&amp;table-expand-inline=1"" data-icon-position="""" data-hide-link-title=""0"">View inline</a></li>
<li class=""view-popup last""><a href=""/highwire/markup/289372/expansion?width=1000&amp;height=500&amp;iframe=true&amp;postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed"" class=""colorbox colorbox-load table-expand-popup"" rel=""gallery-fragment-tables"" data-icon-position="""" data-hide-link-title=""0"">View popup</a></li>
</ul></div></div><div class=""table-caption""><span class=""table-label"">Table 4</span> <p id=""p-75"" class=""first-child"">Summary of the review stages, contrasting systematic literature review methods with systematic health app review methods</p><div class=""sb-div caption-clear""></div></div></div></div></div><div class=""section discussion"" id=""sec-20""><h2 class="""">Discussion</h2><p id=""p-144"">This methods paper outlines the 7-step process for conducting systematic reviews of commercial health apps. Through comparison with systematic literature reviews, we explore the complexities of each stage of an app review and provide suggestions on how to formulate a research question, develop and run scoping searches, register the protocol, determine the eligibility criteria, conduct the final search and screening, extract data, perform quality assessments and synthesise the findings. We also propose that the novel TECH framework is adopted to allow a standardised specification to be developed and applied in health app reviews, similar to using PICO, SPICE or SPIDER in traditional systematic reviews.<a id=""xref-ref-29-2"" class=""xref-bibr"" href=""#ref-29"">29</a> Additionally, we highlight the potential for PPIE activities within health app reviews.</p><p id=""p-145"">Although health app reviews share core features with systematic reviews, three key differences warrant discussion. First, commercial health apps, and reviews of these, are more transitory with rapid changes in the mHealth landscape that private industry providers dominate. For example, the geographical and price specificity of app sources are not replicable in the same way that evidence sources for systematic reviews are, and apps can appear, change and disappear quickly, impacting the replicability of the search results. Authors must be aware of the need to report granular details of their searches and to publish their review within a reasonable time from the search date, such as by using preprint servers while peer-review in a scientific journal takes place. Furthermore, the resources available to the review team are more critical than in systematic reviewing, so it is essential to transparently document scoping searches and iterative adjustments to review scope and inclusion criteria.<a id=""xref-ref-58-1"" class=""xref-bibr"" href=""#ref-58"">58</a></p><p id=""p-146"">Second, the critical appraisal process for health app reviews requires a radically different approach to quality assessments for literature reviews, with multiple approaches and tools being used to explore app functionality and quality. This proliferation of assessment approaches means there is no ‘gold standard’ equivalent to the Cochrane Risk of Bias tool for RCTs in systematic reviews.<a id=""xref-ref-48-2"" class=""xref-bibr"" href=""#ref-48"">48 59</a> There is also no equivalent to the wider consideration of evidence certainty which is provided by Grading of Recommendations, Assessment, Development and Evaluation (GRADE) in systematic reviews.<a id=""xref-ref-60-1"" class=""xref-bibr"" href=""#ref-60"">60</a> Researchers must select an approach that best fits the review aims. Additionally, they may need to consider national standards within some countries, such as those from the UK National Institute for Health Research on digital health technologies evaluation,<a id=""xref-ref-61-1"" class=""xref-bibr"" href=""#ref-61"">61</a> which may require specific approaches for the review context.</p><p id=""p-147"">The third significant difference between systematic literature and health app reviews is the extent to which guidance, guidelines and infrastructure support them. The methodological and reporting guidance for health app reviews is in its infancy. In contrast, an extensive body of literature outlining methods for different types of systematic and now scoping and rapid reviews exists.<a id=""xref-ref-62-1"" class=""xref-bibr"" href=""#ref-62"">62–64</a> There are also clear reporting guidelines for systematic reviews, which have been expanded to scoping reviews but are lacking for health app reviews.<a id=""xref-ref-34-2"" class=""xref-bibr"" href=""#ref-34"">34 65</a> While we are undertaking research to develop methods and reporting guidance for app reviews, systematic review guidance should be referred to and adapted as necessary as an interim measure. Ultimately, there may also be a need for a tool which will allow critical appraisal of these types of mHealth reviews to parallel those that exist for systematic reviews such as AMSTAR-2<a id=""xref-ref-66-1"" class=""xref-bibr"" href=""#ref-66"">66</a> or ROBIS.<a id=""xref-ref-67-1"" class=""xref-bibr"" href=""#ref-67"">67</a></p><p id=""p-148"">We propose that this outline will guide the conduct of good quality app reviews that can inform healthcare practitioners, patients, carers, health service managers, educators, and policymakers. We also recommend the prospective registration of an app review protocol on OSF as a suitable alternative to PROSPERO where systematic review protocols are held,<a id=""xref-ref-68-1"" class=""xref-bibr"" href=""#ref-68"">68</a> and the use of preprint servers to make app reviews openly available online, allowing for rapid dissemination of findings ahead of journal publication.</p><div id=""sec-21"" class=""subsection""><h3>Implications</h3><p id=""p-149"">This outline will help ensure that others can easily replicate the methods and that future app reviews are conducted in a standardised and rigorous manner. However, while outlining the methods, we noted gaps in conducting and reporting commercial health app reviews that need addressing. Hence, we are developing reporting guidelines for systematic health app reviews and plan to subsequently develop a quality appraisal tool. Similar to the 27-item PRISMA guideline for systematic reviews<a id=""xref-ref-34-3"" class=""xref-bibr"" href=""#ref-34"">34</a> and the 22-item PRISMA-Scr guideline for scoping reviews,<a id=""xref-ref-65-1"" class=""xref-bibr"" href=""#ref-65"">65</a> our guideline will consist of a structured list of items that should be included when reporting commercial health app reviews.</p><p id=""p-150"">For those conducting commercial health app reviews, there is an opportunity for the inclusion of stakeholders to strengthen the quality and impact of their findings. This is particularly beneficial if the intended target audience experiences barriers when using health apps, as clear recommendations from an app review can help to improve the design and function of future versions of an app. Researchers should also be aware of the context in which the review is being conducted. Namely, companies owning the apps may use the review for business development and promotion opportunities or contest the quality scores. However, this highlights an opportunity for further stakeholder engagement: researchers could collaborate or consult with developers to ensure that the product aligns with the research assessment process of an app’s quality. This has the potential to influence and promote accessibility and quality as aspects of development that might not be considered otherwise. While industry developers focus on creating a commercially viable product, understanding this review process will potentially enhance and refine their development process to create a superior app than initially proposed. Ultimately, it is important to be aware of any conflicts of interest between researchers who are conducting reviews in systematic and robust ways, and industry who may wish to promote their work and financially benefit from the review findings. As with systematic reviews, collaborations which have the potential to generate such conflicts of interest should be fully and transparently reported in reviews, and review methods which minimise their potential impact should be implemented.</p></div><div id=""sec-22"" class=""subsection""><h3>Strengths and limitations</h3><p id=""p-151"">This methodological discussion has numerous strengths, such as an experienced research team who have conducted various health app reviews and various types of traditional literature and systematic reviews. This was supplemented by identifying and including relevant app reviews from the top medical informatics journals and a robust analysis and comparison between traditional systematic reviews and commercial health app reviews. However, a systematic search of health app reviews was not undertaken (this will form part of our work in developing reporting guidance), nor did we focus on the emerging field of extended reality (ie, virtual, augmented and mixed reality) and their corresponding apps, many of which are health-related and available in other app stores (eg, Steam and Oculus/Meta). Additionally, our app reviews focused on apps for clients (eg, patients or the public) and healthcare providers rather than for the health system or data services, which are also target users of digital interventions.<a id=""xref-ref-69-1"" class=""xref-bibr"" href=""#ref-69"">69</a> It is likely that the recommendations in this discussion about evaluating commercially available mHealth apps will also apply to other health apps, including extended reality, and could support researchers working in these fields.</p></div></div><div class=""section conclusions"" id=""sec-23""><h2 class="""">Conclusion</h2><p id=""p-152"">Reviews of commercial apps can provide insights into the availability of apps for a specific health topic, including their quality and functionality. We have proposed a 7-step method in an effort to standardise the process of conducting mHealth reviews. At each step, we have discussed the methods in contrast to systematic literature reviews, given that the process should similarly be systematic and robust. We have also introduced the novel TECH acronym, which will assist researchers with writing research questions for app reviews and determining the eligibility criteria. Through ongoing collaboration, we will continue to advocate for transparency and quality in app reviews by working on reporting guidelines and a quality appraisal tool.</p></div><div class=""section ref-list"" id=""ref-list-1""><h2 class="""">References",1. Tell your friend about the research in a text message.,"In the summary, cover the following information:     
- Identify 1 or 2 key messages from this text that would make interesting or helpful health content.     
- If available, include the key statistics that describe the research findings.     Otherwise, skip this step.     
- If applicable, get a brief description of the research participants,     such as age, sex, and health conditions. Otherwise, you can skip this step.    
- Think about why the general population should care about the research.","Once you have written your text message:     
Evaluate your text message to see if it may be confusing or redundant.     
If so, re-write it so it is clear and concise. Otherwise, keep it the same.     
2. Create an intriguing subject line for the text.    
3. Return your final response in a JSON format with the following format:     
{""headline"": <subject line from step 2>,     
""body"": <text from step 1>}","1. Tell your friend about the research in a text message. In the summary, cover the following information:     
- Identify 1 or 2 key messages from this text that would make interesting or helpful health content.     
- If available, include the key statistics that describe the research findings.     Otherwise, skip this step.     
- If applicable, get a brief description of the research participants,     such as age, sex, and health conditions. Otherwise, you can skip this step.    
- Think about why the general population should care about the research. 
    
Once you have written your text message:     
Evaluate your text message to see if it may be confusing or redundant.     
If so, re-write it so it is clear and concise. Otherwise, keep it the same.     
2. Create an intriguing subject line for the text.    
3. Return your final response in a JSON format with the following format:     
{""headline"": <subject line from step 2>,     
""body"": <text from step 1>}","{""headline"": ""Systematic review of mobile health apps reveals concerns about quality and effectiveness"", ""body"": ""A recent systematic review evaluated commercially available mobile health apps and found concerns about the quality of advice and support provided. Over 325,000 health apps were developed in 2018, covering a wide range of health conditions and behaviors. The review identified various frameworks and methods for evaluating app quality, as well as guidelines for reporting evaluations. The TECH approach, which focuses on target users, evaluation focus, connectedness, and health domain, was proposed to develop research questions for app reviews. The review process includes scoping searches, determining eligibility criteria, final search and screening, data extraction, quality and functionality assessments, and analysis and synthesis of findings. Patient and public involvement and engagement (PPIE) can also enhance the review process. The findings of app reviews can inform healthcare practitioners, patients, and policymakers in selecting and using evidence-based health apps.""}"
2023-06-18 2131,web_articles/2023-06-15,Conducting a systematic review and evaluation of commercially available mobile applications (apps) on a health-related topic: the TECH approach and a step-by-step methodological guide,2,"You are a science writer texting friends, family, and colleagues about the science research you have read.",gpt-3.5-turbo-16k-0613,"Conducting a systematic review and evaluation of commercially available mobile applications (apps) on a health-related topic: the TECH approach and a step-by-step methodological guide

<h2 class="""">Introduction</h2><p id=""p-11"">With the rise in the use of smartphones and other mobile technologies, there has been an increase in the availability of health applications (mHealth apps) designed to be used by individuals for various health issues. Health apps can also support health and care professionals in their daily clinical practice by providing decision support, access to clinical guidelines and education and training.<a id=""xref-ref-1-1"" class=""xref-bibr"" href=""#ref-1"">1</a> In 2018, over 325 000 health apps were developed,<a id=""xref-ref-2-1"" class=""xref-bibr"" href=""#ref-2"">2</a> covering many health conditions and targeted behaviours. For example, mHealth apps can help to support self-management of conditions like diabetes,<a id=""xref-ref-3-1"" class=""xref-bibr"" href=""#ref-3"">3</a> facilitate remote monitoring of patients with chronic conditions<a id=""xref-ref-4-1"" class=""xref-bibr"" href=""#ref-4"">4</a> or support patients with general behaviour change such as increasing/monitoring physical activity<a id=""xref-ref-5-1"" class=""xref-bibr"" href=""#ref-5"">5</a> or dietary change.<a id=""xref-ref-6-1"" class=""xref-bibr"" href=""#ref-6"">6</a> Some health apps also support public health initiatives, such as promoting healthy lifestyles and encouraging the uptake of screening and vaccination programmes.<a id=""xref-ref-7-1"" class=""xref-bibr"" href=""#ref-7"">7 8</a> The World Health Organization (WHO) have released mHealth apps to educate people about road safety, sun protection measures and COVID-19.<a id=""xref-ref-9-1"" class=""xref-bibr"" href=""#ref-9"">9</a> However, concerns have been raised about the quality of advice and support such health apps provide.<a id=""xref-ref-10-1"" class=""xref-bibr"" href=""#ref-10"">10</a> Bates <em>et al</em> also highlight concerns with the accuracy of apps designed to support medical diagnosis and potential gaps in the quality of apps regarding safety and privacy.<a id=""xref-ref-2-2"" class=""xref-bibr"" href=""#ref-2"">2</a></p><p id=""p-12"">There have been attempts to provide frameworks for evaluating the quality of mHealth apps. A systematic review identified 45 frameworks for evaluating mHealth apps, which varied according to the target users (eg, developers, patients), specific conditions (eg, diabetes, mental health, cancer, pain) and various elements of evaluation that are identified in the core domains for Health Technology Assessment framework (such as safety and effectiveness).<a id=""xref-ref-11-1"" class=""xref-bibr"" href=""#ref-11"">11</a> Other frameworks have promoted a more holistic approach by encompassing privacy and security, the evidence base, ease of use and data integration<a id=""xref-ref-12-1"" class=""xref-bibr"" href=""#ref-12"">12</a> or ethical principles related to using health apps in health psychology.<a id=""xref-ref-13-1"" class=""xref-bibr"" href=""#ref-13"">13</a> Reviews have also identified existing methods for assessing mHealth app quality,<a id=""xref-ref-14-1"" class=""xref-bibr"" href=""#ref-14"">14</a> as well as guidelines for reporting evaluations of specific types of technology, such as sensors<a id=""xref-ref-15-1"" class=""xref-bibr"" href=""#ref-15"">15</a> and mHealth interventions, more broadly.<a id=""xref-ref-16-1"" class=""xref-bibr"" href=""#ref-16"">16</a> This includes the development of the Consolidated Standards of Reporting Trials of Electronic and Mobile HEalth Applications and onLine TeleHealth (CONSORT-EHEALTH) checklist, an extension of the original CONSORT checklist for reporting randomised controlled trials (RCTs).<a id=""xref-ref-17-1"" class=""xref-bibr"" href=""#ref-17"">17</a> This focuses on improving the reporting of evidence from research into the use and effectiveness of mHealth applications in research studies.</p><p id=""p-13"">The existing initiatives reflect a focus on the quality of research activity, whereby mHealth interventions are evaluated in research studies, and the results of those studies are reported.<a id=""xref-ref-18-1"" class=""xref-bibr"" href=""#ref-18"">18</a> The process of systematically reviewing published studies then provides an overview of the evidence base for the use and effectiveness of different types of mHealth interventions for different patient populations and various purposes. However, guidance is missing on how to systematically review commercially developed mHealth apps (ie, the software products available to download from app stores), which are often not derived from research or subject to evaluation in research studies.</p><p id=""p-14"">The process of searching, screening, extracting and analysing data, and critically appraising mHealth apps available via commercial platforms can differ from traditional approaches to reviewing published research studies of health apps. A key difference between a systematic literature review and a systematic health app review is the items evaluated. In a systematic review, reviewers attempt to identify evidence from research studies from peer-reviewed journals or grey literature to evaluate the effectiveness (RCT evidence) or other characteristics that influence the effectiveness, uptake and engagement with digital health interventions. As we have outlined, several existing systematic reviews of mHealth applications do this. In a systematic health app review, we focus on providing a transparent and replicable evaluation of the functionality, quality and purpose of mHealth apps for particular user groups or health conditions. A systematic health app review is informed by the principles and process of more traditional systematic reviews, in terms of approaches to searching, use of inclusion/exclusion criteria and explicit assessment measures of quality. However, how these are operationalised is methodologically different and is the focus of this paper. By building on our research team’s experiences of conducting and publishing various reviews of commercially available mHealth apps, we provide an overview of the methodological considerations, aiming to systematise the process and support high-quality reviews of mHealth apps. In doing so, we outline the 7-step process for conducting systematic health app reviews.</p></div><div class=""section methods"" id=""sec-6""><h2 class="""">Methods</h2><p id=""p-15"">In this paper, we use examples from our previous work as case studies, supported by work from other authors to develop a new framework for conducting a review of commercially available health apps. We combine our experience (see <a id=""xref-table-wrap-1-1"" class=""xref-table"" href=""#T1"">table 1</a>) with the results of a hand search of the top medical informatics journals (ie, The Lancet Digital Health, npj Digital Medicine, Journal of Biomedical Informatics and the Journal of the American Medical Informatics Association) over the last five years (2018–2022) to identify other reviews of commercial health apps to contribute to the discussion of this methodological approach. Based on this, we propose methods for writing the research question and aim, determining the eligibility criteria and carrying out the review and highlight and discuss the methodological issues raised at each stage.</p><div id=""T1"" class=""table pos-float""><div class=""table-inline table-callout-links""><div class=""callout""><span>View this table:</span><ul class=""callout-links""><li class=""view-inline first""><a href=""##"" class=""table-expand-inline"" data-table-url=""/highwire/markup/289350/expansion?postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed&amp;table-expand-inline=1"" data-icon-position="""" data-hide-link-title=""0"">View inline</a></li>
<li class=""view-popup last""><a href=""/highwire/markup/289350/expansion?width=1000&amp;height=500&amp;iframe=true&amp;postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed"" class=""colorbox colorbox-load table-expand-popup"" rel=""gallery-fragment-tables"" data-icon-position="""" data-hide-link-title=""0"">View popup</a></li>
</ul></div></div><div class=""table-caption""><span class=""table-label"">Table 1</span> <p id=""p-16"" class=""first-child"">Summary of our app reviews, which are used as cases to inform the methods for conducting systematic app reviews</p><div class=""sb-div caption-clear""></div></div></div><p id=""p-17"">The reviews we draw on cover a range of apps and provide examples of a number of the decisions and challenges in conducting such reviews. Two of our reviews informed wider research studies; a review of apps used to support hand hygiene to provide the focus for a subsequent research evaluation,<a id=""xref-ref-19-2"" class=""xref-bibr"" href=""#ref-19"">19</a> and a review of patient-facing genetics apps to inform the design and development of a genetic counselling app.<a id=""xref-ref-20-2"" class=""xref-bibr"" href=""#ref-20"">20</a> Our other app reviews have provided evidence alongside a more traditional systematic review of the published research literature or as an independent review to guide clinicians/patients about mHealth apps they could use to support patient care.</p><p id=""p-18"">As the app review process follows much of the same steps as conducting systematic literature reviews, we also drew on some existing guidance to formulate the seven steps. This included the work by Khan <em>et al</em><a id=""xref-ref-21-1"" class=""xref-bibr"" href=""#ref-21"">21</a> who name five steps for conducting systematic literature reviews: (1) framing the question, (2) identifying relevant publications, (3) assessing the quality of studies, (4) summarising the evidence and (5) interpreting the findings. Xiao and Watson<a id=""xref-ref-22-1"" class=""xref-bibr"" href=""#ref-22"">22</a> name similar steps to conducting reviews but added steps for developing and validating the review protocol, screening for inclusion, extracting data and reporting the findings.</p></div><div class=""section results"" id=""sec-7""><h2 class="""">Results</h2><p id=""p-19"">Through discussion within the research team and drawing on our experiences of conducting app reviews and through cross-checking with app reviews by other author teams, we have outlined seven steps to support rigour in conducting reviews of health apps available on the app market. The steps are: (1) writing a research question or aim; (2) conducting scoping searches and developing the protocol; (3) determining the eligibility criteria using the TECH framework; (4) conducting the final search and screening of health apps; (5) data extraction; (6) quality, functionality and other assessments and (7) analysis and synthesis of findings. Each step is discussed in turn.</p><div id=""sec-8"" class=""subsection""><h3>Step 1: writing a research question (or aims)</h3><p id=""p-20"">The focus of an app review will influence the development of the research questions or aims and underpinning approach to evaluating health apps. If the purpose is to produce a standalone review to support future research and innovation in a specific health domain, understanding existing gaps can help formulate a more general research question. However, if the review is the starting point of a programme of research that aims to design, develop and evaluate a new health app with a population of patients, carers, health professionals or the public then the research questions may be more focused to examine aspects of apps such as their quality, functionality or availability. Formulating an answerable review question is essential for systematic literature reviews. While formulating a review question helps guide all stages of a systematic literature review (eg, searching, screening, extracting and synthesising), not every question format applies to systematic app reviews. For example, the PICO format is appropriate for systematic literature reviews looking at the effectiveness of interventions in a target population. However, in systematic health app reviews, reviewers can only access the results of effectiveness or evaluation studies (if conducted) if they are published. A bespoke alternative is required, just as with systematic qualitative reviews where SPIDER is used.</p><p id=""p-21"">Therefore, we propose the acronym ‘TECH’, which represents (1) Target user, (2) Evaluation focus, (3) Connectedness and (4) Health domain, as a mechanism to develop a focused research question to guide a health app review. TECH was designed through discussion by the research team and by mapping key concepts against existing frameworks (eg, SPIDER and PICO). <a id=""xref-fig-1-1"" class=""xref-fig"" href=""#F1"">Figure 1</a> presents the acronym, questions which researchers may consider (and the similarity to other acronyms) and a worked example for one of our reviews which aimed to identify commercially available atrial fibrillation self-management apps, analyse and synthesise characteristics, functions, privacy/security, incorporated behaviour change techniques and quality and usability.<a id=""xref-ref-23-2"" class=""xref-bibr"" href=""#ref-23"">23</a> TECH is designed to capture the nuances of health domains, supporting the development of clear, answerable research questions. It is important to note that connectedness refers to connecting with other devices or applications and existing human-driven digital or other services—such as a health app for booking appointments with therapists.</p><div id=""F1"" class=""fig pos-float  odd""><div class=""highwire-figure""><div class=""fig-inline-img-wrapper""><div class=""fig-inline-img""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.large.jpg?width=800&amp;height=600&amp;carousel=1"" title=""TECH framework with a worked example."" class=""highwire-fragment fragment-images colorbox-load"" rel=""gallery-fragment-images-142261359"" data-figure-caption='&lt;div class=""highwire-markup""&gt;TECH framework with a worked example.&lt;/div&gt;' data-icon-position="""" data-hide-link-title=""0""><span class=""hw-responsive-img""><img class=""highwire-fragment fragment-image lazyload"" alt=""Figure 1"" src=""data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"" data-src=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.medium.gif"" width=""440"" height=""271""><noscript><img class=""highwire-fragment fragment-image"" alt=""Figure 1"" src=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.medium.gif"" width=""440"" height=""271""></noscript></span></a></div></div><ul class=""highwire-figure-links inline""><li class=""download-fig first""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.large.jpg?download=true"" class=""highwire-figure-link highwire-figure-link-download"" title=""Download Figure 1"" data-icon-position="""" data-hide-link-title=""0"">Download figure</a></li>
<li class=""new-tab""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.large.jpg"" class=""highwire-figure-link highwire-figure-link-newtab"" target=""_blank"" data-icon-position="""" data-hide-link-title=""0"">Open in new tab</a></li>
<li class=""download-ppt last""><a href=""/highwire/powerpoint/289385"" class=""highwire-figure-link highwire-figure-link-ppt"" data-icon-position="""" data-hide-link-title=""0"">Download powerpoint</a></li>
</ul></div><div class=""fig-caption"" xmlns:xhtml=""http://www.w3.org/1999/xhtml""><span class=""fig-label"">Figure 1</span> <p id=""p-22"" class=""first-child"">TECH framework with a worked example.</p><div class=""sb-div caption-clear""></div></div></div></div><div id=""sec-9"" class=""subsection""><h3>Step 2: conducting scoping searches and developing the protocol</h3><p id=""p-23"">A preliminary (scoping) search of the health app market via the Apple, Google and Microsoft app stores is an essential first step to help determine whether the number of commercial health apps available is feasible to review. It is worth noting that the language used in descriptions of commercial mHealth apps can vary widely and differ from the scientific language used in published research studies. Hence, a broad search using a range of terminology should be employed initially to avoid missing relevant health apps. We recommend that researchers use basic keywords focused on the health domain/topic (see <a id=""xref-fig-1-2"" class=""xref-fig"" href=""#F1"">figure 1</a>) as the search function within app stores is limited. For example, for our hand hygiene app review we only used two keywords: hand hygiene and hand washing.<a id=""xref-ref-19-3"" class=""xref-bibr"" href=""#ref-19"">19</a> In our cancer app review,<a id=""xref-ref-24-2"" class=""xref-bibr"" href=""#ref-24"">24</a> we used more keywords, but all were related to the health domain and only one focused on the target user (patients): cancer, cancer patient, cancer treatment, cancer management and cancer side effects.</p><p id=""p-24"">If too few health apps are returned, this might allow for broadening the scope of the topic or adding more keywords, while too many apps will likely require the scope and language used to be narrowed. This means that the research question and the eligibility criteria may need to be refined iteratively, with multiple scoping searches performed until a reasonable number of apps are identified. The number of potential apps that may be included in the review can be counted by reading the app’s name and description and judging its relevance to the topic.</p><p id=""p-25"">To give an indication of how many apps is reasonable to review, we previously identified 236,<a id=""xref-ref-25-2"" class=""xref-bibr"" href=""#ref-25"">25</a> 405,<a id=""xref-ref-24-3"" class=""xref-bibr"" href=""#ref-24"">24</a> 555,<a id=""xref-ref-23-3"" class=""xref-bibr"" href=""#ref-23"">23</a> 668,<a id=""xref-ref-19-4"" class=""xref-bibr"" href=""#ref-19"">19</a> 754<a id=""xref-ref-20-3"" class=""xref-bibr"" href=""#ref-20"">20</a> and 3938<a id=""xref-ref-26-2"" class=""xref-bibr"" href=""#ref-26"">26</a> health apps from initial searches, before screening or deduplication took place. One of our reviews identified 7561 apps before screening<a id=""xref-ref-27-2"" class=""xref-bibr"" href=""#ref-27"">27</a> due to the topic (exercise), for which many apps exist. Following the initial screening of app titles and app store descriptions, this number was significantly reduced, and only 13 were included in the review. However, each research team should decide what number is appropriate by considering resources (eg, time, budget, and the number of reviewers available) and the topic of interest.</p><p id=""p-26"">Scoping searches can also help to identify important considerations for refining the inclusion and exclusion criteria. For example, in our review of hand hygiene apps,<a id=""xref-ref-19-5"" class=""xref-bibr"" href=""#ref-19"">19</a> we initially intended to target healthcare providers as the population of interest. However, the scoping search highlighted that few apps specified their intended users. We, therefore, removed healthcare providers as the intended audience from the inclusion criteria and replaced this with adults more generally.</p><p id=""p-27"">It is important to note that the scoping search should be conducted when the team is almost ready to begin the final searches, as health apps can disappear and emerge quickly from app stores. This means that the numbers determined from the scoping searches will likely differ from the number of apps identified in the final search. Longer periods between the scoping and final search will result in more substantial differences in the number of apps available.</p><p id=""p-28"">In addition to scoping searches in the app stores, we recommend conducting initial searches in databases (eg, MEDLINE and SCOPUS) and protocol registration databases to identify whether similar app reviews have been published or are underway. We also strongly recommend that researchers prospectively develop a protocol to guide their methods. Unlike systematic reviews which should usually be registered on PROSPERO (<a href=""https://www.crd.york.ac.uk/prospero/"">https://www.crd.york.ac.uk/prospero/</a>) or OSF (<a href=""https://osf.io/"">https://osf.io/</a>), there have not been any formal requirements to publish protocols of systematic health app reviews. However, we recommend that future protocols for commercial health app checks be published (in advance of the searches) on OSF to reduce the likelihood that the review is unnecessarily duplicated and ensure greater research transparency. This is becoming accepted practice for other reviews (eg, scoping reviews) for which PROSPERO registration is currently not possible.<a id=""xref-ref-28-1"" class=""xref-bibr"" href=""#ref-28"">28</a></p></div><div id=""sec-10"" class=""subsection""><h3>Step 3: determining the eligibility criteria using the TECH framework</h3><p id=""p-29"">Inclusion and exclusion criteria should be carefully defined using the information obtained in the scoping searches. Frameworks such as PICO or SPIDER may help develop eligibility criteria for a review so that characteristics such as the population/sample, type of intervention or phenomenon of interest and outcomes related to mHealth apps are considered.<a id=""xref-ref-29-1"" class=""xref-bibr"" href=""#ref-29"">29</a> However, we propose using a more focused framework for health app reviews to support the nuances of undertaking this type of systematic review, as the aim is not to examine the effectiveness of apps or to synthesise the findings of qualitative studies on health apps. As described above, the ‘TECH’ acronym considers the Target user, Evaluation focus, Connectedness and Health domain (see <a id=""xref-fig-1-3"" class=""xref-fig"" href=""#F1"">figure 1</a> for the worked example). Well-thought-out eligibility criteria using the TECH framework may support the development of an appropriate search strategy considering four aspects of commercial health apps, which can lead to a systematic and unbiased selection of appropriate apps. Additionally, reviewers should consider searching the literature to identify published and relevant app reviews to refine the eligibility criteria further.</p><p id=""p-30"">A health app may be characterised by its intended use and target audience. Therefore, we suggest separating the eligibility criteria into two components: app characteristics (evaluation focus, connectedness, health domain) and target audience characteristics (eg, age, gender, race/ethnicity, geographic location). App characteristics are likely to include the type of health intervention or health prevention method, such as self-management, that can be captured under the ‘Evaluation focus’, along with the target disease, problem or focus of the health app, which falls under the ‘Health domain’. It may be helpful to state whether the health condition is specific or if it covers a broad category of health-related issues. Some health apps can link to other software applications, connect to hardware devices (eg, wearable technologies) or rely on additional external devices (eg, virtual reality headsets or smartwatches) to function correctly, which should be captured under the ‘Connectedness’ criteria. The target audience characteristics will likely include the population type, including patients, healthcare professionals, healthcare students, carers, the public, or particular organisations. Audience characteristics may also include age ranges or if the app is aimed at children or adults; whether the app is aimed at individuals or groups; whether it is assumed that users will pay to access the app (or content within it) and whether the app is available in certain languages or locations.</p></div><div id=""sec-11"" class=""subsection""><h3>Step 4: conducting the final search and screening of health apps</h3><p id=""p-31"">Once the scoping searches are complete and the search terms have been collated, the final search can be run on the main app stores (ie, Apple, Google Play and Microsoft) to identify potentially relevant health apps. Several third-party app stores or repositories are also available for Android-based apps (eg, Amazon Appstore, F-Droid and Samsung Galaxy Apps), some of which are open source making the download process easier. The volume of app stores that can be searched will depend on the time and resources available to the review team. Other approaches include using a proprietary software database which enables searching for mobile health apps across iOS and Google Play app stores<a id=""xref-ref-30-1"" class=""xref-bibr"" href=""#ref-30"">30</a> or publicly available online rating frameworks for health apps that use expert reviewers<a id=""xref-ref-31-1"" class=""xref-bibr"" href=""#ref-31"">31</a> such as the Organisation for the Review and Care of Health Apps (ORCHA, <a href=""https://orchahealth.com/"">https://orchahealth.com/</a>), the PsyberGuide by One Mind (<a href=""https://onemindpsyberguide.org/"">https://onemindpsyberguide.org/</a>) and MindTools. This approach could be combined with independent searching and evaluating of commercial mHealth apps to ensure an exhaustive assessment is conducted.</p><p id=""p-32"">In contrast to established bibliographical research databases such as MEDLINE or PubMed, which enable complex searches (eg, use of Boolean operators and filtering options), the search function within app stores is limited. Basic filters may exclude apps that cost or only include child-friendly apps. Some stores (eg, the Google Play store) also enable for users to identify family-friendly apps and distinguish the type of app being searched (ie, phone, tablet, TV, Chromebook, watch or car). The Apple app store also has basic filters for the price (any or free), category (including health and fitness) and sorting (relevance, popularity, ratings or release date). Other factors app stores use may also affect search results (eg, app rating or use of adverts). To overcome this, searching across multiple app stores is advisable.</p><p id=""p-33"">The search on an app store results in a list of available apps and their descriptions. Unlike research literature databases, there are no options to export the results in a useful format (eg, RIS) for uploading to specialised screening and data management software (eg, Covidence<a id=""xref-ref-32-1"" class=""xref-bibr"" href=""#ref-32"">32</a> or Rayyan<a id=""xref-ref-33-1"" class=""xref-bibr"" href=""#ref-33"">33</a>). Hence, the research team must review each app on the results page of each app store to determine if it meets the inclusion criteria for the review. Like systematic literature reviews, using two or more researchers to reach a consensus on eligibility from the review enhances rigour and study quality. Screening should ideally be conducted on the same day to avoid differences in search results on the different app stores, which can vary day-to-day and across countries. A workaround can be to log the app name, version number and link to the webpage on the app store hosting each app to capture the search results and ensure these are used consistently by the review team.</p><p id=""p-34"">The second screening stage involves downloading all apps deemed to have met the inclusion criteria, requiring at least one Android and one iPhone smartphone between the review team. Strategies for this can include having separate researchers download apps using different devices or sharing one device between reviewers. Some health apps also require user accounts to be set up and verified before allowing access to their full functionality, which may be required to assess eligibility. In one of our reviews, we approached app developers for full app access, finding that they were more than happy to allow this.<a id=""xref-ref-27-3"" class=""xref-bibr"" href=""#ref-27"">27</a> Additional researchers can be consulted to resolve differences during the second screening phase. Finally, modifying a Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) flow diagram<a id=""xref-ref-34-1"" class=""xref-bibr"" href=""#ref-34"">34</a> can provide a transparent overview of the search and screening process. This also requires clearly stating the number of duplicates across the searches in addition to how many apps were excluded at each screening stage, with the reasons outlined at the second stage.</p></div><div id=""sec-12"" class=""subsection""><h3>Step 5: data extraction</h3><p id=""p-35"">Like systematic literature reviews, the data extraction process in app reviews requires identifying relevant information from the eligible apps. Data are extracted into a pre-defined data extraction (coding) sheet by using the app. The length of use to extract the information depends on the types of apps, number of data extraction items and the focus of the review. For example, some apps will take longer to review as they may require more comprehensive information to be extracted, users to register personal profiles or send push notifications at specific times of the day (eg, behaviour change apps).</p><p id=""p-36"">A range of data extraction items may be used. Henson <em>et al</em><a id=""xref-ref-12-2"" class=""xref-bibr"" href=""#ref-12"">12</a> developed a five-level framework for evaluating health apps, concluding that background information, privacy and security, evidence, ease of use and data integration are vital components to consider. Across our previous work, we have categorised the items as descriptive information, technical information and content (see <a id=""xref-table-wrap-2-1"" class=""xref-table"" href=""#T2"">table 2</a>). We have included additional items regarding gamification principles and tactics used in an app review by Rajani <em>et al</em><a id=""xref-ref-35-1"" class=""xref-bibr"" href=""#ref-35"">35</a> and levels of personalisation, security and privacy, which Parmar <em>et al</em><a id=""xref-ref-30-2"" class=""xref-bibr"" href=""#ref-30"">30</a> proposed. Other approaches used in one of our reviews<a id=""xref-ref-23-4"" class=""xref-bibr"" href=""#ref-23"">23</a> include extracting information on the Online Trust Alliance Best Practices Privacy Recommendations.<a id=""xref-ref-36-1"" class=""xref-bibr"" href=""#ref-36"">36</a> This considers four elements: (1) basic notice/disclosure, (2) key compliance policies, (3) protected privacy and protected sharing criteria and (4) miscellaneous privacy elements. Lastly, our osteoporosis app review considered ratings by the ORCHA (<a href=""https://orchahealth.com/"">https://orchahealth.com/</a>). ORCHA objectively reviews health apps, giving scores for three domains (data privacy, professional assurance and usability/accessibility) and an overall score (%). Scores below 65% imply the presence of some issues, and below 45% indicate considerable issues.</p><div id=""T2"" class=""table pos-float""><div class=""table-inline table-callout-links""><div class=""callout""><span>View this table:</span><ul class=""callout-links""><li class=""view-inline first""><a href=""##"" class=""table-expand-inline"" data-table-url=""/highwire/markup/289338/expansion?postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed&amp;table-expand-inline=1"" data-icon-position="""" data-hide-link-title=""0"">View inline</a></li>
<li class=""view-popup last""><a href=""/highwire/markup/289338/expansion?width=1000&amp;height=500&amp;iframe=true&amp;postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed"" class=""colorbox colorbox-load table-expand-popup"" rel=""gallery-fragment-tables"" data-icon-position="""" data-hide-link-title=""0"">View popup</a></li>
</ul></div></div><div class=""table-caption""><span class=""table-label"">Table 2</span> <p id=""p-37"" class=""first-child"">Example of data extraction items for a commercial health app review</p><div class=""sb-div caption-clear""></div></div></div><p id=""p-59"">We encourage researchers to develop their own data extraction items relevant to their topics of interest. For example, in our hand hygiene app review and like other app reviews,<a id=""xref-ref-37-1"" class=""xref-bibr"" href=""#ref-37"">37 38</a> we developed criteria to assess the comprehensiveness of the content by identifying themes across reputable sources and guidelines on hand hygiene. Other examples include extracting information about health app security and privacy, including HIPPA or COPPA compliance, whether a medical disclaimer was provided, encrypted data disclaimer, and user verification strategies during login.<a id=""xref-ref-30-3"" class=""xref-bibr"" href=""#ref-30"">30</a></p><p id=""p-60"">We also note that sometimes the information sought is not readily available or transparently reported within apps. In this case, researchers should note where information is missing, using acronyms like N/R (not reported) or N/A (not available). This can also be an interesting finding and an opportunity for apps to be improved. For example, excluding information about data sharing may be concerning for health apps that collect and record personal medical information.</p><p id=""p-61"">Readability metrics can also help determine how appropriate the language used in each app is. Researchers can determine readability by copying a paragraph into a Microsoft Word document and using two Flesch-Kincaid metrics that are built into the word processing software.<a id=""xref-ref-39-1"" class=""xref-bibr"" href=""#ref-39"">39 40</a> The Flesch-Kincaid Reading Ease score ranges from 0 to 100, with higher scores indicating that the material is easier to read.<a id=""xref-ref-39-2"" class=""xref-bibr"" href=""#ref-39"">39</a> The Flesch-Kincaid Grade Level gives a score that refers to the equivalent grade level of education in the USA.<a id=""xref-ref-40-1"" class=""xref-bibr"" href=""#ref-40"">40</a> For example, a score of 12 indicates that a twelfth grader (aged 17 or 18) in the USA should be able to understand the content.</p></div><div id=""sec-13"" class=""subsection""><h3>Step 6: quality, functionality, and other assessments</h3><div id=""sec-14"" class=""subsection""><h4>Quality</h4><p id=""p-62"">Evaluating the quality of apps requires a different approach to using critical appraisal tools or risk of bias measures commonly used in systematic literature reviews. Quality can be assessed using the Mobile App Rating Scale (MARS), which comprises 19 items across four objective scales (engagement, functionality, aesthetics and information quality) and an additional 4-item subjective quality scale.<a id=""xref-ref-41-1"" class=""xref-bibr"" href=""#ref-41"">41</a> Each item is rated on a 5-point Likert scale: (1) inadequate, (2) poor, (3) acceptable, (4) good and (5) excellent. MARS has been translated into several languages, including French, Spanish, German and Italian<a id=""xref-ref-42-1"" class=""xref-bibr"" href=""#ref-42"">42–45</a> and is suitable for assessing mobile apps for health conditions due to its reliability, validity and objectivity.<a id=""xref-ref-46-1"" class=""xref-bibr"" href=""#ref-46"">46</a> In all but two of the app reviews we have conducted to date, we have excluded the subjective quality scale to ensure that assessments are as objective as possible. Nevertheless, Stoyanov <em>et al</em><a id=""xref-ref-41-2"" class=""xref-bibr"" href=""#ref-41"">41</a> reported that the objective measures correlated well with the subjective measures. A step-by-step training video on the use of the MARS is available on YouTube.<a id=""xref-ref-47-1"" class=""xref-bibr"" href=""#ref-47"">47</a></p><p id=""p-63"">The MARS question ‘Has the app been trialled/tested?’ can be answered by searching for literature on evaluation (eg, usability, satisfaction or effectiveness) and using more traditional methods, such as risk of bias, to evaluate the quality of the evidence.<a id=""xref-ref-48-1"" class=""xref-bibr"" href=""#ref-48"">48</a> However, some review teams may wish to take this a step further if the review aims to recommend evidence-based apps to their target population. For example, in our review on strength and balance exercises for older adults,<a id=""xref-ref-27-4"" class=""xref-bibr"" href=""#ref-27"">27</a> we also visited the app/developer websites and contacted the developers directly for information on any evaluations that had taken place concerning the effectiveness of the apps in preventing falls. Due to the absence of evaluations, we compared the interventions promoted by the app with those used in known ‘gold standard’ strength and balance programmes to determine if they had an evidence base.</p><p id=""p-64"">Researchers may also build on the MARS items to assess the quality of each app in more detail. In one of our reviews, we added predetermined criteria to further evaluate the current state of development of apps for pain assessment and to provide future directions to developers.<a id=""xref-ref-26-3"" class=""xref-bibr"" href=""#ref-26"">26</a> For example, for the item about customisation, we looked at whether the app provides a setting tool allowing users to change the interface to suit them best. For the interactivity item, we also extracted data on the manikin regarding dimension (2-dimensional or 3-dimensional), orientation (left/right) and gender (male, female or neutral). It is important to note that directly amending the MARS may impact the validity of the tool. However, additional relevant items can further explore the dimensions.</p></div><div id=""sec-15"" class=""subsection""><h4>Functionality</h4><p id=""p-65"">We recommend using the IMS Institute for Healthcare Informatics functionality score to assess the functionality of apps.<a id=""xref-ref-49-1"" class=""xref-bibr"" href=""#ref-49"">49</a> This records the availability of 11 different functions within an app, rated 1 if they are present and 0 if otherwise (see <a id=""xref-table-wrap-3-1"" class=""xref-table"" href=""#T3"">table 3</a>). It complements the MARS functionality score, which measures the quality of performance, ease of use, navigation and design of an app using rating scales. The IMS functionality score is calculated from seven main criteria (inform, instruct, record, display, guide, remind or alert and communicate) and four subcategories under the ‘record’ item (collect, share, evaluate, intervene). An overall functionality score, between 0 and 11 for the full scale, is calculated by summing the scores across the individual items. The IMS scale may be tailored to ensure relevance for a specific review; for instance, in our review of hand hygiene apps, we omitted the ‘evaluate data’ criteria because it was irrelevant to the topic.<a id=""xref-ref-19-6"" class=""xref-bibr"" href=""#ref-19"">19</a></p><div id=""T3"" class=""table pos-float""><div class=""table-inline table-callout-links""><div class=""callout""><span>View this table:</span><ul class=""callout-links""><li class=""view-inline first""><a href=""##"" class=""table-expand-inline"" data-table-url=""/highwire/markup/289355/expansion?postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed&amp;table-expand-inline=1"" data-icon-position="""" data-hide-link-title=""0"">View inline</a></li>
<li class=""view-popup last""><a href=""/highwire/markup/289355/expansion?width=1000&amp;height=500&amp;iframe=true&amp;postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed"" class=""colorbox colorbox-load table-expand-popup"" rel=""gallery-fragment-tables"" data-icon-position="""" data-hide-link-title=""0"">View popup</a></li>
</ul></div></div><div class=""table-caption""><span class=""table-label"">Table 3</span> <p id=""p-66"" class=""first-child"">IMS Institute for Healthcare Informatics functionality score items and descriptions</p><div class=""sb-div caption-clear""></div></div></div><p id=""p-67"">Written results from the IMS scale are generally supported with visual representations, such as radar graphs/charts, which map variables onto axes protruding from a central point (see <a id=""xref-fig-2-1"" class=""xref-fig"" href=""#F2"">figure 2</a>). Each axis can represent a different item of the IMS, with values plotted onto each axis. These types of data visualisations could be helpful for clinicians, patients, developers and other stakeholders so they can quickly see which health apps are ranked high or low across a range of evaluation metrics to inform decision-making about which, if any, to use.</p><div id=""F2"" class=""fig pos-float  odd""><div class=""highwire-figure""><div class=""fig-inline-img-wrapper""><div class=""fig-inline-img""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.large.jpg?width=800&amp;height=600&amp;carousel=1"" title=""Simulated radar graph mapping the 11 IMS criteria."" class=""highwire-fragment fragment-images colorbox-load"" rel=""gallery-fragment-images-142261359"" data-figure-caption='&lt;div class=""highwire-markup""&gt;Simulated radar graph mapping the 11 IMS criteria.&lt;/div&gt;' data-icon-position="""" data-hide-link-title=""0""><span class=""hw-responsive-img""><img class=""highwire-fragment fragment-image lazyload"" alt=""Figure 2"" src=""data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"" data-src=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.medium.gif"" width=""440"" height=""266""><noscript><img class=""highwire-fragment fragment-image"" alt=""Figure 2"" src=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.medium.gif"" width=""440"" height=""266""></noscript></span></a></div></div><ul class=""highwire-figure-links inline""><li class=""download-fig first""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.large.jpg?download=true"" class=""highwire-figure-link highwire-figure-link-download"" title=""Download Figure 2"" data-icon-position="""" data-hide-link-title=""0"">Download figure</a></li>
<li class=""new-tab""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.large.jpg"" class=""highwire-figure-link highwire-figure-link-newtab"" target=""_blank"" data-icon-position="""" data-hide-link-title=""0"">Open in new tab</a></li>
<li class=""download-ppt last""><a href=""/highwire/powerpoint/289366"" class=""highwire-figure-link highwire-figure-link-ppt"" data-icon-position="""" data-hide-link-title=""0"">Download powerpoint</a></li>
</ul></div><div class=""fig-caption""><span class=""fig-label"">Figure 2</span> <p id=""p-68"" class=""first-child"">Simulated radar graph mapping the 11 IMS criteria.</p><div class=""sb-div caption-clear""></div></div></div></div><div id=""sec-16"" class=""subsection""><h4>Other assessments</h4><p id=""p-69"">Other ways to evaluate health apps include examining the user reviews of each app on the app stores. Plante <em>et al</em><a id=""xref-ref-50-1"" class=""xref-bibr"" href=""#ref-50"">50</a> took this approach when reviewing blood pressure measuring smartphone apps by downloading the ratings and reviews from the iTunes store and developing a series of narrative themes linked to the high and low-rated apps. Some themes associated with high user ratings included accuracy, login functionality, convenience and successful measurement. In contrast, lower-rated apps were associated with inaccuracy, inability to produce a successful reading and refunds requested by users. A qualitative approach to understanding the experiences of a range of users can help inform the final evaluation of a range of apps.</p><p id=""p-70"">Other rating tools include THESIS, developed by evaluating over 200 mHealth apps with a panel of experts.<a id=""xref-ref-51-1"" class=""xref-bibr"" href=""#ref-51"">51</a> THESIS encompasses six domains: (1) transparency, (2) health content, (3) technical content, (4) security/privacy, (5) usability and (6) subjective rating, which considers other factors like software stability, interoperability, bandwidth and application size. Additionally, health apps have been evaluated for their security features (eg, app signing security, encryption schemes, malware presence, permissions and secure communication adoption) along with users' subjective perceptions of app security.<a id=""xref-ref-52-1"" class=""xref-bibr"" href=""#ref-52"">52</a> However, this approach requires technical expertise to undertake static and dynamic analysis techniques that may be outside some research teams' scope.</p><p id=""p-71"">Other reviews have also evaluated criteria such as ethical values and medical claims. This includes beneficence, non-maleficence, autonomy, justice and legal obligation in COVID-19 mobile phone apps following the Systems Wide Analysis of mobile health-related Technologies provided in the NHS Digital Assessment Questionnaire<a id=""xref-ref-53-1"" class=""xref-bibr"" href=""#ref-53"">53</a> and the medical claims of mental health apps such as scientific language, technical expertise and lived experience perspectives.<a id=""xref-ref-10-2"" class=""xref-bibr"" href=""#ref-10"">10</a></p></div></div><div id=""sec-17"" class=""subsection""><h3>Step 7: analysis and synthesis of findings</h3><p id=""p-72"">Data synthesis may be performed by generating descriptive statistics (sums, averages, standard deviations and percentages) on relevant items or combining these with forms of qualitative synthesis. Previously, we identified the highest-scoring apps regarding functionality and quality, presenting these with a written description of their main features. Inter-rater reliability can be calculated for the binary IMS Institute for Healthcare Informatics functionality scores using Cohen’s Kappa statistic<a id=""xref-ref-54-1"" class=""xref-bibr"" href=""#ref-54"">54</a> and an intraclass correlation coefficient (ICC) can be used to calculate inter-rater reliability for the ordinal MARS scores.<a id=""xref-ref-55-1"" class=""xref-bibr"" href=""#ref-55"">55</a> The ICC is the most commonly used statistic for assessing inter-rater reliability for ordinal variables. We have typically used an absolute agreement 2-way mixed-effects, average-measures model,<a id=""xref-ref-56-1"" class=""xref-bibr"" href=""#ref-56"">56</a> which assumes that the raters are fixed and that systematic differences between raters are relevant.</p></div><div id=""sec-18"" class=""subsection""><h3>Patient and public involvement and engagement</h3><p id=""p-73"">None of the commercial health app reviews generated by our research team actively included patients or the public due to pragmatic reasons, including time, resources and funding constraints. Additionally, we did not identify any health app review recently published in the top medical informatics journals that took this approach. However, patient and public involvement and engagement (PPIE) is viewed favourably by many funders, researchers and policymakers as it can add value to health research and facilitates its dissemination and impact. As outlined above, all stages of a commercial app review could benefit from the perspectives of patients, carers and members of the public towards the health apps being evaluated. As with traditional systematic reviews, they could assist with reviewing the protocol, searching, screening, extracting and analysing data<a id=""xref-ref-57-1"" class=""xref-bibr"" href=""#ref-57"">57</a> and co-production by undertaking quality, usability or other assessments and participating in various dissemination activities. PPIE could provide another valuable dimension to the process and enrich the results of a commercial health app review.</p></div><div id=""sec-19"" class=""subsection""><h3>Key differences between systematic literature reviews and systematic health app reviews</h3><p id=""p-74"">Here, we summarise the main differences between a traditional approach to systematic reviewing literature versus undertaking a commercial health app review, as outlined in this methodological discussion (see <a id=""xref-table-wrap-4-1"" class=""xref-table"" href=""#T4"">table 4</a>).</p><div id=""T4"" class=""table pos-float""><div class=""table-inline table-callout-links""><div class=""callout""><span>View this table:</span><ul class=""callout-links""><li class=""view-inline first""><a href=""##"" class=""table-expand-inline"" data-table-url=""/highwire/markup/289372/expansion?postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed&amp;table-expand-inline=1"" data-icon-position="""" data-hide-link-title=""0"">View inline</a></li>
<li class=""view-popup last""><a href=""/highwire/markup/289372/expansion?width=1000&amp;height=500&amp;iframe=true&amp;postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed"" class=""colorbox colorbox-load table-expand-popup"" rel=""gallery-fragment-tables"" data-icon-position="""" data-hide-link-title=""0"">View popup</a></li>
</ul></div></div><div class=""table-caption""><span class=""table-label"">Table 4</span> <p id=""p-75"" class=""first-child"">Summary of the review stages, contrasting systematic literature review methods with systematic health app review methods</p><div class=""sb-div caption-clear""></div></div></div></div></div><div class=""section discussion"" id=""sec-20""><h2 class="""">Discussion</h2><p id=""p-144"">This methods paper outlines the 7-step process for conducting systematic reviews of commercial health apps. Through comparison with systematic literature reviews, we explore the complexities of each stage of an app review and provide suggestions on how to formulate a research question, develop and run scoping searches, register the protocol, determine the eligibility criteria, conduct the final search and screening, extract data, perform quality assessments and synthesise the findings. We also propose that the novel TECH framework is adopted to allow a standardised specification to be developed and applied in health app reviews, similar to using PICO, SPICE or SPIDER in traditional systematic reviews.<a id=""xref-ref-29-2"" class=""xref-bibr"" href=""#ref-29"">29</a> Additionally, we highlight the potential for PPIE activities within health app reviews.</p><p id=""p-145"">Although health app reviews share core features with systematic reviews, three key differences warrant discussion. First, commercial health apps, and reviews of these, are more transitory with rapid changes in the mHealth landscape that private industry providers dominate. For example, the geographical and price specificity of app sources are not replicable in the same way that evidence sources for systematic reviews are, and apps can appear, change and disappear quickly, impacting the replicability of the search results. Authors must be aware of the need to report granular details of their searches and to publish their review within a reasonable time from the search date, such as by using preprint servers while peer-review in a scientific journal takes place. Furthermore, the resources available to the review team are more critical than in systematic reviewing, so it is essential to transparently document scoping searches and iterative adjustments to review scope and inclusion criteria.<a id=""xref-ref-58-1"" class=""xref-bibr"" href=""#ref-58"">58</a></p><p id=""p-146"">Second, the critical appraisal process for health app reviews requires a radically different approach to quality assessments for literature reviews, with multiple approaches and tools being used to explore app functionality and quality. This proliferation of assessment approaches means there is no ‘gold standard’ equivalent to the Cochrane Risk of Bias tool for RCTs in systematic reviews.<a id=""xref-ref-48-2"" class=""xref-bibr"" href=""#ref-48"">48 59</a> There is also no equivalent to the wider consideration of evidence certainty which is provided by Grading of Recommendations, Assessment, Development and Evaluation (GRADE) in systematic reviews.<a id=""xref-ref-60-1"" class=""xref-bibr"" href=""#ref-60"">60</a> Researchers must select an approach that best fits the review aims. Additionally, they may need to consider national standards within some countries, such as those from the UK National Institute for Health Research on digital health technologies evaluation,<a id=""xref-ref-61-1"" class=""xref-bibr"" href=""#ref-61"">61</a> which may require specific approaches for the review context.</p><p id=""p-147"">The third significant difference between systematic literature and health app reviews is the extent to which guidance, guidelines and infrastructure support them. The methodological and reporting guidance for health app reviews is in its infancy. In contrast, an extensive body of literature outlining methods for different types of systematic and now scoping and rapid reviews exists.<a id=""xref-ref-62-1"" class=""xref-bibr"" href=""#ref-62"">62–64</a> There are also clear reporting guidelines for systematic reviews, which have been expanded to scoping reviews but are lacking for health app reviews.<a id=""xref-ref-34-2"" class=""xref-bibr"" href=""#ref-34"">34 65</a> While we are undertaking research to develop methods and reporting guidance for app reviews, systematic review guidance should be referred to and adapted as necessary as an interim measure. Ultimately, there may also be a need for a tool which will allow critical appraisal of these types of mHealth reviews to parallel those that exist for systematic reviews such as AMSTAR-2<a id=""xref-ref-66-1"" class=""xref-bibr"" href=""#ref-66"">66</a> or ROBIS.<a id=""xref-ref-67-1"" class=""xref-bibr"" href=""#ref-67"">67</a></p><p id=""p-148"">We propose that this outline will guide the conduct of good quality app reviews that can inform healthcare practitioners, patients, carers, health service managers, educators, and policymakers. We also recommend the prospective registration of an app review protocol on OSF as a suitable alternative to PROSPERO where systematic review protocols are held,<a id=""xref-ref-68-1"" class=""xref-bibr"" href=""#ref-68"">68</a> and the use of preprint servers to make app reviews openly available online, allowing for rapid dissemination of findings ahead of journal publication.</p><div id=""sec-21"" class=""subsection""><h3>Implications</h3><p id=""p-149"">This outline will help ensure that others can easily replicate the methods and that future app reviews are conducted in a standardised and rigorous manner. However, while outlining the methods, we noted gaps in conducting and reporting commercial health app reviews that need addressing. Hence, we are developing reporting guidelines for systematic health app reviews and plan to subsequently develop a quality appraisal tool. Similar to the 27-item PRISMA guideline for systematic reviews<a id=""xref-ref-34-3"" class=""xref-bibr"" href=""#ref-34"">34</a> and the 22-item PRISMA-Scr guideline for scoping reviews,<a id=""xref-ref-65-1"" class=""xref-bibr"" href=""#ref-65"">65</a> our guideline will consist of a structured list of items that should be included when reporting commercial health app reviews.</p><p id=""p-150"">For those conducting commercial health app reviews, there is an opportunity for the inclusion of stakeholders to strengthen the quality and impact of their findings. This is particularly beneficial if the intended target audience experiences barriers when using health apps, as clear recommendations from an app review can help to improve the design and function of future versions of an app. Researchers should also be aware of the context in which the review is being conducted. Namely, companies owning the apps may use the review for business development and promotion opportunities or contest the quality scores. However, this highlights an opportunity for further stakeholder engagement: researchers could collaborate or consult with developers to ensure that the product aligns with the research assessment process of an app’s quality. This has the potential to influence and promote accessibility and quality as aspects of development that might not be considered otherwise. While industry developers focus on creating a commercially viable product, understanding this review process will potentially enhance and refine their development process to create a superior app than initially proposed. Ultimately, it is important to be aware of any conflicts of interest between researchers who are conducting reviews in systematic and robust ways, and industry who may wish to promote their work and financially benefit from the review findings. As with systematic reviews, collaborations which have the potential to generate such conflicts of interest should be fully and transparently reported in reviews, and review methods which minimise their potential impact should be implemented.</p></div><div id=""sec-22"" class=""subsection""><h3>Strengths and limitations</h3><p id=""p-151"">This methodological discussion has numerous strengths, such as an experienced research team who have conducted various health app reviews and various types of traditional literature and systematic reviews. This was supplemented by identifying and including relevant app reviews from the top medical informatics journals and a robust analysis and comparison between traditional systematic reviews and commercial health app reviews. However, a systematic search of health app reviews was not undertaken (this will form part of our work in developing reporting guidance), nor did we focus on the emerging field of extended reality (ie, virtual, augmented and mixed reality) and their corresponding apps, many of which are health-related and available in other app stores (eg, Steam and Oculus/Meta). Additionally, our app reviews focused on apps for clients (eg, patients or the public) and healthcare providers rather than for the health system or data services, which are also target users of digital interventions.<a id=""xref-ref-69-1"" class=""xref-bibr"" href=""#ref-69"">69</a> It is likely that the recommendations in this discussion about evaluating commercially available mHealth apps will also apply to other health apps, including extended reality, and could support researchers working in these fields.</p></div></div><div class=""section conclusions"" id=""sec-23""><h2 class="""">Conclusion</h2><p id=""p-152"">Reviews of commercial apps can provide insights into the availability of apps for a specific health topic, including their quality and functionality. We have proposed a 7-step method in an effort to standardise the process of conducting mHealth reviews. At each step, we have discussed the methods in contrast to systematic literature reviews, given that the process should similarly be systematic and robust. We have also introduced the novel TECH acronym, which will assist researchers with writing research questions for app reviews and determining the eligibility criteria. Through ongoing collaboration, we will continue to advocate for transparency and quality in app reviews by working on reporting guidelines and a quality appraisal tool.</p></div><div class=""section ref-list"" id=""ref-list-1""><h2 class="""">References",1. Tell your friend about the research in a text message.,"In the summary, cover the following information:     
- Identify 1 or 2 key messages from this text that would make interesting or helpful health content.     
- If available, include the key statistics that describe the research findings.     Otherwise, skip this step.     
- If applicable, get a brief description of the research participants,     such as age, sex, and health conditions. Otherwise, you can skip this step.    
- Think about why the general population should care about the research.","Once you have written your text message:     
Evaluate your text message to see if it may be confusing or redundant.     
If so, re-write it so it is clear and concise. Otherwise, keep it the same.     
2. Create an intriguing subject line for the text.    
3. Return your final response in a JSON format with the following format:     
{""headline"": <subject line from step 2>,     
""body"": <text from step 1>}","1. Tell your friend about the research in a text message. In the summary, cover the following information:     
- Identify 1 or 2 key messages from this text that would make interesting or helpful health content.     
- If available, include the key statistics that describe the research findings.     Otherwise, skip this step.     
- If applicable, get a brief description of the research participants,     such as age, sex, and health conditions. Otherwise, you can skip this step.    
- Think about why the general population should care about the research. 
    
Once you have written your text message:     
Evaluate your text message to see if it may be confusing or redundant.     
If so, re-write it so it is clear and concise. Otherwise, keep it the same.     
2. Create an intriguing subject line for the text.    
3. Return your final response in a JSON format with the following format:     
{""headline"": <subject line from step 2>,     
""body"": <text from step 1>}","{""headline"": ""New method for evaluating commercial health apps"", ""body"": ""A recent study outlines a 7-step process for conducting systematic reviews of commercially available health apps. The study highlights the need for standardized methods in evaluating the quality and functionality of health apps. The TECH framework is introduced to guide the development of research questions and eligibility criteria for app reviews. This research has the potential to improve the transparency and rigor of app reviews, ultimately benefiting healthcare practitioners, patients, and policymakers.""}"
2023-06-18 2131,web_articles/2023-06-15,Conducting a systematic review and evaluation of commercially available mobile applications (apps) on a health-related topic: the TECH approach and a step-by-step methodological guide,3,"You are a science writer texting friends, family, and colleagues about the science research you have read.",gpt-3.5-turbo-16k-0613,"Conducting a systematic review and evaluation of commercially available mobile applications (apps) on a health-related topic: the TECH approach and a step-by-step methodological guide

<h2 class="""">Introduction</h2><p id=""p-11"">With the rise in the use of smartphones and other mobile technologies, there has been an increase in the availability of health applications (mHealth apps) designed to be used by individuals for various health issues. Health apps can also support health and care professionals in their daily clinical practice by providing decision support, access to clinical guidelines and education and training.<a id=""xref-ref-1-1"" class=""xref-bibr"" href=""#ref-1"">1</a> In 2018, over 325 000 health apps were developed,<a id=""xref-ref-2-1"" class=""xref-bibr"" href=""#ref-2"">2</a> covering many health conditions and targeted behaviours. For example, mHealth apps can help to support self-management of conditions like diabetes,<a id=""xref-ref-3-1"" class=""xref-bibr"" href=""#ref-3"">3</a> facilitate remote monitoring of patients with chronic conditions<a id=""xref-ref-4-1"" class=""xref-bibr"" href=""#ref-4"">4</a> or support patients with general behaviour change such as increasing/monitoring physical activity<a id=""xref-ref-5-1"" class=""xref-bibr"" href=""#ref-5"">5</a> or dietary change.<a id=""xref-ref-6-1"" class=""xref-bibr"" href=""#ref-6"">6</a> Some health apps also support public health initiatives, such as promoting healthy lifestyles and encouraging the uptake of screening and vaccination programmes.<a id=""xref-ref-7-1"" class=""xref-bibr"" href=""#ref-7"">7 8</a> The World Health Organization (WHO) have released mHealth apps to educate people about road safety, sun protection measures and COVID-19.<a id=""xref-ref-9-1"" class=""xref-bibr"" href=""#ref-9"">9</a> However, concerns have been raised about the quality of advice and support such health apps provide.<a id=""xref-ref-10-1"" class=""xref-bibr"" href=""#ref-10"">10</a> Bates <em>et al</em> also highlight concerns with the accuracy of apps designed to support medical diagnosis and potential gaps in the quality of apps regarding safety and privacy.<a id=""xref-ref-2-2"" class=""xref-bibr"" href=""#ref-2"">2</a></p><p id=""p-12"">There have been attempts to provide frameworks for evaluating the quality of mHealth apps. A systematic review identified 45 frameworks for evaluating mHealth apps, which varied according to the target users (eg, developers, patients), specific conditions (eg, diabetes, mental health, cancer, pain) and various elements of evaluation that are identified in the core domains for Health Technology Assessment framework (such as safety and effectiveness).<a id=""xref-ref-11-1"" class=""xref-bibr"" href=""#ref-11"">11</a> Other frameworks have promoted a more holistic approach by encompassing privacy and security, the evidence base, ease of use and data integration<a id=""xref-ref-12-1"" class=""xref-bibr"" href=""#ref-12"">12</a> or ethical principles related to using health apps in health psychology.<a id=""xref-ref-13-1"" class=""xref-bibr"" href=""#ref-13"">13</a> Reviews have also identified existing methods for assessing mHealth app quality,<a id=""xref-ref-14-1"" class=""xref-bibr"" href=""#ref-14"">14</a> as well as guidelines for reporting evaluations of specific types of technology, such as sensors<a id=""xref-ref-15-1"" class=""xref-bibr"" href=""#ref-15"">15</a> and mHealth interventions, more broadly.<a id=""xref-ref-16-1"" class=""xref-bibr"" href=""#ref-16"">16</a> This includes the development of the Consolidated Standards of Reporting Trials of Electronic and Mobile HEalth Applications and onLine TeleHealth (CONSORT-EHEALTH) checklist, an extension of the original CONSORT checklist for reporting randomised controlled trials (RCTs).<a id=""xref-ref-17-1"" class=""xref-bibr"" href=""#ref-17"">17</a> This focuses on improving the reporting of evidence from research into the use and effectiveness of mHealth applications in research studies.</p><p id=""p-13"">The existing initiatives reflect a focus on the quality of research activity, whereby mHealth interventions are evaluated in research studies, and the results of those studies are reported.<a id=""xref-ref-18-1"" class=""xref-bibr"" href=""#ref-18"">18</a> The process of systematically reviewing published studies then provides an overview of the evidence base for the use and effectiveness of different types of mHealth interventions for different patient populations and various purposes. However, guidance is missing on how to systematically review commercially developed mHealth apps (ie, the software products available to download from app stores), which are often not derived from research or subject to evaluation in research studies.</p><p id=""p-14"">The process of searching, screening, extracting and analysing data, and critically appraising mHealth apps available via commercial platforms can differ from traditional approaches to reviewing published research studies of health apps. A key difference between a systematic literature review and a systematic health app review is the items evaluated. In a systematic review, reviewers attempt to identify evidence from research studies from peer-reviewed journals or grey literature to evaluate the effectiveness (RCT evidence) or other characteristics that influence the effectiveness, uptake and engagement with digital health interventions. As we have outlined, several existing systematic reviews of mHealth applications do this. In a systematic health app review, we focus on providing a transparent and replicable evaluation of the functionality, quality and purpose of mHealth apps for particular user groups or health conditions. A systematic health app review is informed by the principles and process of more traditional systematic reviews, in terms of approaches to searching, use of inclusion/exclusion criteria and explicit assessment measures of quality. However, how these are operationalised is methodologically different and is the focus of this paper. By building on our research team’s experiences of conducting and publishing various reviews of commercially available mHealth apps, we provide an overview of the methodological considerations, aiming to systematise the process and support high-quality reviews of mHealth apps. In doing so, we outline the 7-step process for conducting systematic health app reviews.</p></div><div class=""section methods"" id=""sec-6""><h2 class="""">Methods</h2><p id=""p-15"">In this paper, we use examples from our previous work as case studies, supported by work from other authors to develop a new framework for conducting a review of commercially available health apps. We combine our experience (see <a id=""xref-table-wrap-1-1"" class=""xref-table"" href=""#T1"">table 1</a>) with the results of a hand search of the top medical informatics journals (ie, The Lancet Digital Health, npj Digital Medicine, Journal of Biomedical Informatics and the Journal of the American Medical Informatics Association) over the last five years (2018–2022) to identify other reviews of commercial health apps to contribute to the discussion of this methodological approach. Based on this, we propose methods for writing the research question and aim, determining the eligibility criteria and carrying out the review and highlight and discuss the methodological issues raised at each stage.</p><div id=""T1"" class=""table pos-float""><div class=""table-inline table-callout-links""><div class=""callout""><span>View this table:</span><ul class=""callout-links""><li class=""view-inline first""><a href=""##"" class=""table-expand-inline"" data-table-url=""/highwire/markup/289350/expansion?postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed&amp;table-expand-inline=1"" data-icon-position="""" data-hide-link-title=""0"">View inline</a></li>
<li class=""view-popup last""><a href=""/highwire/markup/289350/expansion?width=1000&amp;height=500&amp;iframe=true&amp;postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed"" class=""colorbox colorbox-load table-expand-popup"" rel=""gallery-fragment-tables"" data-icon-position="""" data-hide-link-title=""0"">View popup</a></li>
</ul></div></div><div class=""table-caption""><span class=""table-label"">Table 1</span> <p id=""p-16"" class=""first-child"">Summary of our app reviews, which are used as cases to inform the methods for conducting systematic app reviews</p><div class=""sb-div caption-clear""></div></div></div><p id=""p-17"">The reviews we draw on cover a range of apps and provide examples of a number of the decisions and challenges in conducting such reviews. Two of our reviews informed wider research studies; a review of apps used to support hand hygiene to provide the focus for a subsequent research evaluation,<a id=""xref-ref-19-2"" class=""xref-bibr"" href=""#ref-19"">19</a> and a review of patient-facing genetics apps to inform the design and development of a genetic counselling app.<a id=""xref-ref-20-2"" class=""xref-bibr"" href=""#ref-20"">20</a> Our other app reviews have provided evidence alongside a more traditional systematic review of the published research literature or as an independent review to guide clinicians/patients about mHealth apps they could use to support patient care.</p><p id=""p-18"">As the app review process follows much of the same steps as conducting systematic literature reviews, we also drew on some existing guidance to formulate the seven steps. This included the work by Khan <em>et al</em><a id=""xref-ref-21-1"" class=""xref-bibr"" href=""#ref-21"">21</a> who name five steps for conducting systematic literature reviews: (1) framing the question, (2) identifying relevant publications, (3) assessing the quality of studies, (4) summarising the evidence and (5) interpreting the findings. Xiao and Watson<a id=""xref-ref-22-1"" class=""xref-bibr"" href=""#ref-22"">22</a> name similar steps to conducting reviews but added steps for developing and validating the review protocol, screening for inclusion, extracting data and reporting the findings.</p></div><div class=""section results"" id=""sec-7""><h2 class="""">Results</h2><p id=""p-19"">Through discussion within the research team and drawing on our experiences of conducting app reviews and through cross-checking with app reviews by other author teams, we have outlined seven steps to support rigour in conducting reviews of health apps available on the app market. The steps are: (1) writing a research question or aim; (2) conducting scoping searches and developing the protocol; (3) determining the eligibility criteria using the TECH framework; (4) conducting the final search and screening of health apps; (5) data extraction; (6) quality, functionality and other assessments and (7) analysis and synthesis of findings. Each step is discussed in turn.</p><div id=""sec-8"" class=""subsection""><h3>Step 1: writing a research question (or aims)</h3><p id=""p-20"">The focus of an app review will influence the development of the research questions or aims and underpinning approach to evaluating health apps. If the purpose is to produce a standalone review to support future research and innovation in a specific health domain, understanding existing gaps can help formulate a more general research question. However, if the review is the starting point of a programme of research that aims to design, develop and evaluate a new health app with a population of patients, carers, health professionals or the public then the research questions may be more focused to examine aspects of apps such as their quality, functionality or availability. Formulating an answerable review question is essential for systematic literature reviews. While formulating a review question helps guide all stages of a systematic literature review (eg, searching, screening, extracting and synthesising), not every question format applies to systematic app reviews. For example, the PICO format is appropriate for systematic literature reviews looking at the effectiveness of interventions in a target population. However, in systematic health app reviews, reviewers can only access the results of effectiveness or evaluation studies (if conducted) if they are published. A bespoke alternative is required, just as with systematic qualitative reviews where SPIDER is used.</p><p id=""p-21"">Therefore, we propose the acronym ‘TECH’, which represents (1) Target user, (2) Evaluation focus, (3) Connectedness and (4) Health domain, as a mechanism to develop a focused research question to guide a health app review. TECH was designed through discussion by the research team and by mapping key concepts against existing frameworks (eg, SPIDER and PICO). <a id=""xref-fig-1-1"" class=""xref-fig"" href=""#F1"">Figure 1</a> presents the acronym, questions which researchers may consider (and the similarity to other acronyms) and a worked example for one of our reviews which aimed to identify commercially available atrial fibrillation self-management apps, analyse and synthesise characteristics, functions, privacy/security, incorporated behaviour change techniques and quality and usability.<a id=""xref-ref-23-2"" class=""xref-bibr"" href=""#ref-23"">23</a> TECH is designed to capture the nuances of health domains, supporting the development of clear, answerable research questions. It is important to note that connectedness refers to connecting with other devices or applications and existing human-driven digital or other services—such as a health app for booking appointments with therapists.</p><div id=""F1"" class=""fig pos-float  odd""><div class=""highwire-figure""><div class=""fig-inline-img-wrapper""><div class=""fig-inline-img""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.large.jpg?width=800&amp;height=600&amp;carousel=1"" title=""TECH framework with a worked example."" class=""highwire-fragment fragment-images colorbox-load"" rel=""gallery-fragment-images-142261359"" data-figure-caption='&lt;div class=""highwire-markup""&gt;TECH framework with a worked example.&lt;/div&gt;' data-icon-position="""" data-hide-link-title=""0""><span class=""hw-responsive-img""><img class=""highwire-fragment fragment-image lazyload"" alt=""Figure 1"" src=""data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"" data-src=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.medium.gif"" width=""440"" height=""271""><noscript><img class=""highwire-fragment fragment-image"" alt=""Figure 1"" src=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.medium.gif"" width=""440"" height=""271""></noscript></span></a></div></div><ul class=""highwire-figure-links inline""><li class=""download-fig first""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.large.jpg?download=true"" class=""highwire-figure-link highwire-figure-link-download"" title=""Download Figure 1"" data-icon-position="""" data-hide-link-title=""0"">Download figure</a></li>
<li class=""new-tab""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F1.large.jpg"" class=""highwire-figure-link highwire-figure-link-newtab"" target=""_blank"" data-icon-position="""" data-hide-link-title=""0"">Open in new tab</a></li>
<li class=""download-ppt last""><a href=""/highwire/powerpoint/289385"" class=""highwire-figure-link highwire-figure-link-ppt"" data-icon-position="""" data-hide-link-title=""0"">Download powerpoint</a></li>
</ul></div><div class=""fig-caption"" xmlns:xhtml=""http://www.w3.org/1999/xhtml""><span class=""fig-label"">Figure 1</span> <p id=""p-22"" class=""first-child"">TECH framework with a worked example.</p><div class=""sb-div caption-clear""></div></div></div></div><div id=""sec-9"" class=""subsection""><h3>Step 2: conducting scoping searches and developing the protocol</h3><p id=""p-23"">A preliminary (scoping) search of the health app market via the Apple, Google and Microsoft app stores is an essential first step to help determine whether the number of commercial health apps available is feasible to review. It is worth noting that the language used in descriptions of commercial mHealth apps can vary widely and differ from the scientific language used in published research studies. Hence, a broad search using a range of terminology should be employed initially to avoid missing relevant health apps. We recommend that researchers use basic keywords focused on the health domain/topic (see <a id=""xref-fig-1-2"" class=""xref-fig"" href=""#F1"">figure 1</a>) as the search function within app stores is limited. For example, for our hand hygiene app review we only used two keywords: hand hygiene and hand washing.<a id=""xref-ref-19-3"" class=""xref-bibr"" href=""#ref-19"">19</a> In our cancer app review,<a id=""xref-ref-24-2"" class=""xref-bibr"" href=""#ref-24"">24</a> we used more keywords, but all were related to the health domain and only one focused on the target user (patients): cancer, cancer patient, cancer treatment, cancer management and cancer side effects.</p><p id=""p-24"">If too few health apps are returned, this might allow for broadening the scope of the topic or adding more keywords, while too many apps will likely require the scope and language used to be narrowed. This means that the research question and the eligibility criteria may need to be refined iteratively, with multiple scoping searches performed until a reasonable number of apps are identified. The number of potential apps that may be included in the review can be counted by reading the app’s name and description and judging its relevance to the topic.</p><p id=""p-25"">To give an indication of how many apps is reasonable to review, we previously identified 236,<a id=""xref-ref-25-2"" class=""xref-bibr"" href=""#ref-25"">25</a> 405,<a id=""xref-ref-24-3"" class=""xref-bibr"" href=""#ref-24"">24</a> 555,<a id=""xref-ref-23-3"" class=""xref-bibr"" href=""#ref-23"">23</a> 668,<a id=""xref-ref-19-4"" class=""xref-bibr"" href=""#ref-19"">19</a> 754<a id=""xref-ref-20-3"" class=""xref-bibr"" href=""#ref-20"">20</a> and 3938<a id=""xref-ref-26-2"" class=""xref-bibr"" href=""#ref-26"">26</a> health apps from initial searches, before screening or deduplication took place. One of our reviews identified 7561 apps before screening<a id=""xref-ref-27-2"" class=""xref-bibr"" href=""#ref-27"">27</a> due to the topic (exercise), for which many apps exist. Following the initial screening of app titles and app store descriptions, this number was significantly reduced, and only 13 were included in the review. However, each research team should decide what number is appropriate by considering resources (eg, time, budget, and the number of reviewers available) and the topic of interest.</p><p id=""p-26"">Scoping searches can also help to identify important considerations for refining the inclusion and exclusion criteria. For example, in our review of hand hygiene apps,<a id=""xref-ref-19-5"" class=""xref-bibr"" href=""#ref-19"">19</a> we initially intended to target healthcare providers as the population of interest. However, the scoping search highlighted that few apps specified their intended users. We, therefore, removed healthcare providers as the intended audience from the inclusion criteria and replaced this with adults more generally.</p><p id=""p-27"">It is important to note that the scoping search should be conducted when the team is almost ready to begin the final searches, as health apps can disappear and emerge quickly from app stores. This means that the numbers determined from the scoping searches will likely differ from the number of apps identified in the final search. Longer periods between the scoping and final search will result in more substantial differences in the number of apps available.</p><p id=""p-28"">In addition to scoping searches in the app stores, we recommend conducting initial searches in databases (eg, MEDLINE and SCOPUS) and protocol registration databases to identify whether similar app reviews have been published or are underway. We also strongly recommend that researchers prospectively develop a protocol to guide their methods. Unlike systematic reviews which should usually be registered on PROSPERO (<a href=""https://www.crd.york.ac.uk/prospero/"">https://www.crd.york.ac.uk/prospero/</a>) or OSF (<a href=""https://osf.io/"">https://osf.io/</a>), there have not been any formal requirements to publish protocols of systematic health app reviews. However, we recommend that future protocols for commercial health app checks be published (in advance of the searches) on OSF to reduce the likelihood that the review is unnecessarily duplicated and ensure greater research transparency. This is becoming accepted practice for other reviews (eg, scoping reviews) for which PROSPERO registration is currently not possible.<a id=""xref-ref-28-1"" class=""xref-bibr"" href=""#ref-28"">28</a></p></div><div id=""sec-10"" class=""subsection""><h3>Step 3: determining the eligibility criteria using the TECH framework</h3><p id=""p-29"">Inclusion and exclusion criteria should be carefully defined using the information obtained in the scoping searches. Frameworks such as PICO or SPIDER may help develop eligibility criteria for a review so that characteristics such as the population/sample, type of intervention or phenomenon of interest and outcomes related to mHealth apps are considered.<a id=""xref-ref-29-1"" class=""xref-bibr"" href=""#ref-29"">29</a> However, we propose using a more focused framework for health app reviews to support the nuances of undertaking this type of systematic review, as the aim is not to examine the effectiveness of apps or to synthesise the findings of qualitative studies on health apps. As described above, the ‘TECH’ acronym considers the Target user, Evaluation focus, Connectedness and Health domain (see <a id=""xref-fig-1-3"" class=""xref-fig"" href=""#F1"">figure 1</a> for the worked example). Well-thought-out eligibility criteria using the TECH framework may support the development of an appropriate search strategy considering four aspects of commercial health apps, which can lead to a systematic and unbiased selection of appropriate apps. Additionally, reviewers should consider searching the literature to identify published and relevant app reviews to refine the eligibility criteria further.</p><p id=""p-30"">A health app may be characterised by its intended use and target audience. Therefore, we suggest separating the eligibility criteria into two components: app characteristics (evaluation focus, connectedness, health domain) and target audience characteristics (eg, age, gender, race/ethnicity, geographic location). App characteristics are likely to include the type of health intervention or health prevention method, such as self-management, that can be captured under the ‘Evaluation focus’, along with the target disease, problem or focus of the health app, which falls under the ‘Health domain’. It may be helpful to state whether the health condition is specific or if it covers a broad category of health-related issues. Some health apps can link to other software applications, connect to hardware devices (eg, wearable technologies) or rely on additional external devices (eg, virtual reality headsets or smartwatches) to function correctly, which should be captured under the ‘Connectedness’ criteria. The target audience characteristics will likely include the population type, including patients, healthcare professionals, healthcare students, carers, the public, or particular organisations. Audience characteristics may also include age ranges or if the app is aimed at children or adults; whether the app is aimed at individuals or groups; whether it is assumed that users will pay to access the app (or content within it) and whether the app is available in certain languages or locations.</p></div><div id=""sec-11"" class=""subsection""><h3>Step 4: conducting the final search and screening of health apps</h3><p id=""p-31"">Once the scoping searches are complete and the search terms have been collated, the final search can be run on the main app stores (ie, Apple, Google Play and Microsoft) to identify potentially relevant health apps. Several third-party app stores or repositories are also available for Android-based apps (eg, Amazon Appstore, F-Droid and Samsung Galaxy Apps), some of which are open source making the download process easier. The volume of app stores that can be searched will depend on the time and resources available to the review team. Other approaches include using a proprietary software database which enables searching for mobile health apps across iOS and Google Play app stores<a id=""xref-ref-30-1"" class=""xref-bibr"" href=""#ref-30"">30</a> or publicly available online rating frameworks for health apps that use expert reviewers<a id=""xref-ref-31-1"" class=""xref-bibr"" href=""#ref-31"">31</a> such as the Organisation for the Review and Care of Health Apps (ORCHA, <a href=""https://orchahealth.com/"">https://orchahealth.com/</a>), the PsyberGuide by One Mind (<a href=""https://onemindpsyberguide.org/"">https://onemindpsyberguide.org/</a>) and MindTools. This approach could be combined with independent searching and evaluating of commercial mHealth apps to ensure an exhaustive assessment is conducted.</p><p id=""p-32"">In contrast to established bibliographical research databases such as MEDLINE or PubMed, which enable complex searches (eg, use of Boolean operators and filtering options), the search function within app stores is limited. Basic filters may exclude apps that cost or only include child-friendly apps. Some stores (eg, the Google Play store) also enable for users to identify family-friendly apps and distinguish the type of app being searched (ie, phone, tablet, TV, Chromebook, watch or car). The Apple app store also has basic filters for the price (any or free), category (including health and fitness) and sorting (relevance, popularity, ratings or release date). Other factors app stores use may also affect search results (eg, app rating or use of adverts). To overcome this, searching across multiple app stores is advisable.</p><p id=""p-33"">The search on an app store results in a list of available apps and their descriptions. Unlike research literature databases, there are no options to export the results in a useful format (eg, RIS) for uploading to specialised screening and data management software (eg, Covidence<a id=""xref-ref-32-1"" class=""xref-bibr"" href=""#ref-32"">32</a> or Rayyan<a id=""xref-ref-33-1"" class=""xref-bibr"" href=""#ref-33"">33</a>). Hence, the research team must review each app on the results page of each app store to determine if it meets the inclusion criteria for the review. Like systematic literature reviews, using two or more researchers to reach a consensus on eligibility from the review enhances rigour and study quality. Screening should ideally be conducted on the same day to avoid differences in search results on the different app stores, which can vary day-to-day and across countries. A workaround can be to log the app name, version number and link to the webpage on the app store hosting each app to capture the search results and ensure these are used consistently by the review team.</p><p id=""p-34"">The second screening stage involves downloading all apps deemed to have met the inclusion criteria, requiring at least one Android and one iPhone smartphone between the review team. Strategies for this can include having separate researchers download apps using different devices or sharing one device between reviewers. Some health apps also require user accounts to be set up and verified before allowing access to their full functionality, which may be required to assess eligibility. In one of our reviews, we approached app developers for full app access, finding that they were more than happy to allow this.<a id=""xref-ref-27-3"" class=""xref-bibr"" href=""#ref-27"">27</a> Additional researchers can be consulted to resolve differences during the second screening phase. Finally, modifying a Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) flow diagram<a id=""xref-ref-34-1"" class=""xref-bibr"" href=""#ref-34"">34</a> can provide a transparent overview of the search and screening process. This also requires clearly stating the number of duplicates across the searches in addition to how many apps were excluded at each screening stage, with the reasons outlined at the second stage.</p></div><div id=""sec-12"" class=""subsection""><h3>Step 5: data extraction</h3><p id=""p-35"">Like systematic literature reviews, the data extraction process in app reviews requires identifying relevant information from the eligible apps. Data are extracted into a pre-defined data extraction (coding) sheet by using the app. The length of use to extract the information depends on the types of apps, number of data extraction items and the focus of the review. For example, some apps will take longer to review as they may require more comprehensive information to be extracted, users to register personal profiles or send push notifications at specific times of the day (eg, behaviour change apps).</p><p id=""p-36"">A range of data extraction items may be used. Henson <em>et al</em><a id=""xref-ref-12-2"" class=""xref-bibr"" href=""#ref-12"">12</a> developed a five-level framework for evaluating health apps, concluding that background information, privacy and security, evidence, ease of use and data integration are vital components to consider. Across our previous work, we have categorised the items as descriptive information, technical information and content (see <a id=""xref-table-wrap-2-1"" class=""xref-table"" href=""#T2"">table 2</a>). We have included additional items regarding gamification principles and tactics used in an app review by Rajani <em>et al</em><a id=""xref-ref-35-1"" class=""xref-bibr"" href=""#ref-35"">35</a> and levels of personalisation, security and privacy, which Parmar <em>et al</em><a id=""xref-ref-30-2"" class=""xref-bibr"" href=""#ref-30"">30</a> proposed. Other approaches used in one of our reviews<a id=""xref-ref-23-4"" class=""xref-bibr"" href=""#ref-23"">23</a> include extracting information on the Online Trust Alliance Best Practices Privacy Recommendations.<a id=""xref-ref-36-1"" class=""xref-bibr"" href=""#ref-36"">36</a> This considers four elements: (1) basic notice/disclosure, (2) key compliance policies, (3) protected privacy and protected sharing criteria and (4) miscellaneous privacy elements. Lastly, our osteoporosis app review considered ratings by the ORCHA (<a href=""https://orchahealth.com/"">https://orchahealth.com/</a>). ORCHA objectively reviews health apps, giving scores for three domains (data privacy, professional assurance and usability/accessibility) and an overall score (%). Scores below 65% imply the presence of some issues, and below 45% indicate considerable issues.</p><div id=""T2"" class=""table pos-float""><div class=""table-inline table-callout-links""><div class=""callout""><span>View this table:</span><ul class=""callout-links""><li class=""view-inline first""><a href=""##"" class=""table-expand-inline"" data-table-url=""/highwire/markup/289338/expansion?postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed&amp;table-expand-inline=1"" data-icon-position="""" data-hide-link-title=""0"">View inline</a></li>
<li class=""view-popup last""><a href=""/highwire/markup/289338/expansion?width=1000&amp;height=500&amp;iframe=true&amp;postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed"" class=""colorbox colorbox-load table-expand-popup"" rel=""gallery-fragment-tables"" data-icon-position="""" data-hide-link-title=""0"">View popup</a></li>
</ul></div></div><div class=""table-caption""><span class=""table-label"">Table 2</span> <p id=""p-37"" class=""first-child"">Example of data extraction items for a commercial health app review</p><div class=""sb-div caption-clear""></div></div></div><p id=""p-59"">We encourage researchers to develop their own data extraction items relevant to their topics of interest. For example, in our hand hygiene app review and like other app reviews,<a id=""xref-ref-37-1"" class=""xref-bibr"" href=""#ref-37"">37 38</a> we developed criteria to assess the comprehensiveness of the content by identifying themes across reputable sources and guidelines on hand hygiene. Other examples include extracting information about health app security and privacy, including HIPPA or COPPA compliance, whether a medical disclaimer was provided, encrypted data disclaimer, and user verification strategies during login.<a id=""xref-ref-30-3"" class=""xref-bibr"" href=""#ref-30"">30</a></p><p id=""p-60"">We also note that sometimes the information sought is not readily available or transparently reported within apps. In this case, researchers should note where information is missing, using acronyms like N/R (not reported) or N/A (not available). This can also be an interesting finding and an opportunity for apps to be improved. For example, excluding information about data sharing may be concerning for health apps that collect and record personal medical information.</p><p id=""p-61"">Readability metrics can also help determine how appropriate the language used in each app is. Researchers can determine readability by copying a paragraph into a Microsoft Word document and using two Flesch-Kincaid metrics that are built into the word processing software.<a id=""xref-ref-39-1"" class=""xref-bibr"" href=""#ref-39"">39 40</a> The Flesch-Kincaid Reading Ease score ranges from 0 to 100, with higher scores indicating that the material is easier to read.<a id=""xref-ref-39-2"" class=""xref-bibr"" href=""#ref-39"">39</a> The Flesch-Kincaid Grade Level gives a score that refers to the equivalent grade level of education in the USA.<a id=""xref-ref-40-1"" class=""xref-bibr"" href=""#ref-40"">40</a> For example, a score of 12 indicates that a twelfth grader (aged 17 or 18) in the USA should be able to understand the content.</p></div><div id=""sec-13"" class=""subsection""><h3>Step 6: quality, functionality, and other assessments</h3><div id=""sec-14"" class=""subsection""><h4>Quality</h4><p id=""p-62"">Evaluating the quality of apps requires a different approach to using critical appraisal tools or risk of bias measures commonly used in systematic literature reviews. Quality can be assessed using the Mobile App Rating Scale (MARS), which comprises 19 items across four objective scales (engagement, functionality, aesthetics and information quality) and an additional 4-item subjective quality scale.<a id=""xref-ref-41-1"" class=""xref-bibr"" href=""#ref-41"">41</a> Each item is rated on a 5-point Likert scale: (1) inadequate, (2) poor, (3) acceptable, (4) good and (5) excellent. MARS has been translated into several languages, including French, Spanish, German and Italian<a id=""xref-ref-42-1"" class=""xref-bibr"" href=""#ref-42"">42–45</a> and is suitable for assessing mobile apps for health conditions due to its reliability, validity and objectivity.<a id=""xref-ref-46-1"" class=""xref-bibr"" href=""#ref-46"">46</a> In all but two of the app reviews we have conducted to date, we have excluded the subjective quality scale to ensure that assessments are as objective as possible. Nevertheless, Stoyanov <em>et al</em><a id=""xref-ref-41-2"" class=""xref-bibr"" href=""#ref-41"">41</a> reported that the objective measures correlated well with the subjective measures. A step-by-step training video on the use of the MARS is available on YouTube.<a id=""xref-ref-47-1"" class=""xref-bibr"" href=""#ref-47"">47</a></p><p id=""p-63"">The MARS question ‘Has the app been trialled/tested?’ can be answered by searching for literature on evaluation (eg, usability, satisfaction or effectiveness) and using more traditional methods, such as risk of bias, to evaluate the quality of the evidence.<a id=""xref-ref-48-1"" class=""xref-bibr"" href=""#ref-48"">48</a> However, some review teams may wish to take this a step further if the review aims to recommend evidence-based apps to their target population. For example, in our review on strength and balance exercises for older adults,<a id=""xref-ref-27-4"" class=""xref-bibr"" href=""#ref-27"">27</a> we also visited the app/developer websites and contacted the developers directly for information on any evaluations that had taken place concerning the effectiveness of the apps in preventing falls. Due to the absence of evaluations, we compared the interventions promoted by the app with those used in known ‘gold standard’ strength and balance programmes to determine if they had an evidence base.</p><p id=""p-64"">Researchers may also build on the MARS items to assess the quality of each app in more detail. In one of our reviews, we added predetermined criteria to further evaluate the current state of development of apps for pain assessment and to provide future directions to developers.<a id=""xref-ref-26-3"" class=""xref-bibr"" href=""#ref-26"">26</a> For example, for the item about customisation, we looked at whether the app provides a setting tool allowing users to change the interface to suit them best. For the interactivity item, we also extracted data on the manikin regarding dimension (2-dimensional or 3-dimensional), orientation (left/right) and gender (male, female or neutral). It is important to note that directly amending the MARS may impact the validity of the tool. However, additional relevant items can further explore the dimensions.</p></div><div id=""sec-15"" class=""subsection""><h4>Functionality</h4><p id=""p-65"">We recommend using the IMS Institute for Healthcare Informatics functionality score to assess the functionality of apps.<a id=""xref-ref-49-1"" class=""xref-bibr"" href=""#ref-49"">49</a> This records the availability of 11 different functions within an app, rated 1 if they are present and 0 if otherwise (see <a id=""xref-table-wrap-3-1"" class=""xref-table"" href=""#T3"">table 3</a>). It complements the MARS functionality score, which measures the quality of performance, ease of use, navigation and design of an app using rating scales. The IMS functionality score is calculated from seven main criteria (inform, instruct, record, display, guide, remind or alert and communicate) and four subcategories under the ‘record’ item (collect, share, evaluate, intervene). An overall functionality score, between 0 and 11 for the full scale, is calculated by summing the scores across the individual items. The IMS scale may be tailored to ensure relevance for a specific review; for instance, in our review of hand hygiene apps, we omitted the ‘evaluate data’ criteria because it was irrelevant to the topic.<a id=""xref-ref-19-6"" class=""xref-bibr"" href=""#ref-19"">19</a></p><div id=""T3"" class=""table pos-float""><div class=""table-inline table-callout-links""><div class=""callout""><span>View this table:</span><ul class=""callout-links""><li class=""view-inline first""><a href=""##"" class=""table-expand-inline"" data-table-url=""/highwire/markup/289355/expansion?postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed&amp;table-expand-inline=1"" data-icon-position="""" data-hide-link-title=""0"">View inline</a></li>
<li class=""view-popup last""><a href=""/highwire/markup/289355/expansion?width=1000&amp;height=500&amp;iframe=true&amp;postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed"" class=""colorbox colorbox-load table-expand-popup"" rel=""gallery-fragment-tables"" data-icon-position="""" data-hide-link-title=""0"">View popup</a></li>
</ul></div></div><div class=""table-caption""><span class=""table-label"">Table 3</span> <p id=""p-66"" class=""first-child"">IMS Institute for Healthcare Informatics functionality score items and descriptions</p><div class=""sb-div caption-clear""></div></div></div><p id=""p-67"">Written results from the IMS scale are generally supported with visual representations, such as radar graphs/charts, which map variables onto axes protruding from a central point (see <a id=""xref-fig-2-1"" class=""xref-fig"" href=""#F2"">figure 2</a>). Each axis can represent a different item of the IMS, with values plotted onto each axis. These types of data visualisations could be helpful for clinicians, patients, developers and other stakeholders so they can quickly see which health apps are ranked high or low across a range of evaluation metrics to inform decision-making about which, if any, to use.</p><div id=""F2"" class=""fig pos-float  odd""><div class=""highwire-figure""><div class=""fig-inline-img-wrapper""><div class=""fig-inline-img""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.large.jpg?width=800&amp;height=600&amp;carousel=1"" title=""Simulated radar graph mapping the 11 IMS criteria."" class=""highwire-fragment fragment-images colorbox-load"" rel=""gallery-fragment-images-142261359"" data-figure-caption='&lt;div class=""highwire-markup""&gt;Simulated radar graph mapping the 11 IMS criteria.&lt;/div&gt;' data-icon-position="""" data-hide-link-title=""0""><span class=""hw-responsive-img""><img class=""highwire-fragment fragment-image lazyload"" alt=""Figure 2"" src=""data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"" data-src=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.medium.gif"" width=""440"" height=""266""><noscript><img class=""highwire-fragment fragment-image"" alt=""Figure 2"" src=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.medium.gif"" width=""440"" height=""266""></noscript></span></a></div></div><ul class=""highwire-figure-links inline""><li class=""download-fig first""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.large.jpg?download=true"" class=""highwire-figure-link highwire-figure-link-download"" title=""Download Figure 2"" data-icon-position="""" data-hide-link-title=""0"">Download figure</a></li>
<li class=""new-tab""><a href=""https://bmjopen.bmj.com/content/bmjopen/13/6/e073283/F2.large.jpg"" class=""highwire-figure-link highwire-figure-link-newtab"" target=""_blank"" data-icon-position="""" data-hide-link-title=""0"">Open in new tab</a></li>
<li class=""download-ppt last""><a href=""/highwire/powerpoint/289366"" class=""highwire-figure-link highwire-figure-link-ppt"" data-icon-position="""" data-hide-link-title=""0"">Download powerpoint</a></li>
</ul></div><div class=""fig-caption""><span class=""fig-label"">Figure 2</span> <p id=""p-68"" class=""first-child"">Simulated radar graph mapping the 11 IMS criteria.</p><div class=""sb-div caption-clear""></div></div></div></div><div id=""sec-16"" class=""subsection""><h4>Other assessments</h4><p id=""p-69"">Other ways to evaluate health apps include examining the user reviews of each app on the app stores. Plante <em>et al</em><a id=""xref-ref-50-1"" class=""xref-bibr"" href=""#ref-50"">50</a> took this approach when reviewing blood pressure measuring smartphone apps by downloading the ratings and reviews from the iTunes store and developing a series of narrative themes linked to the high and low-rated apps. Some themes associated with high user ratings included accuracy, login functionality, convenience and successful measurement. In contrast, lower-rated apps were associated with inaccuracy, inability to produce a successful reading and refunds requested by users. A qualitative approach to understanding the experiences of a range of users can help inform the final evaluation of a range of apps.</p><p id=""p-70"">Other rating tools include THESIS, developed by evaluating over 200 mHealth apps with a panel of experts.<a id=""xref-ref-51-1"" class=""xref-bibr"" href=""#ref-51"">51</a> THESIS encompasses six domains: (1) transparency, (2) health content, (3) technical content, (4) security/privacy, (5) usability and (6) subjective rating, which considers other factors like software stability, interoperability, bandwidth and application size. Additionally, health apps have been evaluated for their security features (eg, app signing security, encryption schemes, malware presence, permissions and secure communication adoption) along with users' subjective perceptions of app security.<a id=""xref-ref-52-1"" class=""xref-bibr"" href=""#ref-52"">52</a> However, this approach requires technical expertise to undertake static and dynamic analysis techniques that may be outside some research teams' scope.</p><p id=""p-71"">Other reviews have also evaluated criteria such as ethical values and medical claims. This includes beneficence, non-maleficence, autonomy, justice and legal obligation in COVID-19 mobile phone apps following the Systems Wide Analysis of mobile health-related Technologies provided in the NHS Digital Assessment Questionnaire<a id=""xref-ref-53-1"" class=""xref-bibr"" href=""#ref-53"">53</a> and the medical claims of mental health apps such as scientific language, technical expertise and lived experience perspectives.<a id=""xref-ref-10-2"" class=""xref-bibr"" href=""#ref-10"">10</a></p></div></div><div id=""sec-17"" class=""subsection""><h3>Step 7: analysis and synthesis of findings</h3><p id=""p-72"">Data synthesis may be performed by generating descriptive statistics (sums, averages, standard deviations and percentages) on relevant items or combining these with forms of qualitative synthesis. Previously, we identified the highest-scoring apps regarding functionality and quality, presenting these with a written description of their main features. Inter-rater reliability can be calculated for the binary IMS Institute for Healthcare Informatics functionality scores using Cohen’s Kappa statistic<a id=""xref-ref-54-1"" class=""xref-bibr"" href=""#ref-54"">54</a> and an intraclass correlation coefficient (ICC) can be used to calculate inter-rater reliability for the ordinal MARS scores.<a id=""xref-ref-55-1"" class=""xref-bibr"" href=""#ref-55"">55</a> The ICC is the most commonly used statistic for assessing inter-rater reliability for ordinal variables. We have typically used an absolute agreement 2-way mixed-effects, average-measures model,<a id=""xref-ref-56-1"" class=""xref-bibr"" href=""#ref-56"">56</a> which assumes that the raters are fixed and that systematic differences between raters are relevant.</p></div><div id=""sec-18"" class=""subsection""><h3>Patient and public involvement and engagement</h3><p id=""p-73"">None of the commercial health app reviews generated by our research team actively included patients or the public due to pragmatic reasons, including time, resources and funding constraints. Additionally, we did not identify any health app review recently published in the top medical informatics journals that took this approach. However, patient and public involvement and engagement (PPIE) is viewed favourably by many funders, researchers and policymakers as it can add value to health research and facilitates its dissemination and impact. As outlined above, all stages of a commercial app review could benefit from the perspectives of patients, carers and members of the public towards the health apps being evaluated. As with traditional systematic reviews, they could assist with reviewing the protocol, searching, screening, extracting and analysing data<a id=""xref-ref-57-1"" class=""xref-bibr"" href=""#ref-57"">57</a> and co-production by undertaking quality, usability or other assessments and participating in various dissemination activities. PPIE could provide another valuable dimension to the process and enrich the results of a commercial health app review.</p></div><div id=""sec-19"" class=""subsection""><h3>Key differences between systematic literature reviews and systematic health app reviews</h3><p id=""p-74"">Here, we summarise the main differences between a traditional approach to systematic reviewing literature versus undertaking a commercial health app review, as outlined in this methodological discussion (see <a id=""xref-table-wrap-4-1"" class=""xref-table"" href=""#T4"">table 4</a>).</p><div id=""T4"" class=""table pos-float""><div class=""table-inline table-callout-links""><div class=""callout""><span>View this table:</span><ul class=""callout-links""><li class=""view-inline first""><a href=""##"" class=""table-expand-inline"" data-table-url=""/highwire/markup/289372/expansion?postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed&amp;table-expand-inline=1"" data-icon-position="""" data-hide-link-title=""0"">View inline</a></li>
<li class=""view-popup last""><a href=""/highwire/markup/289372/expansion?width=1000&amp;height=500&amp;iframe=true&amp;postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed"" class=""colorbox colorbox-load table-expand-popup"" rel=""gallery-fragment-tables"" data-icon-position="""" data-hide-link-title=""0"">View popup</a></li>
</ul></div></div><div class=""table-caption""><span class=""table-label"">Table 4</span> <p id=""p-75"" class=""first-child"">Summary of the review stages, contrasting systematic literature review methods with systematic health app review methods</p><div class=""sb-div caption-clear""></div></div></div></div></div><div class=""section discussion"" id=""sec-20""><h2 class="""">Discussion</h2><p id=""p-144"">This methods paper outlines the 7-step process for conducting systematic reviews of commercial health apps. Through comparison with systematic literature reviews, we explore the complexities of each stage of an app review and provide suggestions on how to formulate a research question, develop and run scoping searches, register the protocol, determine the eligibility criteria, conduct the final search and screening, extract data, perform quality assessments and synthesise the findings. We also propose that the novel TECH framework is adopted to allow a standardised specification to be developed and applied in health app reviews, similar to using PICO, SPICE or SPIDER in traditional systematic reviews.<a id=""xref-ref-29-2"" class=""xref-bibr"" href=""#ref-29"">29</a> Additionally, we highlight the potential for PPIE activities within health app reviews.</p><p id=""p-145"">Although health app reviews share core features with systematic reviews, three key differences warrant discussion. First, commercial health apps, and reviews of these, are more transitory with rapid changes in the mHealth landscape that private industry providers dominate. For example, the geographical and price specificity of app sources are not replicable in the same way that evidence sources for systematic reviews are, and apps can appear, change and disappear quickly, impacting the replicability of the search results. Authors must be aware of the need to report granular details of their searches and to publish their review within a reasonable time from the search date, such as by using preprint servers while peer-review in a scientific journal takes place. Furthermore, the resources available to the review team are more critical than in systematic reviewing, so it is essential to transparently document scoping searches and iterative adjustments to review scope and inclusion criteria.<a id=""xref-ref-58-1"" class=""xref-bibr"" href=""#ref-58"">58</a></p><p id=""p-146"">Second, the critical appraisal process for health app reviews requires a radically different approach to quality assessments for literature reviews, with multiple approaches and tools being used to explore app functionality and quality. This proliferation of assessment approaches means there is no ‘gold standard’ equivalent to the Cochrane Risk of Bias tool for RCTs in systematic reviews.<a id=""xref-ref-48-2"" class=""xref-bibr"" href=""#ref-48"">48 59</a> There is also no equivalent to the wider consideration of evidence certainty which is provided by Grading of Recommendations, Assessment, Development and Evaluation (GRADE) in systematic reviews.<a id=""xref-ref-60-1"" class=""xref-bibr"" href=""#ref-60"">60</a> Researchers must select an approach that best fits the review aims. Additionally, they may need to consider national standards within some countries, such as those from the UK National Institute for Health Research on digital health technologies evaluation,<a id=""xref-ref-61-1"" class=""xref-bibr"" href=""#ref-61"">61</a> which may require specific approaches for the review context.</p><p id=""p-147"">The third significant difference between systematic literature and health app reviews is the extent to which guidance, guidelines and infrastructure support them. The methodological and reporting guidance for health app reviews is in its infancy. In contrast, an extensive body of literature outlining methods for different types of systematic and now scoping and rapid reviews exists.<a id=""xref-ref-62-1"" class=""xref-bibr"" href=""#ref-62"">62–64</a> There are also clear reporting guidelines for systematic reviews, which have been expanded to scoping reviews but are lacking for health app reviews.<a id=""xref-ref-34-2"" class=""xref-bibr"" href=""#ref-34"">34 65</a> While we are undertaking research to develop methods and reporting guidance for app reviews, systematic review guidance should be referred to and adapted as necessary as an interim measure. Ultimately, there may also be a need for a tool which will allow critical appraisal of these types of mHealth reviews to parallel those that exist for systematic reviews such as AMSTAR-2<a id=""xref-ref-66-1"" class=""xref-bibr"" href=""#ref-66"">66</a> or ROBIS.<a id=""xref-ref-67-1"" class=""xref-bibr"" href=""#ref-67"">67</a></p><p id=""p-148"">We propose that this outline will guide the conduct of good quality app reviews that can inform healthcare practitioners, patients, carers, health service managers, educators, and policymakers. We also recommend the prospective registration of an app review protocol on OSF as a suitable alternative to PROSPERO where systematic review protocols are held,<a id=""xref-ref-68-1"" class=""xref-bibr"" href=""#ref-68"">68</a> and the use of preprint servers to make app reviews openly available online, allowing for rapid dissemination of findings ahead of journal publication.</p><div id=""sec-21"" class=""subsection""><h3>Implications</h3><p id=""p-149"">This outline will help ensure that others can easily replicate the methods and that future app reviews are conducted in a standardised and rigorous manner. However, while outlining the methods, we noted gaps in conducting and reporting commercial health app reviews that need addressing. Hence, we are developing reporting guidelines for systematic health app reviews and plan to subsequently develop a quality appraisal tool. Similar to the 27-item PRISMA guideline for systematic reviews<a id=""xref-ref-34-3"" class=""xref-bibr"" href=""#ref-34"">34</a> and the 22-item PRISMA-Scr guideline for scoping reviews,<a id=""xref-ref-65-1"" class=""xref-bibr"" href=""#ref-65"">65</a> our guideline will consist of a structured list of items that should be included when reporting commercial health app reviews.</p><p id=""p-150"">For those conducting commercial health app reviews, there is an opportunity for the inclusion of stakeholders to strengthen the quality and impact of their findings. This is particularly beneficial if the intended target audience experiences barriers when using health apps, as clear recommendations from an app review can help to improve the design and function of future versions of an app. Researchers should also be aware of the context in which the review is being conducted. Namely, companies owning the apps may use the review for business development and promotion opportunities or contest the quality scores. However, this highlights an opportunity for further stakeholder engagement: researchers could collaborate or consult with developers to ensure that the product aligns with the research assessment process of an app’s quality. This has the potential to influence and promote accessibility and quality as aspects of development that might not be considered otherwise. While industry developers focus on creating a commercially viable product, understanding this review process will potentially enhance and refine their development process to create a superior app than initially proposed. Ultimately, it is important to be aware of any conflicts of interest between researchers who are conducting reviews in systematic and robust ways, and industry who may wish to promote their work and financially benefit from the review findings. As with systematic reviews, collaborations which have the potential to generate such conflicts of interest should be fully and transparently reported in reviews, and review methods which minimise their potential impact should be implemented.</p></div><div id=""sec-22"" class=""subsection""><h3>Strengths and limitations</h3><p id=""p-151"">This methodological discussion has numerous strengths, such as an experienced research team who have conducted various health app reviews and various types of traditional literature and systematic reviews. This was supplemented by identifying and including relevant app reviews from the top medical informatics journals and a robust analysis and comparison between traditional systematic reviews and commercial health app reviews. However, a systematic search of health app reviews was not undertaken (this will form part of our work in developing reporting guidance), nor did we focus on the emerging field of extended reality (ie, virtual, augmented and mixed reality) and their corresponding apps, many of which are health-related and available in other app stores (eg, Steam and Oculus/Meta). Additionally, our app reviews focused on apps for clients (eg, patients or the public) and healthcare providers rather than for the health system or data services, which are also target users of digital interventions.<a id=""xref-ref-69-1"" class=""xref-bibr"" href=""#ref-69"">69</a> It is likely that the recommendations in this discussion about evaluating commercially available mHealth apps will also apply to other health apps, including extended reality, and could support researchers working in these fields.</p></div></div><div class=""section conclusions"" id=""sec-23""><h2 class="""">Conclusion</h2><p id=""p-152"">Reviews of commercial apps can provide insights into the availability of apps for a specific health topic, including their quality and functionality. We have proposed a 7-step method in an effort to standardise the process of conducting mHealth reviews. At each step, we have discussed the methods in contrast to systematic literature reviews, given that the process should similarly be systematic and robust. We have also introduced the novel TECH acronym, which will assist researchers with writing research questions for app reviews and determining the eligibility criteria. Through ongoing collaboration, we will continue to advocate for transparency and quality in app reviews by working on reporting guidelines and a quality appraisal tool.</p></div><div class=""section ref-list"" id=""ref-list-1""><h2 class="""">References",1. Tell your friend about the research in a text message.,"In the summary, cover the following information:     
- Identify 1 or 2 key messages from this text that would make interesting or helpful health content.     
- If available, include the key statistics that describe the research findings.     Otherwise, skip this step.     
- If applicable, get a brief description of the research participants,     such as age, sex, and health conditions. Otherwise, you can skip this step.    
- Think about why the general population should care about the research.","Once you have written your text message:     
Evaluate your text message to see if it may be confusing or redundant.     
If so, re-write it so it is clear and concise. Otherwise, keep it the same.     
2. Create an intriguing subject line for the text.    
3. Return your final response in a JSON format with the following format:     
{""headline"": <subject line from step 2>,     
""body"": <text from step 1>}","1. Tell your friend about the research in a text message. In the summary, cover the following information:     
- Identify 1 or 2 key messages from this text that would make interesting or helpful health content.     
- If available, include the key statistics that describe the research findings.     Otherwise, skip this step.     
- If applicable, get a brief description of the research participants,     such as age, sex, and health conditions. Otherwise, you can skip this step.    
- Think about why the general population should care about the research. 
    
Once you have written your text message:     
Evaluate your text message to see if it may be confusing or redundant.     
If so, re-write it so it is clear and concise. Otherwise, keep it the same.     
2. Create an intriguing subject line for the text.    
3. Return your final response in a JSON format with the following format:     
{""headline"": <subject line from step 2>,     
""body"": <text from step 1>}","{""headline"": ""Systematic review of health apps reveals concerns about quality and support"", ""body"": ""A recent systematic review evaluated the quality and functionality of commercially available health apps. The review identified over 325,000 health apps developed in 2018, covering a range of health conditions and behaviors. Concerns were raised about the accuracy of medical diagnosis apps and the overall quality of advice and support provided by health apps. The review also highlighted the need for frameworks and guidelines to evaluate health apps and improve their reporting. The findings have implications for healthcare professionals, patients, and developers in ensuring the effectiveness and safety of health apps.""}"
